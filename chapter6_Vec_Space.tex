\chapter{Vector Spaces and Coordinate Bases}
\label{chap:vec_space}

The previous chapters have provided a basic understanding of matrices and vectors separately. What bridge these two quantities together are the concepts of \textit{vector (sub)spaces}, \textit{linear combination}, \textit{span}, \textit{linear independence}, and \textit{coordinate bases}. We will study about the so-called \textit{four fundamental subspaces} induced by a matrix and see how they are interconnected. 

\section{Making of the Real $n$-space $\mathbb{R}^n$}

\subsection{$\mathbb{R}^n$ as a Vector Space}

We have briefly mentioned in Definition \ref{defn:real_nspace} that the real $n$-space $\mathbb{R}^n$ is mathematically a vector space, but without stating the actual requirements. In fact, to be qualified as a \index{Vector Space}\keywordhl{vector space}, a set has to satisfy the ten axioms below. We will limit ourselves to \index{Vector Space!Real Vector Space}\keywordhl{real vector spaces} for now.
\begin{defn}[Axioms of a Real Vector Space]
\label{defn:realvecspaceaxiom}
A real vector space is a non-empty set $\mathcal{V}$ with a zero vector \textbf{0}, such that for all elements (vectors) $\vec{u}, \vec{v}, \vec{w} \in \mathcal{V}$ in the set, and real numbers (as the \textit{scalars}) $a, b \in \mathbb{R}$ (for a complex vector space replace $\mathbb{R}$ by $\mathbb{C}$ here), we have
\begin{enumerate}
\item $\vec{u} + \vec{v} \in \mathcal{V}$ (Closure under Vector Addition: Addition between two vectors is defined and the resulting vector is still in the vector space.)
\item $\vec{u} + \vec{v} = \vec{v} + \vec{u}$ (Commutative Property of Addition)
\item $(\vec{u} + \vec{v}) + \vec{w} = \vec{u} + (\vec{v} + \vec{w})$ (Associative Property of Addition)
\item $\vec{u} + \textbf{0} = \textbf{0} + \vec{u} = \vec{u}$ (Zero Vector as the Additive Identity)
\item For any $\vec{u}$, there exists $\vec{w}$ such that $\vec{u} + \vec{w} = \textbf{0}$. This $\vec{w}$ is denoted as $-\vec{u}$. (Existence of Additive Inverse)
\item $a\vec{u} \in \mathcal{V}$ (Closure under Scalar Multiplication: Multiplying a vector by any scalar (real number for a real vector space) is defined and the output vector is still in the vector space.)
\item $a(\vec{u} + \vec{v}) = a\vec{u} + a\vec{v}$ (Distributive Property of Scalar Multiplication)
\item $(a+b)\vec{u} = a\vec{u} + b\vec{u}$ (Distributive Property of Scalar Multiplication)
\item $a(b\vec{u}) = (ab)\vec{u}$ (Associative Property of Scalar Multiplication)
\item $1\vec{u} = \vec{u}$ (The real number $1$ as the Multiplicative Identity)
\end{enumerate}
\end{defn}
The real $n$-space $\mathbb{R}^n$ satisfies all the axioms above and is finite-dimensional, particularly $n$-dimensional (the notion of dimension here should be intuitive, but we will go through it more precisely later), with addition and scalar multiplication being the usual ones as defined in Section \ref{section:vectoraddmul}, and the zero vector is simply $\textbf{0} = (0,0,0,\ldots,0)^T$ with $n$ zeros. We will skip the detailed proof but interested readers can try to justify all of them. To build the definition of a vector space from axioms allows the generalization and application of the concepts of vector space to more abstract structures. However, for most usages, we will focus on $\mathbb{R}^n$, and the vector space axioms are provided above mainly for reference. We defer the treatment of complex vector spaces to Chapter \ref{chap:complex}.

\subsection{Subspaces of $\mathbb{R}^n$}

It will be very boring if we consider only the entire $\mathbb{R}^n$ as a vector space. In last chapter, we show that geometrically there can be lower-dimensional shapes like lines/planes/hyperplanes residing in $\mathbb{R}^n$. This raises the question if we can similarly find \index{Subspace}\keywordhl{subspaces} embedded in $\mathbb{R}^n$ that is a subset of $\mathbb{R}^n$ which still fulfills the vector space axioms such that it is a vector space in its own right. Nevertheless, to determine if a subset of vector space is a subspace, we don't need to check all the ten axioms but rather just two of them.
\begin{thm}[Criteria for a Subspace]
\label{thm:subspacecriteria}
If $\mathcal{W}$ is a non-empty subset of a (real) vector space $\mathcal{V}$ (i.e. $\mathcal{W} \subseteq \mathcal{V}$), then $\mathcal{W}$ is called a (real) subspace of $\mathcal{V}$ if the following criteria are satisfied:
\begin{enumerate}
\item For any $\vec{u}, \vec{v} \in \mathcal{W}$, $\vec{u} + \vec{v} \in \mathcal{W}$ (Closed under Addition)
\item For any scalar $a$ ($\in \mathbb{R}$) and $\vec{u} \in \mathcal{W}$, $a\vec{u} \in \mathcal{W}$ (Closed under Scalar Multiplication), particularly when $a = 0$, $0\vec{u} = \textbf{0} \in \mathcal{W}$ so that a subspace always contains the zero vector of $\mathcal{V}$.
\end{enumerate}
These are the same conditions of (1) and (6) in Definition \ref{defn:realvecspaceaxiom}. Or equivalently, for any $\vec{u}, \vec{v} \in \mathcal{W}$ and two scalars $a$ and $b$, $a\vec{u} + b\vec{v} \in \mathcal{W}$.
\end{thm}
\begin{exmp}
Consider the following subsets of $\mathbb{R}^2$ and decide if they are subspaces of $\mathbb{R}^2$ by verifying the two criteria listed in Theorem \ref{thm:subspacecriteria}.
\begin{enumerate}[label=(\alph*)]
\item The line $x - 2y = 0$,
\item The $y$-axis,
\item The positive $y$-axis,
\item The line $2x + y = 1$,
\item The parabola $y = x^2$,
\item The point $(-1,1)^T$,
\item The first quadrant $x > 0$, $y > 0$,
\item The origin $\textbf{0} = (0,0)^T$,
\item $\mathbb{R}^2$ itself.
\end{enumerate}
\end{exmp}
\begin{solution}
\begin{enumerate}[label=(\alph*)]
\item The vector form of the line is $\mathcal{W} = \{(x,y)^T = t(2,1)^T \mid -\infty < t < \infty\}$. To check the first condition, let's say $\vec{u} = t_1(2,1)^T \in \mathcal{W}$ and $\vec{v} = t_2(2,1)^T \in \mathcal{W}$ for some $t_1$ and $t_2$, then $\vec{u} + \vec{v} = t_1(2,1)^T + t_2(2,1)^T = (t_1 + t_2)(2,1)^T = s(2,1)^T \in \mathcal{W}$ also lies on the straight line where $s = t_1 + t_2$, so $\mathcal{W}$ is closed under addition. To check the second condition, this time we simply let $\vec{u} = t(2,1)^T \in \mathcal{W}$. Subsequently, $a\vec{u} = at(2,1)^T = r(2,1)^T \in \mathcal{W}$, for any scalar $a$ and $r = at$, so it is closed under scalar multiplication. Hence the line $x-2y = 0$ is a subspace of $\mathbb{R}^2$.
\item Same arguments as above but with $\mathcal{W} = \{(x,y)^T = t(0,1)^T \mid -\infty < t < \infty\}$, so the $y$-axis is also a subspace of $\mathbb{R}^2$.
\item For any point on the positive $y$-axis, multiplying it by a negative number places it on the negative $y$-axis instead, so it is not closed under scalar multiplication and thus not a subspace of $\mathbb{R}^2$. 
\item Denote the collection of points on the line as $\mathcal{W}$. Pick $\vec{u} = (1, -1)^T \in \mathcal{W}$ and $\vec{v} = (0, 1)^T \in \mathcal{W}$, then $\vec{u} + \vec{v} = (1, 0)^T \notin \mathcal{W}$ as $2(1) + (0) = 2 \neq 1$, so it is not closed under addition and fails to be a subspace of $\mathbb{R}^2$.
\item Denote the collection of points on the parabola as $\mathcal{W}$. Pick $\vec{u} = (1,1)^T \in \mathcal{W}$ and $\vec{v} = (2,4)^T \in \mathcal{W}$, then $\vec{u} + \vec{v} = (3,5)^T \notin \mathcal{W}$ is apparently not on the parabola, so it is not closed under addition and can't be a subspace of $\mathbb{R}^2$.
\item It is easy to see that it fails to be closed under either addition or scalar multiplication (for example, take $a(-1,1)^T$ with $a\neq 1$) and is not a subspace of $\mathbb{R}^2$.
\item Denote the collection of points on the first quadrant as $\mathcal{W}$. Pick $\vec{u} = (1,1)^T \in \mathcal{W}$, then $(-1)\vec{u} = -(1,1)^T = (-1,-1)^T \notin \mathcal{W}$ is outside the first quadrant. Therefore, it is not closed under scalar multiplication and hence not a subspace of $\mathbb{R}^2$.
\item It trivially satisfies the two criteria ($\textbf{0}$ is the only element in the set, $\textbf{0} + \textbf{0} = \textbf{0}$ and $a\textbf{0} = \textbf{0}$ for any scalar $a$) and is a subspace of $\mathbb{R}^2$.
\item $\mathbb{R}^2$ is a vector space to begin with and technically a subset of itself (it contains itself, although not a proper one) so by definition it is a subspace of $\mathbb{R}^2$.
\end{enumerate}
\end{solution}
Generalizing the above discussion, we can easily infer that for $\mathbb{R}^2$, only the origin (the zero subspace), an infinitely long straight line that passes through the origin, or $\mathbb{R}^2$ itself can be its subspaces. We often use the phrase \index{Subspace!Proper Subspace}\keywordhl{proper subspaces} to exclude the accommodating vector space itself ($\mathbb{R}^2$ in this case). For any $\mathbb{R}^n$, the \index{Subspace!Zero Subspace}\keywordhl{zero subspace} $\{\textbf{0}\}$ and \index{Subspace!Improper Subspace}\keywordhl{improper subspace} $\mathbb{R}^n$ are always two subspaces of it. \\
\\
Short Exercise: Determine if the following subsets of $\mathbb{R}^3$ is a subspace of $\mathbb{R}^3$.\footnote{Yes, No, Yes, No, Yes, No, Yes, No, No. In fact, all possible subspaces of $\mathbb{R}^3$ are $\{\textbf{0}\}$, any infinitely long line/extending plane through the origin and $\mathbb{R}^3$ itself.}
\begin{enumerate}[label=(\alph*)]
\item The origin $\textbf{0} = (0,0,0)^T$,
\item The point $(1,2,3)^T$,
\item The line $(x,y,z)^T = t(-1, 1, 2)^T$ for any scalar $t$,
\item The line $(x,y,z)^T = (1, -1, 3) + t(1, 2, -1)^T$ for any scalar $t$,
\item The plane $x + 2y - 3z = 0$,
\item The plane $x + y + 4z = 5$,
\item $\mathbb{R}^3$ itself,
\item The sphere $x^2 + y^2 + z^2 = 1$,
\item The cone $x^2 + y^2 = z^2$.
\end{enumerate}
From now on, we assume all vector (sub)spaces mentioned are finite-dimensional.

\subsection{Span by Linear Combinations of Vectors}
\label{section:span}
The last section sees subspaces from a top-down perspective as some subsets of a larger vector space. Here, we are going to take another look at them with a bottom-up perspective, about how to generate a subspace of $\mathbb{R}^n$ from some vectors of it. To do so, we need to first understand what is a \index{Linear Combination}\keywordhl{linear combination} of vectors.
\begin{defn}[Linear Combination of Vectors]
\label{defn:linearcomb}
A linear combination of vectors $\vec{u}_1, \vec{u}_2, \vec{u}_3, \ldots, \vec{u}_q \in \mathcal{V}$ where $\mathcal{V}$ is some vector space has the form of
\begin{align*}
\sum_{j=1}^q c_j\vec{u}_j = c_1\vec{u}_1 + c_2\vec{u}_2 + c_3\vec{u}_3 + \cdots + c_q\vec{u}_q
\end{align*}
where the coefficients $c_j$ are some scalars (real numbers for a real vector space) and the amount of vectors $q$ is finite.
\end{defn}
A simple example would be, if there are two vectors $\vec{u} = (1,2)^T$ and $\vec{v} = (3,4)^T \in \mathbb{R}^2$, then $\vec{h} = (5,6)^T \in \mathbb{R}^2$ can be written as a linear combination of $\vec{u}$ and $\vec{v}$ because $\vec{h} = (5,6)^T = -(1,2)^T + 2(3,4)^T = -\vec{u} + 2\vec{v}$. \\
\\
Short Exercise: If $\vec{h} = (1,4)^T$ instead, express $\vec{h}$ as a linear combination of $\vec{u}$ and $\vec{v}$.\footnote{$(1,4)^T = 4(1,2)^T - (3,4)^T$.}\par

Attentive readers may realize that the short exercise above can be considered as a task to find out the solution for the system
\begin{align*}
\begin{bmatrix}
1 & 3 \\
2 & 4 \\
\end{bmatrix}
\begin{bmatrix}
c_1 \\
c_2
\end{bmatrix} =
\begin{bmatrix}
1 \\
4
\end{bmatrix}
\end{align*}
Generalizing this, to decide whether a vector $\vec{h} \in \mathbb{R}^n$ can be written as the linear combination of other vectors $\vec{u}_j \in \mathbb{R}^n$ in some set, $j = 1, 2, \ldots, q$, is equivalent to determining whether the linear system $A\vec{x} = \vec{h}$ has a solution, where $A$ equals to $[\vec{u}_1|\vec{u}_2|\cdots|\vec{u}_q]$ (writing out $\vec{u}_j$ column by column). Here, the matrix product $A\vec{x}$ is a compact way to represent a linear combination of the column vectors that compose $A$.
\begin{proper}
\label{proper:linearcombmatrix}
A linear combination $c_1\vec{u}_1 + c_2\vec{u}_2 + c_3\vec{u}_3 + \cdots + c_q\vec{u}_q$ made up of some vectors $\vec{u}_1, \vec{u}_2, \vec{u}_3, \ldots, \vec{u}_q \in \mathbb{R}^n$ as the one in Definition \ref{defn:linearcomb}, can be expressed by the matrix product $A\vec{x}$, where
\begin{align*}
&A = [\vec{u}_1|\vec{u}_2|\vec{u}_3|\cdots|\vec{u}_q]
&\vec{x} =
\begin{bmatrix}
c_1 \\
c_2 \\
c_3 \\
\vdots \\
c_q
\end{bmatrix}
\end{align*}
\end{proper}
From this perspective, the first/second/last column of a matrix $A$ can be formulated as
\begin{align*}
A&
\begin{bmatrix}
1 \\
0 \\
0 \\
\vdots \\
0
\end{bmatrix}
&
A&
\begin{bmatrix}
0 \\
1 \\
0 \\
\vdots \\
0
\end{bmatrix}
&
A&
\begin{bmatrix}
0 \\
0 \\
0 \\
\vdots \\
1
\end{bmatrix}
\end{align*}
and similarly for other columns. For example,
\begin{align*}
\begin{bmatrix}
5 & \color{red}{1} & -1 & 2 \\
2 & \color{red}{3} & 0 & 7 \\
4 & \color{red}{-2} & 3 & 1
\end{bmatrix}
\begin{bmatrix}
0 \\
\color{red}{1} \\
0 \\
0
\end{bmatrix} 
&=
\begin{bmatrix}
\color{red}{1} \\
\color{red}{3} \\
\color{red}{-2}
\end{bmatrix} \\
\begin{bmatrix}
\color{Green}{5} & \color{red}{1} & \color{blue}{-1} & 2 \\
\color{Green}{2} & \color{red}{3} & \color{blue}{0} & 7 \\
\color{Green}{4} & \color{red}{-2} & \color{blue}{3} & 1
\end{bmatrix}
\begin{bmatrix}
\color{Green}{-1} \\
\color{red}{2} \\
\color{blue}{3} \\
0
\end{bmatrix} 
&= 
\begin{bmatrix}
\color{Green}{5} & \color{red}{1} & \color{blue}{-1} & 2 \\
\color{Green}{2} & \color{red}{3} & \color{blue}{0} & 7 \\
\color{Green}{4} & \color{red}{-2} & \color{blue}{3} & 1
\end{bmatrix}
\left(
\begin{bmatrix}
\color{Green}{-1} \\
0 \\
0 \\
0
\end{bmatrix} 
+
\begin{bmatrix}
0 \\
\color{red}{2} \\
0 \\
0
\end{bmatrix} 
+
\begin{bmatrix}
0 \\
0 \\
\color{blue}{3} \\
0
\end{bmatrix} 
+
\begin{bmatrix}
0 \\
0 \\
0 \\
0
\end{bmatrix} 
\right) \\
&=
(\textcolor{Green}{-1})
\begin{bmatrix}
5 \\
2 \\
4
\end{bmatrix}
+
(\textcolor{red}{2})
\begin{bmatrix}
1 \\
3 \\
-2
\end{bmatrix}
+
(\textcolor{blue}{3})
\begin{bmatrix}
-1 \\
0 \\
3
\end{bmatrix}
+
0
\begin{bmatrix}
2 \\
7 \\
1
\end{bmatrix}
=
\begin{bmatrix}
-6 \\
4 \\
1
\end{bmatrix}
\end{align*}
\begin{exmp}
Show that $\vec{h} = (2,4,3)^T$ cannot be written as a linear combination of $\vec{u}_1 = (-1, 0, 1)^T$ and $\vec{u}_2 = (1, 1, 0)^T$.
\end{exmp}
\begin{solution}
From the discussion above, the objective is equivalent to showing that the linear system
\begin{align*}
\begin{bmatrix}
-1 & 1 \\
0 & 1 \\
1 & 0
\end{bmatrix}
\begin{bmatrix}
c_1 \\
c_2
\end{bmatrix}
=
\begin{bmatrix}
2 \\
4 \\
3
\end{bmatrix}
\end{align*}
has no solution. We can apply the method of Gaussian Elimination as demonstrated in Section \ref{subsection:SolLinSysGauss}, which leads to
\begin{align*}
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}|wc{10pt}@{}}
-1 & 1 & 2\\
0 & 1 & 4\\
1 & 0 & 3
\end{array}\right] 
&\to
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}|wc{10pt}@{}}
1 & 0 & 3 \\
0 & 1 & 4 \\
-1 & 1 & 2
\end{array}\right] & R_1 \leftrightarrow R_3 \\
&\to
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}|wc{10pt}@{}}
1 & 0 & 3 \\
0 & 1 & 4 \\
0 & 0 & 1
\end{array}\right] & R_3 + R_1 - R_2 \to R_3
\end{align*}
The last row is inconsistent and hence there is no solution to the linear system and $\vec{h}$ cannot be expressed by a linear combination of $\vec{u}_1$ and $\vec{u}_2$.
\end{solution}
With the idea of linear combination, we can define the \keywordhl{span} generated by a (finite) set of vectors.
\begin{defn}[Span]
\label{defn:span}
The span of $q$ vectors in a set $\mathcal{S} = \{\vec{u}_1, \vec{u}_2, \vec{u}_3, \ldots, \vec{u}_q\}$ where all $\vec{u}_j \in \mathcal{V}$ are from the same vector space, is consisted of all their possible linear combinations as given in Definition \ref{defn:linearcomb}, and is denoted as
\begin{align*}
\text{span}(\mathcal{S}) = \{\sum_{j=1}^{q} c_j\vec{u}_j \mid \text{for any scalar $c_j$ and $\vec{u}_j \in \mathcal{S}$}\}
\end{align*}
again we will limit ourselves to the cases where the coefficients $c_j$ are real and $q$ is finite. If $\vec{u}_j \in \mathbb{R}^n$, then as suggested by Properties \ref{proper:linearcombmatrix}, the span can be thought in the form of 
\begin{align*}
\text{span}(\mathcal{S}) = \{A\vec{x} \mid \vec{x} \in \mathbb{R}_q\}
\end{align*}
with $A = [\vec{u}_1|\vec{u}_2|\cdots|\vec{u}_q]$ and
$\vec{x} = (c_1, c_2, \ldots, c_q)^T$ being the coefficient vector.
\end{defn}
For example, the span of $\mathcal{S}_1 = \{(-1,1)^T\}$ is simply $t(-1,1)^T$ where $-\infty < t < \infty$, or the line $y = -x$. The span of $\mathcal{S}_2 = \{(1,0,2)^T, (0,1,-1)^T\}$ (notice that the two vectors are not a constant multiple of each other) is $s(1,0,2)^T + t(0,1,-1)^T$ where $-\infty < s,t < \infty$, or the plane $2x - y - z = 0$ (see Section \ref{section:vecgeohighdim}). Adding more vectors in the spanning set does not always imply the span will be larger. For example, the span of $\mathcal{S}_3 = \{(1,0)^T, (0,1)^T\}$ and $\mathcal{S}_4 = \{(1,0)^T, (0,1)^T, (1,1)^T, (1,-1)^T\}$ are both $\mathbb{R}^2$ obviously. This issue will be addressed in later sections.
\begin{exmp}
\label{exmp:S3S4}
Show that any vector in $\mathbb{R}^2$ can be written as infinitely many different linear combinations of the four vectors in the set $\mathcal{S}_4$ mentioned above.
\end{exmp}
\begin{solution}
This is to decide that the linear system
\begin{align*}
\begin{bmatrix}
1 & 0 & 1 & 1 \\
0 & 1 & 1 & -1
\end{bmatrix}
\begin{bmatrix}
c_1 \\
c_2 \\
c_3 \\
c_4
\end{bmatrix}
=
\begin{bmatrix}
x \\
y
\end{bmatrix}    
\end{align*}
has infinitely many solutions for any pair of $(x,y)$. The augmented form
\begin{align*}
\left[\begin{array}{@{}cccc|c@{}}
1 & 0 & 1 & 1 & x \\
0 & 1 & 1 & -1 & y
\end{array}\right]
\end{align*}
is already in reduced row echelon form. There is a corresponding pivot for both $x$ and $y$, and no zero row is present, which means that there would not be any inconsistency and we can always construct a family of solutions by assigning free variables to unknowns of the non-pivotal columns, let's say $c_3 = s$ and $c_4 = t$. Then $c_1 = x - s - t$, $c_2 = y - s + t$. As a result, any linear combination in the form of
\begin{align*}
(x-s-t)(1,0)^T + (y - s + t)(0,1)^T + s(1,1)^T + t(1,-1)^T
\end{align*}
will produce the vector $(x,y)^T$ as desired with any value of $s$ and $t$, and there are infinitely many of them. This example shows that a vector in the span (in this case $\mathbb{R}^2$) generated by a set of vectors ($\mathcal{S}_4$) can possibly be written as more than one linear combinations of the constituent vectors in the set.
\end{solution}
An important property of spans is that they are subspaces and vice versa. This fact integrates the top-down (it is a subset of a larger vector space) and bottom-up (it is formed by linear combinations of vectors) view of subspaces.
\begin{proper}
\label{proper:subspace_n_span}
The span of a subset of vectors in $\mathcal{V}$ is a subspace of $\mathcal{V}$. A subspace of $\mathcal{V}$ is always some span (not necessarily unique) of vectors $\in \mathcal{V}$.
\end{proper}
\begin{proof}
Span $\rightarrow$ Subspace: We check if the two criteria in Theorem \ref{thm:subspacecriteria} hold for a span. Let the span be the one defined in Definition \ref{defn:span}, then any vector in the span can be written as $\sum_{j=1}^{q} c_j\vec{u}_j$ for some constants $c_j$. Let $\vec{v} = \sum_{j=1}^{q} \alpha_j\vec{u}_j \in \text{span}(\mathcal{S})$ and $\vec{w} = \sum_{j=1}^{q} \beta_j\vec{u}_j \in \text{span}(\mathcal{S})$ for some sets of constants $\alpha_j$ and $\beta_j$, then their sum $\vec{v} + \vec{w} = \sum_{j=1}^{q} \alpha_j\vec{u}_j + \sum_{j=1}^{q} \beta_j\vec{u}_j = \sum_{j=1}^{q} (\alpha_j + \beta_j)\vec{u}_j = \sum_{j=1}^{q} \gamma_j\vec{u}_j \in \text{span}(\mathcal{S})$ where $\gamma_j = \alpha_j + \beta_j$ is seen to be closed under addition. Similarly, writing $a\vec{w} = a(\sum_{j=1}^{q} \beta_j\vec{u}_j) = \sum_{j=1}^{q} (a\beta_j)\vec{u}_j \in \text{span}(\mathcal{S})$ shows that the span is closed under scalar multiplication and we are done. Subsequently, we say $\mathcal{W} = \text{span}(\mathcal{S})$ is a subspace \textit{generated} by the set $\mathcal{S}$ and $\mathcal{S}$ is known as a \index{Spanning Set}\index{Generating Set}\keywordhl{spanning/generating set} for $\mathcal{W}$. \\
\\
Subspace $\rightarrow$ Span: We postpone the proof which will come naturally as we learn about linear independence and coordinate basis in the upcoming sections. But before that, we would benefit from showing a related result.
\end{proof}
\begin{proper}
\label{proper:WcontainsspanS}
Any subspace of $\mathcal{V}$ that contains a subset of vectors $\mathcal{S}$ from $\mathcal{V}$ also contains the span of $\mathcal{S}$.
\end{proper}
\begin{proof}
Let $\mathcal{S} = \{\vec{u}_1, \vec{u}_2, \ldots, \vec{u}_q\}$. For any vector $\vec{v} \in \text{span}(\mathcal{S})$, by Definition \ref{defn:span}, it can be written as some linear combination $\vec{v} = c_1\vec{u}_1 + c_2\vec{u}_2 + \cdots + c_q\vec{u}_q$ where $c_j$ are some constants and $\vec{u}_j \in \mathcal{S}$. Denote the subspace that contains $\mathcal{S}$ by $\mathcal{W}$. Since $\mathcal{S} \subseteq \mathcal{W}$, $\vec{u}_1, \vec{u}_2 \ldots, \vec{u}_q \in \mathcal{W}$ as well. By recursively applying the alternative version of Theorem \ref{thm:subspacecriteria} to add up the $\vec{u}_j$\footnote{By the theorem, $c_1\vec{u}_1 + c_2\vec{u}_2$ is in the subspace. Using the theorem again, $(c_1\vec{u}_1 + c_2\vec{u}_2) + c_3\vec{u}_3$ is also in the subspace, and so on.}, $\vec{v} = c_1\vec{u}_1 + c_2\vec{u}_2 + \cdots + c_q\vec{u}_q$ is shown to be included in $\mathcal{W}$. Since this can be done for any $\vec{v} \in \text{span}(\mathcal{S})$, $\text{span}(\mathcal{S}) \subseteq \mathcal{W}$.
\end{proof}

% $\mathcal{W}$ as a subspace of $\mathbb{R}^n$ contains at most $n$ linearly independent vectors. Assume $\mathcal{W}$ is non-empty and take any non-zero vector in $\mathcal{W}$, denote it by $\vec{u}_1$. The span of $\mathcal{S} = \{\vec{u}_1\}$ is itself a subspace of the subspace $\mathcal{W}$ by noting that it is a subset of $\mathcal{W}$ and extending the first part of Properties \ref{proper:subspace_n_span}. If this subspace of subspace is exactly $\mathcal{W}$, in the sense that no other $\vec{v} \in \mathcal{W}$ is not included by $\text{span}(\mathcal{S})$, then we are done because this subspace is a span by construction. Otherwise, take another non-zero vector $\vec{u}_2 \in \mathcal{W}$ which is linearly independent of $\vec{u}_1$ and add it to $\mathcal{S}$. Now $\text{span}(\mathcal{S}) = \text{span}(\{\vec{u}_1, \vec{u}_2\})$ is enlarged by one dimension but still a subspace of $\mathcal{W}$ and we can check if it coincides with $\mathcal{W}$, otherwise, repeat the procedure with $\vec{u}_3, \vec{u}_4, \ldots$ until $\text{span}(\mathcal{S}) = \mathcal{W}$ then we can stop. Remember we can at most add up to $\vec{u}_n$ since $\mathcal{W}$ contains at most $n$ linearly independent vectors, so in the middle of somewhere we would done with $\vec{u}_p$, where $p \leq n$, and hence we know that $\mathcal{W}$ is some span (and a $p$-dimensional subspace).

\subsection{Linear Independence}
\label{section:linearind}

Now we are going to tackle the problem of linear independence, which has profound implications in linear algebra. Given a set of vectors, if every one of them can not be expressed as the linear combination of other members, or speaking loosely, each of them is not "dependent" on other vectors, then such a set of vectors is said to be \index{Linearly Independent}\keywordhl{linearly independent}. Otherwise, if at least one of them can be expressed as some linear combination of other vectors, then the set is known as \index{Linearly Dependent}\keywordhl{linearly dependent}.\\
\\
Indeed, to check linear independence of $q$ vectors, one may directly show that for every vector $\vec{u}_j$ in the set, $j = 1,2,3,\ldots,q$, it cannot be written as the linear combination of other vectors $\vec{u}_k$ in the set, $k \neq j$ . A slightly easier way is looking at the linear combination of just the first $j-1$ vectors (from $\vec{u}_1$ up to $\vec{u}_{j-1}$) for $\vec{u}_j$. However, it is not plausible if the amount of vectors is large. Fortunately, we have a theorem which significantly simplifies our work.
\begin{thm}
\label{thm:linearindep}
For a set of vectors $\mathcal{S} = \{\vec{u}_1, \vec{u}_2, \vec{u}_3, \ldots, \vec{u}_q\}$ where $\vec{u}_j \in \mathcal{V}$, they are linearly independent if and only if, the linear system $c_1\vec{u}_1 + c_2\vec{u}_2 + c_3\vec{u}_3 + \cdots + c_q\vec{u}_q = \textbf{0}$ has the trivial solution where all the coefficients $c_j = \textbf{0}$ are zeros as the unique solution. Using the matrix notation in Properties \ref{proper:linearcombmatrix} when $\vec{u}_j \in \mathbb{R}^n$, it means that the homogeneous system $A\vec{x} = \textbf{0}$ where $A = [\vec{u}_1|\vec{u}_2|\vec{u}_3|\cdots|\vec{u}_q]$ only has the trivial solution $c_j = \vec{x} = \textbf{0}$.
\end{thm}
\begin{proof}
The "if" direction: We need to show that $c_j = \textbf{0}$ being the only solution to $c_1\vec{u}_1 + c_2\vec{u}_2 + c_3\vec{u}_3 + \cdots + c_q\vec{u}_q = \textbf{0}$ implies that $\vec{u}_1, \vec{u}_2, \vec{u}_3, \ldots, \vec{u}_q$ are linearly independent. We can prove the contrapositive where the opposite of the conclusion, $\vec{u}_1, \vec{u}_2, \vec{u}_3, \ldots, \vec{u}_q$ are linearly dependent, implies there is non-trivial solution to the equation. This requires that at least one of these vectors, without the loss of generality, let's say $\vec{u}_1$, can be written as the linear combination of other vectors in the form of
\begin{align*}
\vec{u}_1 = a_2\vec{u}_2 + a_3\vec{u}_3 + \cdots + a_q\vec{u}_q
\end{align*}
Rearranging gives 
\begin{align*}
\vec{u}_1 - a_2\vec{u}_2 - a_3\vec{u}_3 - \cdots - a_q\vec{u}_q = \textbf{0}
\end{align*}
which shows that the coefficients $c_1 = 1, c_2 = -a_2, c_3 = -a_3, \ldots, c_q = -a_q$ is another solution other than $c_j = \textbf{0}$ to $c_1\vec{u}_1 + c_2\vec{u}_2 + c_3\vec{u}_3 + \cdots + c_q\vec{u}_q = \textbf{0}$ (particularly for $c_1$). \\
The "only if" direction: We want to show the converse that linear independence of $\vec{u}_1, \vec{u}_2, \vec{u}_3, \ldots, \vec{u}_q$ only permits $c_j = \textbf{0}$ as the unique solution to $c_1\vec{u}_1 + c_2\vec{u}_2 + c_3\vec{u}_3 + \cdots + c_q\vec{u}_q = \textbf{0}$. To do so, we can again resort to its contrapositive, i.e. the existence of an alternative solution of $c_j = a_j$ which are not all zeros to the linear system, means that the vectors in $\mathcal{S}$ are linearly dependent. Choose one of the $a_j$ that is not zero and denote it by $a_k$, then
\begin{align*}
a_1\vec{u}_1 + \cdots + a_{k-1}\vec{u}_{k-1} + a_k\vec{u}_k + a_{k+1}\vec{u}_{k+1} + \cdots + a_q\vec{u}_q = \textbf{0} \\
\vec{u}_k = -\frac{a_1}{a_k}\vec{u}_1 - \cdots - \frac{a_{k-1}}{a_k}\vec{u}_{k-1} - \frac{a_{k+1}}{a_k}\vec{u}_{k+1} - \cdots - \frac{a_q}{a_k}\vec{u}_q 
\end{align*}
where we have divided the equation by the non-zero $a_k$ to avoid dividing by zero and rearranged to show that $\vec{u}_k$ can be written in some linear combination of other vectors as constructed by above, and thus vectors in $\mathcal{S}$ are linearly dependent. 
\end{proof}
\begin{exmp}
\label{exmp:exmplinearindep}
Determine if $\vec{u} = (1,2,1)^T$, $\vec{v} = (3,4,2)^T$, $\vec{w} = (6,8,1)^T$ are linearly independent.
\end{exmp}
By Theorem \ref{thm:linearindep}, this is equivalent to decide if $A\vec{x} = \textbf{0}$, where $A = [\vec{u}|\vec{v}|\vec{w}]$ has the trivial solution as the only solution. With the help of Theorem \ref{thm:sqlinsysunique}, we know that it is equivalent to check if $\text{det}(A)$ is zero or not. Since
\begin{align*}
|A| &=
\begin{vmatrix}
1 & 3 & 6\\
2 & 4 & 8 \\
1 & 2 & 1
\end{vmatrix} = 6 \neq 0
\end{align*}
We conclude that $A\vec{x} = \textbf{0}$ only has the trivial solution $\vec{x} = \textbf{0}$ and these three vectors are linearly independent. \\
\\
Short Exercise: Redo the above example with $\vec{u} = (1,1,3)^T$, $\vec{v} = (1,3,2)^T$, $\vec{w} = (2,8,3)^T$.\footnote{The determinant of $A = [\vec{u}|\vec{v}|\vec{w}]$ in the case is $|A| = 0$, and hence by the remark for Theorem \ref{thm:sqlinsysunique} the linear system $A\vec{x} = \textbf{0}$ has infinitely many solutions, and these three vectors are linearly dependent by Theorem \ref{thm:linearindep}.}\par
As a corollary, any set containing the zero vector $\textbf{0}$ must be linearly dependent. (Why?)\footnote{For any such a set $\mathcal{S}_0 = \{\vec{u}_1, \vec{u}_2, \ldots, \textbf{0}\}$, the linear system $c_1\vec{u}_1 + c_2\vec{u}_2 + \cdots + c_0\textbf{0} = \textbf{0}$ has a family of infinitely many solution with $c_j = 0$ for $j \neq 0$ and any value of $c_0$, which by Theorem \ref{thm:linearindep} they are linearly dependent.} Furthermore, if a vector can be expressed as a linear combination of some linearly independent vectors, this linear combination must be unique in terms of these vectors. To be more precise, we have the following statement.
\begin{proper}
\label{proper:lincombofspan}
For a set of vectors $\mathcal{S} = \{\vec{u}_1, \vec{u}_2, \vec{u}_3, \ldots, \vec{u}_q\}$, $\vec{u}_j \in \mathcal{V}$ which are linearly independent, any vector $\vec{v} \in \text{span}(\mathcal{S})$ in their span can be written as a unique linear combination of the vectors in $\mathcal{S}$. Otherwise, if the vectors in $\mathcal{S}$ are linearly dependent, there will be infinitely many such linear combinations to assemble $\vec{v}$.
\end{proper}
\begin{proof}
Since $\vec{v}$ already belongs to $\text{span}(\mathcal{S})$, it must be possible to express $\vec{v}$ as some linear combination(s) of vectors $\vec{u}_1, \vec{u}_2, \vec{u}_3, \ldots, \vec{u}_q$ in $\mathcal{S}$ by Definition \ref{defn:span}. Now it suffices to show that it is unique. Assume the contrary that there are two distinct linear combinations of vectors in $\mathcal{S}$ that represent $\vec{v}$, and hence we can express it by
\begin{align*}
\vec{v} &= d_1\vec{u}_1 + d_2\vec{u}_2 + d_3\vec{u}_3 + \cdots + d_q\vec{u}_q \\
&= g_1\vec{u}_1 + g_2\vec{u}_2 + g_3\vec{u}_3 + \cdots + g_q\vec{u}_q
\end{align*}
where $d_j$, $g_j$ are two sets of coefficients that happen to be not exactly the same. Subtracting one expression by another leads to
\begin{align*}
\begin{aligned}
&\quad (d_1\vec{u}_1 + d_2\vec{u}_2 + d_3\vec{u}_3 + \cdots + d_q\vec{u}_q) \\
& -(g_1\vec{u}_1 + g_2\vec{u}_2 + g_3\vec{u}_3 + \cdots + g_q\vec{u}_q)
\end{aligned}
&= \vec{v} - \vec{v} \\
(d_1 - g_1)\vec{u}_1 + (d_2 - g_2)\vec{u}_2 + (d_3 - g_3)\vec{u}_3 + \cdots + (d_q - g_q)\vec{u}_q &= \textbf{0} 
\end{align*}
Since $d_j$, $g_j$ are assumed to be not identical, it is a non-trivial solution to the equation $c_1\vec{u}_1 + c_2\vec{u}_2 + c_3\vec{u}_3 + \cdots + c_q\vec{u}_q = \textbf{0}$, where $c_j = d_j - g_j$ are not all zeros. This contradicts our assumption and hence the linear combination of $\vec{u}_1, \vec{u}_2, \vec{u}_3, \ldots, \vec{u}_q$ to generate $\vec{v}$ must be unique.
%In cases of where $\vec{u}_1, \vec{u}_2, \vec{u}_3, \ldots, \vec{u}_q$ are linearly dependent, start from any linear combination $\vec{v} = d_1\vec{u}_1 + d_2\vec{u}_2 + d_3\vec{u}_3 + \cdots + d_q\vec{u}_q$. By Theorem \ref{thm:linearindep}, we have some non-trivial solution of $c_j$ that are not all zeros to the equation $c_1\vec{u}_1 + c_2\vec{u}_2 + c_3\vec{u}_3 + \cdots + c_q\vec{u}_q = \textbf{0}$. Adding this non-trivial solution times a parameter $t$ to the linear combination we have begun with, gives
%\begin{align*}
%\vec{v} + t\textbf{0} &= 
%\begin{aligned}
%& (d_1\vec{u}_1 + d_2\vec{u}_2 + d_3\vec{u}_3 + \cdots + d_q\vec{u}_q) \\
%&+ t(c_1\vec{u}_1 + c_2\vec{u}_2 + c_3\vec{u}_3 + \cdots + c_q\vec{u}_q)
%\end{aligned} \\
%\vec{v} &= (d_1 + tc_1)\vec{u}_1 + (d_2 + tc_2)\vec{u}_2 + (d_3 + tc_3)\vec{u}_3 + \cdots + (d_q + tc_q)\vec{u}_q
%\end{align*}
%This expresses $\vec{v}$ in infinitely many linear combinations of $\vec{u}_j$ as $t$ is varied.
\end{proof}
While we would not show the proof here (it is actually not difficult), the second part of the property above can be observed in Example \ref{exmp:S3S4} where $\mathcal{S}_4$ is clearly linearly dependent. Another property that closely parallels the above one is
\begin{proper}
\label{proper:addvdepend}
For a linearly independent set $\mathcal{S}$ as in Properties \ref{proper:lincombofspan}, and a vector $\vec{v} \in \mathcal{V}$ that is not already in $\mathcal{S}$, $\mathcal{S} \cup \{\vec{v}\}$ is linearly dependent if and only if $\vec{v} \in \text{span}(\mathcal{S})$.
\end{proper}
\begin{proof}
The "if" direction is trivial by the definition of span and linear dependence. For the converse, if $\mathcal{S} \cup \{\vec{v}\}$ is linearly dependent, then there is non-trivial solution $c_j = d_j$ where $d_j$ are not all zeros to the equation $c_1\vec{u}_1 + c_2\vec{u}_2 + \cdots + c_q\vec{u}_q + c_v\vec{v} = \textbf{0}$ by Theorem \ref{thm:linearindep}. Since $\mathcal{S}$ is linearly independent, $d_v \neq 0$, for otherwise $d_v = 0$ and then at least one of the $c_j = d_j$ ($j \neq v$) will be non-zero and lead to a non-trivial solution to $c_1\vec{u}_1 + c_2\vec{u}_2 + \cdots + c_q\vec{u}_q = \textbf{0}$ which contradicts the linear independence of $\mathcal{S}$, so we have $d_1\vec{u}_1 + d_2\vec{u}_2 + \cdots + d_q\vec{u}_q + d_v\vec{v} = \textbf{0}$ and because $d_v \neq 0$ we can obtain
\begin{align*}
\vec{v} &= -\frac{1}{d_v}(d_1\vec{u}_1 + d_2\vec{u}_2 + \cdots + d_q\vec{u}_q)
\end{align*}
showing that $\vec{v}$ is a linear combination of $\vec{u}_j \in \mathcal{S}$.
\end{proof}
Including our earlier discussion in Section \ref{subsection:SolLinSysGauss}, Theorem \ref{thm:linearindep} gives some interesting results.
\begin{enumerate}
\item If there are $q$ vectors of $\mathbb{R}^p$ in a set and $p < q$, i.e. the amount of vectors is more than their dimension, then $A = [\vec{u}_1|\vec{u}_2|\vec{u}_3|\cdots|\vec{u}_q]$ is an $p \times q$ matrix which has more columns ($q$) than rows ($p$). In this case $A\vec{x} = \textbf{0}$ must have at least one free variables and thus infinitely many solutions, hence the vectors must be linearly dependent.
\item Otherwise ($p \geq q$), we can solve $A\vec{x} = \textbf{0}$ by Gaussian Elimination to see if it only has the trivial solution. If so (not), the vectors are linearly independent (dependent). Alternatively, if $A$ is a square matrix, then we may check if its determinant is non-zero, just like what have been done in Example \ref{exmp:exmplinearindep}. Gaussian Elimination still works for any square matrix, and in case of linear independence (dependence), $A$ will (not) be reduced to an identity matrix.
\end{enumerate}
The observation above also leads to an extension of Theorem \ref{thm:equiv2}.
\begin{thm}
\label{thm:equiv3}[Equivalence Statement, ver.\ 3]
For an $n \times n$ real square matrix $A$, the followings are equivalent:
\begin{enumerate}[label=(\alph*)]
\item $A$ is invertible, i.e.\ $A^{-1}$ exists,
\item $\det(A) \neq 0$,
\item The reduced row echelon form of $A$ is $I$,
\item The linear system $A\vec{x} = \vec{h}$ has a unique solution for any $\vec{h}$, particularly $A\vec{x} = \textbf{0}$ has only the trivial solution $\vec{x} = \textbf{0}$,
\item The $n$ column vectors $\vec{u}_1, \vec{u}_2, \vec{u}_3, \ldots, \vec{u}_n$ of $\mathbb{R}^n$ as in $A = [\vec{u}_1|\vec{u}_2|\vec{u}_3|\cdots|\vec{u}_n]$ are linearly independent.
\end{enumerate}
\end{thm}
Meanwhile, generalizing to non-square matrices, we can integrate the works in Section \ref{subsection:SolLinSysGauss} into the same framework of Theorem \ref{thm:linearindep} and have
\begin{thm}
\label{thm:nonsqlinearindep}
For any non-square matrix $A = [\vec{u}_1|\vec{u}_2|\vec{u}_3|\cdots|\vec{u}_q]$, the fact that $A\vec{x} = \textbf{0}$ has only the trivial solution $\vec{x} = \textbf{0}$ (any other non-trivial solution), or the linear system $A\vec{x} = \vec{h}$ has at most one solution for any $\vec{h}$ (infinitely many solutions for some $\vec{h}$), is equivalent to the linear independence (dependence) of $\vec{u}_1, \vec{u}_2, \vec{u}_3, \ldots, \vec{u}_q \in \mathbb{R}^n$.
\end{thm}

\begin{exmp}
Show that $\vec{u} = (2,1,-1,1)^T$, $\vec{v} = (1,2,1,-1)^T$, $\vec{w} = (0,1,1,2)^T$ are linearly independent. What if $\vec{w} = (1,-1,-2,2)^T$ instead?
\end{exmp}
\begin{solution}
From Theorem \ref{thm:linearindep} (or equivalently Theorem \ref{thm:nonsqlinearindep}), we just need to show that the system $A\vec{x} = \textbf{0}$ has only the trivial solution $\vec{x} = \textbf{0}$, where
\begin{align*}
A = [\vec{u}|\vec{v}|\vec{w}] =
\left[
\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}@{\;}}
2 & 1 & 0 \\
1 & 2 & 1 \\
-1 & 1 & 1 \\
1 & -1 & 2
\end{array}
\right]
\end{align*}
To do so we can apply Gaussian Elimination as below.
\begin{align*}
\left[
\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
2 & 1 & 0 & 0\\
1 & 2 & 1 & 0\\
-1 & 1 & 1 & 0\\
1 & -1 & 2 & 0
\end{array}
\right]
&\to
\left[
\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & -1 & 2 & 0 \\
1 & 2 & 1 & 0\\
-1 & 1 & 1 & 0\\
2 & 1 & 0 & 0\\
\end{array}
\right] & R_1 \leftrightarrow R_4 \\
&\to
\left[
\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & -1 & 2 & 0 \\
0 & 3 & -1 & 0\\
0 & 0 & 3 & 0\\
0 & 3 & -4 & 0\\
\end{array}
\right] & 
\begin{aligned}
R_2 - R_1 &\to R_2 \\
R_3 + R_1 &\to R_3 \\
R_4 - 2R_1 &\to R_4 
\end{aligned}\\
&\to
\left[
\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & -1 & 2 & 0 \\
0 & 1 & -\frac{1}{3} & 0\\
0 & 0 & 3 & 0\\
0 & 3 & -4 & 0\\
\end{array}
\right] & \frac{1}{3}R_2 \to R_2 \\
&\to
\left[
\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & -1 & 2 & 0 \\
0 & 1 & -\frac{1}{3} & 0\\
0 & 0 & 3 & 0\\
0 & 0 & -3 & 0\\
\end{array}
\right] & R_4 - 3R_2 \to R_4 \\
&\to
\left[
\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & -1 & 2 & 0 \\
0 & 1 & -\frac{1}{3} & 0\\
0 & 0 & 1 & 0\\
0 & 0 & -3 & 0\\
\end{array}
\right] & \frac{1}{3}R_3 \to R_3 \\
&\to
\left[
\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & -1 & 2 & 0 \\
0 & 1 & -\frac{1}{3} & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 0\\
\end{array}
\right] & R_4 + 3R_1 \to R_4 
\end{align*}
This leads to a redundant row and the trivial solution of $\vec{x} = 0$ (we can go ahead with the backward phase, but the fact that all columns have a pivot in the row echelon form is adequate), and hence the three vectors $\vec{u}, \vec{v}, \vec{w}$ are linearly independent. If $\vec{w} = (1,-1,-2,2)^T$, we can repeat the same analysis such that
\begin{align*}
\left[
\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
2 & 1 & 1 & 0\\
1 & 2 & -1 & 0\\
-1 & 1 & -2 & 0\\
1 & -1 & 2 & 0
\end{array}
\right]
&\to
\left[
\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & -1 & 2 & 0\\
1 & 2 & -1 & 0\\
-1 & 1 & -2 & 0\\
2 & 1 & 1 & 0
\end{array}
\right] & R_1 \leftrightarrow R_4 \\
&\to
\left[
\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & -1 & 2 & 0\\
0 & 3 & -3 & 0\\
0 & 0 & 0 & 0\\
0 & 3 & -3 & 0
\end{array}
\right] & 
\begin{aligned}
R_2 - R_1 \to R_2 \\
R_3 + R_1 \to R_3 \\
R_4 - 2R_1 \to R_4
\end{aligned} \\
&\to
\left[
\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & -1 & 2 & 0\\
0 & 1 & -1 & 0\\
0 & 0 & 0 & 0\\
0 & 3 & -3 & 0
\end{array}
\right] & \frac{1}{3}R_2 \to R_2 \\
&\to
\left[
\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & -1 & 2 & 0\\
0 & 1 & -1 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{array}
\right] & R_4 - 3R_2 \to R_4
\end{align*}
Again, we can continue the elimination process but this is already enough to see that the homogeneous system possesses a free variable and therefore non-trivial solutions. So in this case they are linearly dependent. In fact, it is not hard to see that $\vec{w} = \vec{u} - \vec{v}$.
\end{solution}
Finally, we provide a theorem which is linked to Properties \ref{proper:addvdepend}.
\begin{thm}[Plus/Minus Theorem]
\label{thm:plusminus}
Let $\mathcal{S} = \{\vec{u}_1, \vec{u}_2, \vec{u}_3, \ldots, \vec{u}_q\}$ be a set of vectors, with $\vec{u}_j \in \mathcal{V}$, we have the following two results.
\begin{enumerate}[label=(\alph*)]
    \item If $\mathcal{S}$ is a linearly independent set and $\vec{v}$ is not in $\text{span}(\mathcal{S})$, then $\mathcal{S} \cup \{\vec{v}\}$ formed after inserting $\vec{v}$ into the set is still linearly independent,
    \item If $\vec{w}$ is a vector in $\mathcal{S}$ that can be expressed as a linear combination of other vectors in the set, then the new set $\mathcal{S} - \{\vec{w}\}$ formed after removing $\vec{w}$ from $\mathcal{S}$ has the same span, i.e.
    \begin{align*}
    \text{span}(\mathcal{S}) = \text{span}(\mathcal{S} - \{\vec{w}\})
    \end{align*}
\end{enumerate}
\end{thm}
\begin{proof}
(a) is simply the equivalent of Properties \ref{proper:addvdepend} but expressed as its negation. For (b), assign the vector $\vec{u}_k$ that is being removed where $1 \leq k \leq q$ as $\vec{w}$. We can write $\vec{w} = a_1\vec{u}_1 + a_2\vec{u}_2 + \cdots + a_{k-1}\vec{u}_{k-1} + a_{k+1}\vec{u}_{k+1} + \cdots + a_q\vec{u}_q$ using other vectors where $a_j$, $j \neq k$ are some constants. For any vector $\vec{v} = b_1\vec{u}_1 + b_2\vec{u}_2 + \cdots + b_{k-1}\vec{u}_{k-1} + b_k\vec{u}_k + b_{k+1}\vec{u}_{k+1} + \cdots + b_q\vec{u}_q$ in $\text{span}(\mathcal{S})$ with $b_j$ being the coefficients, it can be rewritten as
\begin{align*}
\vec{v} &= b_1\vec{u}_1 + b_2\vec{u}_2 + \cdots + b_{k-1}\vec{u}_{k-1} + b_k\vec{u}_k + b_{k+1}\vec{u}_{k+1} + \cdots + b_q\vec{u}_q \\
&= b_1\vec{u}_1 + b_2\vec{u}_2 + \cdots + b_{k-1}\vec{u}_{k-1} + b_{k+1}\vec{u}_{k+1} + \cdots + b_q\vec{u}_q + b_k\vec{w} \\
&= b_1\vec{u}_1 + b_2\vec{u}_2 + \cdots + b_{k-1}\vec{u}_{k-1} + b_{k+1}\vec{u}_{k+1} + \cdots + b_q\vec{u}_q \\
&\quad + b_k(a_1\vec{u}_1 + a_2\vec{u}_2 + \cdots + a_{k-1}\vec{u}_{k-1} + a_{k+1}\vec{u}_{k+1} + \cdots + a_q\vec{u}_q) \\
&= (b_1 + b_ka_1) \vec{u}_1 + (b_2 + b_ka_2) \vec{u}_2 + (b_{k-1} + b_ka_{k-1})\vec{u}_{k-1}  \\
&\quad + (b_{k+1} + b_ka_{k+1}) \vec{u}_{k+1} + \cdots + (b_q + b_ka_q)\vec{u}_q \\
&\in \text{span}(\mathcal{S} - \{\vec{u_k}\}) = \text{span}(\mathcal{S} - \{\vec{w}\})
\end{align*}
Therefore for all $\vec{v} \in \text{span}(\mathcal{S})$, $\vec{v} \in \text{span}(\mathcal{S} - \{\vec{w}\})$ and hence $\text{span}(\mathcal{S}) \subseteq \text{span}(\mathcal{S} - \{\vec{w}\})$. It is trivial to show $\text{span}(\mathcal{S} - \{\vec{w}\}) \subseteq \text{span}(\mathcal{S})$, and thus $\text{span}(\mathcal{S}) = \text{span}(\mathcal{S} - \{\vec{w}\})$. This part of the theorem is very relevant to $\mathcal{S}_3$ and $\mathcal{S}_4$ in the previous Example \ref{exmp:S3S4}.
\end{proof}

\subsection{Coordinate Bases for $\mathbb{R}^n$ and its Subspaces}
\label{section:6.1.5}
In Definition \ref{defn:standardunitvec}, we have introduced $n$ standard unit vectors $\hat{e}_1, \hat{e}_2, \ldots, \hat{e}_n$ for the real $n$-space $\mathbb{R}^n$. Obviously the standard unit vectors are linearly independent and their span is exactly $\mathbb{R}^n$. We often refer to the coefficients $x_j$ of $\hat{e}_j$ as the (Cartesian) coordinates of a vector $\vec{v} = x_1\hat{e}_1 + x_2\hat{e}_2 + \cdots + x_n\hat{e}_n$ in $\mathbb{R}^n$. The coordinates $x_j$ are unique, guaranteed by Properties \ref{proper:lincombofspan}. However, sometimes we may want to express an $\mathbb{R}^n$ vector in another \index{Coordinate Basis}\keywordhl{coordinate basis (system)} with axes different from the standard unit vectors (\index{Standard Basis}\keywordhl{standard basis}). Motivated by the properties of the Cartesian coordinate system above, in which the standard unit vectors are linearly independent and span $\mathbb{R}^n$ such that every vector in $\mathbb{R}^n$ can be expressed as a unique linear combination of them (Properties \ref{proper:lincombofspan} again, we require the vectors in a coordinate basis for $\mathbb{R}^n$ to carry the same properties. The coefficients of the aforementioned linear combination will become the coordinates of that vector in this basis.
\begin{defn}[Coordinate Basis for $\mathbb{R}^n$]
\label{defn:coordRn}
A coordinate basis for $\mathbb{R}^n$ should consist of $n$ vectors in $\mathbb{R}^n$ which
\begin{enumerate}[label=(\alph*)]
\item are linearly independent, and
\item span (generate) $\mathbb{R}^n$.
\end{enumerate}
\end{defn}
Some may wonder why the above definition has explicitly required that the number of vectors in a coordinate basis for $\mathbb{R}^n$ to be exactly $n$, although many people would probably think it is reasonable and accept this without a doubt. For the sake of completeness, below we will explain that this is a result coming naturally from the conditions of linear independence and spanning $\mathbb{R}^n$.
\begin{proof}
We have previously shown that Theorem \ref{thm:linearindep} implies that in $\mathbb{R}^n$ if there are more vectors $q$ than the dimension $n$ then they will be linearly dependent. So linear independence means $q \leq n$. To span $\mathbb{R}^n$, it is apparent that $q \geq n$.\footnote{
\label{foot:inconsth}
To formally show this, express the span of $q$ $\mathbb{R}^n$ vectors $c_1\vec{u}_1 + c_2\vec{u}_2 + c_3\vec{u}_3 + \cdots + c_q\vec{u}_q$ by $A\vec{x}$ where $A = [\vec{u}_1|\vec{u}_2|\vec{u}_3|\cdots|\vec{u}_q]$ is an $n \times q$ matrix and $\vec{x} = (c_1, c_2, c_3, \ldots, c_q)^T$ consists of $q$ coefficients as unknowns. If $q < n$, then $A\vec{x} = \vec{h}$ is an overdetermined system such that we can always find some row of full zeros in the reduced row echelon form of $A$ (to the left of the augmented matrix) as we solve the system by Gaussian Elimination. We can always set the number to the right of the augmented matrix resulted from Gaussian Elimination on that row to some non-zero number (let's say, $1$) if not already to make sure it is inconsistent. Invert the entire process of Gaussian Elimination over the augmented matrix to recover $A$ from its reduced row echelon form. To the right of the augmented matrix will then appear $\vec{h}_{\text{inconst}}$. This system $A\vec{x} = \vec{h}_{\text{inconst}}$ is inconsistent by the design above (just do the same steps of Gaussian Elimination again and the inconsistent $1$ to the right will reappear), which shows that the span does not include $\vec{h}_{\text{inconst}}$ and cannot cover the entire $\mathbb{R}^n$.} Hence the number of vectors $q$ must be equal to $n$.
\end{proof}
The following theorem shows that we actually only need to check either one of the conditions in Definition \ref{defn:coordRn}.
\begin{thm}
\label{thm:linindspan}
A set of $n$ vectors of $\mathbb{R}^n$ is linearly independent if and only if they span $\mathbb{R}^n$.
\end{thm}
\begin{proof}
Linear Independence $\rightarrow$ Spanning $\mathbb{R}^n$: Assume $\vec{u}_1, \vec{u}_2, \ldots, \vec{u}_n$ are linear independent with $A = [\vec{u}_1|\vec{u}_2|\cdots|\vec{u}_n]$ being a square matrix. The application of part (e) $\rightarrow$ (d) of Theorem \ref{thm:equiv3} immediately shows that there are always a (unique) solution to $A\vec{x} = \vec{h}$ for any $\vec{h}$ of $\mathbb{R}^n$. Recall that $A\vec{x}$ represents a linear combination of $\vec{u}_1, \vec{u}_2, \ldots, \vec{u}_n$ by Properties \ref{proper:linearcombmatrix}, and thus the above result implies that the span (see Definition \ref{defn:span}) of this set of vectors constitutes the entire $\mathbb{R}^n$.\\
\\
Spanning $\mathbb{R}^n$ $\rightarrow$ Linear Independence: Assume that $\vec{u}_1, \vec{u}_2, \ldots, \vec{u}_n$ are linear dependent, then by (c) and (e) of Theorem \ref{thm:equiv3} the reduced row echelon form of $A = [\vec{u}_1|\vec{u}_2|\cdots|\vec{u}_n]$ is not the identity matrix and contains at least one row of full zeros. Following a logic similar to Footnote \ref{foot:inconsth}, these vectors cannot span $\mathbb{R}^n$ and the contrapositive is proved. 
\end{proof}

\begin{exmp}
\label{exmp:basisR3}
Show that $\mathcal{B} = \{(1,2,1)_S^T, (-1,1,0)_S^T, (1,-1,2)_S^T\}$ forms a basis for $\mathbb{R}^3$ and express $[\vec{x}]_S = (2,1,2)_S^T$ in $\mathcal{B}$ ($\rightarrow [\vec{x}]_B$), where the subscript $S$ emphasizes that the coordinates are relative to standard basis $\mathcal{E}$.
\end{exmp}
\begin{solution}
By Definition \ref{defn:coordRn} and Theorem \ref{thm:linindspan}, the first part is equivalent to checking if the three $\mathbb{R}^3$ vectors in $\mathcal{B}$ are linearly independent. By Theorem \ref{thm:equiv3}, we can simply check if $\det(A)$ is non-zero where 
\begin{align*}
A = 
\begin{bmatrix}
1 & -1 & 1 \\
2 & 1 & -1 \\
1 & 0 & 2
\end{bmatrix}
\end{align*}
A simple calculation reveals that $\det(A) = 6 \neq 0$ so $\mathcal{B}$ is indeed a basis for $\mathbb{R}^3$. To express $(2,1,2)_S^T$ in $\mathcal{B}$ is to find $[\vec{x}]_B = ([x_1]_B, [x_2]_B, [x_3]_B)_B^T$ where $[x_j]_B$ is the $j$-th component of $\vec{x}$ in the coordinate system $\mathcal{B}$ such that the below linear combination holds:
\begin{align*}
[x_1]_B(1,2,1)_S^T + [x_2]_B(-1,1,0)_S^T + [x_3]_B(1,-1,2)_S^T = (2,1,2)_S^T
\end{align*}
or put in matrix form,
\begin{align*}
A[\vec{x}]_B &= [\vec{x}]_S \\
\begin{bmatrix}
1 & -1 & 1 \\
2 & 1 & -1 \\
1 & 0 & 2
\end{bmatrix}
\begin{bmatrix}
[x_1]_B \\
[x_2]_B \\
[x_3]_B
\end{bmatrix}
&=
\begin{bmatrix}
2 \\
1 \\
2
\end{bmatrix}
\end{align*}
We can either use matrix inverse or Gaussian Elimination to solve for the $[x_j]_B$, yielding $[x_1]_B = 1, [x_2]_B = -\frac{1}{2}, [x_3]_B = \frac{1}{2}$, and hence $[\vec{x}]_B = (1, -\frac{1}{2}, \frac{1}{2})^T_B$. The matrix equation $A[\vec{x}]_B = [\vec{x}]_S$ shows that $A$ transforms the coordinate system of a given vector from $\mathcal{B}$ to $\mathcal{E}$, and hence we will write $A = P_B^S$ (thus $P_B^S [\vec{x}]_B = [\vec{x}]_S$) for clarity in the future. Notice that $P_B^S (e_j)_B$ returns the $j$-th basis vector of the new basis $\mathcal{B}$ ($j$-th column of $P_B^S$) expressed in the standard basis $\mathcal{E}$, where $(e_j)_B$ is the numeric tuple representation (emphasized by the absence of hat symbol over $e$) of the $j$-th basis vector in the $\mathcal{B}$ coordinate system with the $j$-th component being $1$ and other being $0$. From now on, we simply omit the subscript $S$ and write $\vec{x}$ in place of $[\vec{x}]_S$ if not specified, to denote vectors in the standard basis as implicitly assumed before.
\end{solution}

Now that we are able to construct a coordinate basis for $\mathbb{R}^n$, it is natural to ask if we can also extend this and come up with some coordinate basis for any subspace of $\mathbb{R}^n$ (since a subspace is itself a vector space too), in the sense that any vector in the subspace can be uniquely expressed by the basis vectors (\textit{linear independence}) and the basis generates the subspace exactly such that its \textit{span} does include all vectors in the subspace and none outside the subspace. This can be achieved by the following procedure.

\begin{proper}
\label{proper:genbasis}
To produce a coordinate basis $\mathcal{B}$ for some subspace (that is not the zero subspace) $\mathcal{W}$ of $\mathbb{R}^n$, take any non-zero vector $\vec{u}_1$ in the subspace. Find another vector $\vec{u}_2$ in the subspace that is linearly independent from $\vec{u}_1$ if available and add it to $\mathcal{B}$, and stop otherwise. Repeat the above step and search for the next vector $\vec{u}_j$ in $\mathcal{W}$ that is linearly independent from all previous $j-1$ vectors $\vec{u_1}, \vec{u}_2, \ldots, \vec{u}_{j-1}$, append it to $\mathcal{B}$, and terminate when no more such a vector can be found. The set of vectors $\mathcal{B} = \{\vec{u_1}, \vec{u}_2, \ldots, \vec{u}_{j-1}, \vec{u}_j$\}, $\vec{u}_j \in \mathcal{W}$, $j \leq n$, collected in this way then forms a basis for the subspace.
\end{proper}
\begin{proof}
The above procedure automatically guarantees linear independence of $\mathcal{B}$ by part (a) of Theorem \ref{thm:plusminus} so any vector in the span of the basis can be uniquely expressed (Properties \ref{proper:lincombofspan}). Now what remains is to show that the basis spans the subspace exactly ($\text{span}(\mathcal{B}) = \mathcal{W}$). We will show that it is not possible for the span of the basis to has a vector that is not in the subspace (the span is contained in the subspace, $\text{span}(\mathcal{B}) \subseteq \mathcal{W}$), and vice versa (the subspace is contained in the span, $\mathcal{W} \subseteq \text{span}(\mathcal{B})$). For the first case, we directly use Properties \ref{proper:WcontainsspanS}, where the fact of $\mathcal{B} \subseteq W$ immediately implies $\text{span}(\mathcal{B}) \subseteq \mathcal{W}$. \\
\\
The second case is trivial as if there is indeed a vector in the subspace that is not within the span of the basis, then such a vector by definition is linearly independent from these basis vectors and the procedure should have not stopped but rather been continued to include this vector. This also completes the second part of the proof in Properties \ref{proper:subspace_n_span} for $\mathbb{R}^n$ specifically, as we have explicitly shown that any subspace of it can coincide with the span of some basis by construction. Notice that while we only show this for $\mathbb{R}^n$, this is valid for any finite-dimensional vector (sub)space, and the treatment for an infinite-dimensional vector space is currently out of the scope. \\
\\
Short Exercise: What does it mean when the number of steps $j$ is equal to $n$ (as in $\mathbb{R}^n$) in Properties \ref{proper:genbasis}?\footnote{The subspace is just $\mathbb{R}^n$ itself. It is also obvious that $j$ can't be greater than $n$.}
\end{proof}

With this, we can now properly define the "dimension" of any subspace of $\mathbb{R}^n$. It is simply the number of vectors in its basis. Some may wonder if it is possible for two bases of the same vector space to have different number of vectors so that the notion of its dimension will be problematic. In fact, all bases of a finite-dimensional vector (sub)space must possess the same amount of vectors, and we note the results below. %Extending from the requirement of a basis for $\mathbb{R}^n$, here a basis of a general vector space $\mathcal{V}$ should be linearly independent and spans $\mathcal{V}$ too. To show this, we need an important result called \index{Steinitz Replacement Theorem}\keywordhl{Steinitz Replacement Theorem}.

%\begin{thm}[Steinitz Replacement Theorem]
%\label{thm:Steinitz}
%Given a vector space $\mathcal{V}$ generated by a set $\mathcal{G}$ consisting of $n$ vectors (not necessarily linearly independent), and another set $\mathcal{S}$ containing $m$ linearly independent vectors from $\mathcal{V}$. Then $m \leq n$ and there exists a subset $\mathcal{H}$ of $\mathcal{G}$ with exactly $n-m$ vectors such that $\mathcal{S} \cup \mathcal{H}$ spans $\mathcal{V}$.
%\end{thm}
%\begin{proof}
%We proceed with mathematical induction on $m$. The base case $m = 0$ is trivial as $\mathcal{S} = \varnothing$ and $\mathcal{H} = \mathcal{G}$. Now assume the statement is true for some integer $m = k \geq 0$ and we have to prove it for $m = k+1$. Let $\mathcal{S} = \{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k, \vec{v}_{k+1}\}$ be a subset of $\mathcal{V}$ with $m = k+1$ linearly independent vectors. It is apparent that after removing $\vec{v}_{k+1}$ from $\mathcal{S}$, $\{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k\}$ is still linearly independent. Then we can use the induction hypothesis to obtain $k \leq n$ and a subset $\{\vec{u}_1, \vec{u}_2, \ldots, \vec{u}_{n-k}\}$ of $\mathcal{G}$ so that $\{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_{k}\} \cup \{\vec{u}_1, \vec{u}_2, \ldots, \vec{u}_{n-k}\}$ generates $\mathcal{V}$. So $\vec{v}_{k+1} \in \mathcal{V}$ can be written as the linear combination of
%\begin{align*}
%\vec{v}_{k+1} = a_1\vec{v}_1 + a_2\vec{v}_2 + \cdots + a_k\vec{v}_k + b_1\vec{u}_1 + b_2\vec{u}_2 + \cdots + b_{n-k}\vec{u}_{n-k} 
%\end{align*}
%It must be true that $n - k \geq 1$, so that some $b_j$ are present for otherwise $\vec{v}_{k+1}$ will be reduced to a linear combination of $\vec{v}_j$ and contradicts the assumption that $\mathcal{S}$ is linearly independent. Hence $m = k + 1 \leq n$. By part (b) of Theorem \ref{thm:plusminus}, $\text{span}(\mathcal{S} \cup \{\vec{u}_1, \vec{u}_2, \ldots, \vec{u}_{n-k}\}) = \text{span}(\{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_{k}, \vec{v}_{k+1}\} \cup \{\vec{u}_1, \vec{u}_2, \ldots, \vec{u}_{n-k}\}) = \text{span}(\{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_{k}\} \cup \{\vec{u}_1, \vec{u}_2, \ldots, \vec{u}_{n-k}\}) = \mathcal{V}$ (the last equality follows from the induction hypothesis). For the same reason, in particular some $b_j$ has to be non-zero, let's say $b_1$. Therefore we can write
%\begin{align*}
%\vec{u}_1 = -\frac{a_1}{b_1}\vec{v}_1 -\frac{a_2}{b_1}\vec{v}_2 - \cdots - \frac{a_k}{b_1}\vec{v}_k + \frac{1}{b_1}\vec{v}_{k+1} - \frac{b_2}{b_1}\vec{u}_2 - \cdots - \frac{b_{n-k}}{b_1}\vec{u}_{n-k}
%\end{align*}
%as a linear combination of $\vec{v}_1, \vec{v_2}, \cdots, \vec{v}_k, \vec{v}_{k+1}$ and $\vec{u}_2, \cdots, \vec{u}_{n-k}$. Let $\mathcal{H} = \{\vec{u}_2, \ldots, \vec{u}_{n-k}\}$. Again by part (b) of Theorem \ref{thm:plusminus}
%\begin{align*}
%&\quad \text{span}(\mathcal{S} \cup \{\vec{u}_1, \vec{u}_2, \ldots, \vec{u}_{n-k}\}) \\
%&= \text{span}(\{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_{k}, \vec{v}_{k+1}\} \cup \{\vec{u}_1, \vec{u}_2, \ldots, \vec{u}_{n-k}\}) \\
%&= \text{span}(\{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_{k}, \vec{v}_{k+1}\} \cup \{\vec{u}_2, \ldots, \vec{u}_{n-k}\}) \\
%&= \text{span}(\mathcal{S} \cup \mathcal{H})
%\end{align*}
%But we just have $\text{span}(\mathcal{S} \cup \{\vec{u}_1, \vec{u}_2, \ldots, \vec{u}_{n-k}\}) = \mathcal{V}$ from above, which readily shows that $ \text{span}(\mathcal{S} \cup \mathcal{H}) = \mathcal{V}$. As $\mathcal{H}$ is a subset of $\mathcal{G}$ with $n - k-1 = n - (k+1)$ vectors, the theorem is true for $m = k+1$, and the induction is completed.
%\end{proof}
%The key point of the theorem is that some vectors in a generating set of a vector space can be replaced by the same number of linearly independent vectors from that vector space, hence comes the name of replacement theorem. We then have the desired proposition as a corollary to the theorem.
\begin{proper}
\label{proper:samenvecsbases}
If $\mathcal{V}$ is a vector space with a finite basis, then all bases of $\mathcal{V}$ are finite and have the same number of vectors.
\end{proper}
%\begin{proof}
%Assume $\mathcal{G}$ is a finite basis for $\mathcal{V}$ consists of $n$ vectors, and let $\mathcal{U}$ be any other basis with $k$ vectors for $\mathcal{V}$. If $\mathcal{U}$ contains more than $n$ vectors such that $k > n$, then we can take a subset $\mathcal{S}$ of $\mathcal{U}$ with exactly $m = n+1$ vectors. By Theorem \ref{thm:Steinitz}, as $\mathcal{G}$ generates $\mathcal{V}$, and $\mathcal{S}$ as a subset of the basis $\mathcal{U}$ is linearly independent, $m = n+1 \leq n$ which leads to a contradiction, so it has to be $k \leq n$. Reversing the roles of $\mathcal{G}$ and $\mathcal{U}$, the same arguments requires $k \geq n$, and therefore $k = n$, i.e. every bases of $\mathcal{V}$ have $n$ vectors and are finite.
%\end{proof}
From the statement above, we see that if we can find any basis with exactly $n$ vectors for a vector space $\mathcal{V}$ where $n$ is finite, then $n$ will be the unique integer such that every basis $\mathcal{V}$ is consisted of this number of vectors. $n$ is then referred to as the \index{Dimension}\keywordhl{dimension} of $\mathcal{V}$, and we define $\text{dim}(\mathcal{V}) = n$. $\mathcal{V}$ is then known as a \index{Finite-dimensional}\keywordhl{finite-dimensional} vector space. If a vector space is not finite-dimensional, i.e. a finite basis cannot be found, then it is called \index{Infinite-dimensional}\keywordhl{infinite-dimensional}. Moreover,
\begin{proper}
\label{proper:dimWleqV}
For any subspace $\mathcal{W}$ of a vector space $\mathcal{V}$, $\dim(\mathcal{W}) \leq \dim(\mathcal{V})$. If $\dim(\mathcal{W}) = \dim(\mathcal{V})$, $\mathcal{W} = \mathcal{V}$.
\end{proper}
\begin{thm}
\label{thm:finitebasissubset}
If a vector space $\mathcal{V}$ is generated by a set $\mathcal{G}$ with a finite amount of vectors, then some subset of $\mathcal{G}$ is a basis for $\mathcal{V}$, and $\mathcal{V}$ has finite bases.
\end{thm}
%\begin{proof}
    %The proof closely parallels that for Properties \ref{proper:genbasis}. If $\mathcal{G}$ has $n$ vectors, then we can choose $\mathcal{B} = \{\vec{u}_1, \vec{u}_2, \cdots, \vec{u}_j\}$, $j \leq n$, such that $\mathcal{B}$ are linearly independent. If $j = n$, then $\mathcal{B} = \mathcal{G}$ is linearly independent, spans $\mathcal{V}$, and hence itself a basis for $\mathcal{V}$. Otherwise, $\text{span}(\mathcal{B}) \subseteq \mathcal{V}$ as $\mathcal{B} \subseteq \mathcal{V}$ by Properties \ref{proper:WcontainsspanS}, and using the logic similar to the second part of proof in Properties \ref{proper:genbasis}, $\mathcal{V} \subseteq \text{span}(\mathcal{B})$ and hence $\text{span}(\mathcal{B}) = \mathcal{V}$. This $\mathcal{B}$, as a linearly independent subset of $\mathcal{G}$, is thus a basis for $\mathcal{V}$.
%\end{proof}
According to this theorem and using part (b) of Theorem \ref{thm:plusminus}, we can trim down a generating set to make it a basis.
\begin{exmp}
\label{exmp:gentrimbasis}
Let $\mathcal{G} = \{(1,0,1,1)^T, (1,2,0,-1)^T, (2,2,1,0)^T, (0,1,0,1)^T\}$ and $\mathcal{W} = \text{span}(\mathcal{G})$ be the subspace generated by $\mathcal{G}$. From $\mathcal{G}$ extract a subset $\mathcal{B}$ of it as the basis for $\mathcal{W}$. Express $\vec{x} = (0,-1,1,3)^T$ in this basis.
\end{exmp}
\begin{solution}
The general form of linear combinations of vectors in $\mathcal{G}$ is $p(1,0,1,1)^T + q(1,2,0,-1)^T + r(2,2,1,0)^T + s(0,1,0,1)^T$, and can be written as
\begin{align*}
A\vec{x} =
\begin{bmatrix}
1 & 1 & 2 & 0\\
0 & 2 & 2 & 1\\
1 & 0 & 1 & 0\\
1 & -1 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
p \\
q \\
r \\
s
\end{bmatrix}
\end{align*}
Now consider the matrix equation $A\vec{x} = \textbf{0}$. Its solution can be found by doing Gaussian Elimination as follows.
\begin{align*}
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{}}
1 & 1 & 2 & 0 & 0\\
0 & 2 & 2 & 1 & 0\\
1 & 0 & 1 & 0 & 0\\
1 & -1 & 0 & 1 & 0
\end{array}\right]
& \rightarrow    
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{}}
1 & 1 & 2 & 0 & 0\\
0 & 2 & 2 & 1 & 0\\
0 & -1 & -1 & 0 & 0\\
0 & -2 & -2 & 1 & 0
\end{array}\right]
&
\begin{aligned}
R_3-R_1&\to R_3 \\
R_4-R_1&\to R_4
\end{aligned} \\
& \rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{}}
1 & 1 & 2 & 0 & 0\\
0 & 1 & 1 & \frac{1}{2} & 0\\
0 & -1 & -1 & 0 & 0\\
0 & -2 & -2 & 1 & 0
\end{array}\right]
& \frac{1}{2}R_2 \to R_2 \\
& \rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{}}
1 & 1 & 2 & 0 & 0\\
0 & 1 & 1 & \frac{1}{2} & 0\\
0 & 0 & 0 & \frac{1}{2} & 0\\
0 & 0 & 0 & 2 & 0
\end{array}\right] 
&
\begin{aligned}
R_3+R_2&\to R_3 \\
R_4+2R_2&\to R_4
\end{aligned} \\
& \rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{}}
1 & 1 & 2 & 0 & 0\\
0 & 1 & 1 & \frac{1}{2} & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 2 & 0
\end{array}\right] 
& 2R_3 \to R_3 \\
& \rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{}}
1 & 1 & 2 & 0 & 0\\
0 & 1 & 1 & \frac{1}{2} & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 0
\end{array}\right]
& R_4 - 2R_3 \to R_4 \\
& \rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{}}
1 & 1 & 2 & 0 & 0\\
0 & 1 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 0
\end{array}\right]
& R_2 - \frac{1}{2}R_3 \to R_2 \\
& \rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{}}
1 & 0 & 1 & 0 & 0\\
0 & 1 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 0
\end{array}\right]
& R_1 - R_2 \to R_1
\end{align*}
The third column, which does not contain a pivot, of this reduced row echelon form implies that $r$ is a free variable and if $p + r = 0$ and $q + r = 0$, then $A\vec{x} = \textbf{0}$ has a non-trivial solution. Simply take $r = 1$, then we have $p = -1$, $q = -1$, and $-(1,0,1,1)^T - (1,2,0,-1)^T + (2,2,1,0)^T = \textbf{0}$. Hence the third vector in $\mathcal{G}$, that is, $(2,2,1,0)^T = (1,0,1,1)^T + (1,2,0,-1)^T$ is a linear combination of its first two vectors. By part (b) of Theorem \ref{thm:plusminus}, we can remove it from $\mathcal{G}$ while keeping its span unchanged, which means that given the new subset $\mathcal{B} = \{(1,0,1,1)^T, (1,2,0,-1)^T, (0,1,0,1)^T\}$, we have $\text{span}(\mathcal{B}) = \text{span}(\mathcal{G}) = \mathcal{W}$. If we are to do Gaussian Elimination again over $A\vec{x} = \textbf{0}$ with the third variable removed, then the effective change on the final reduced row echelon form can be foreseen to be the deletion of the third row, so the new system would only have the trivial solution. By Theorem \ref{thm:nonsqlinearindep}, $\mathcal{B}$ will be linearly independent, and therefore a valid basis for $\mathcal{W}$. From this we also know $\dim(\mathcal{W}) = 3$.\\
\\
In order to express $\vec{x} = (0,-1,1,3)^T$ in the $\mathcal{B}$ basis which contains three generating vectors, we need to find $[\vec{x}]_B = ([x_1]_B, [x_2]_B, [x_3]_B)_B^T$ of three components correspondingly such that $[x_1]_B(1,0,1,1)^T + [x_2]_B(1,2,0,-1)^T + [x_3]_B(0,1,0,1)^T = (0,-1,1,3)^T$. This leads to the system
\begin{align*}
\begin{bmatrix}
1 & 1 & 0\\
0 & 2 & 1 \\
1 & 0 & 0 \\
1 & -1 & 1\\
\end{bmatrix}
\begin{bmatrix}
[x_1]_B \\
[x_2]_B \\
[x_3]_B
\end{bmatrix}
=
\begin{bmatrix}
0 \\
-1 \\
1 \\
3
\end{bmatrix}
\end{align*}
We can find the coordinates by exactly the same steps of Gaussian Elimination as above.
\begin{align*}
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & 1 & 0 & 0\\
0 & 2 & 1 & -1\\
1 & 0 & 0 & 1\\
1 & -1 & 1 & 3\\
\end{array}\right]
& \rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & 1 & 0 & 0\\
0 & 2 & 1 & -1\\
0 & -1 & 0 & 1\\
0 & -2 & 1 & 3\\
\end{array}\right]
&
\begin{aligned}
R_3-R_1&\to R_3 \\
R_4-R_1&\to R_4
\end{aligned} \\
& \rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & 1 & 0 & 0\\
0 & 1 & \frac{1}{2} & -\frac{1}{2}\\
0 & -1 & 0 & 1\\
0 & -2 & 1 & 3\\
\end{array}\right]
& \frac{1}{2}R_2 \to R_2 \\
& \rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & 1 & 0 & 0\\
0 & 1 & \frac{1}{2} & -\frac{1}{2}\\
0 & 0 & \frac{1}{2} & \frac{1}{2}\\
0 & 0 & 2 & 2\\
\end{array}\right]
& \begin{aligned}
R_3+R_2&\to R_3 \\
R_4+2R_2&\to R_4
\end{aligned} \\
& \rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & 1 & 0 & 0\\
0 & 1 & \frac{1}{2} & -\frac{1}{2}\\
0 & 0 & 1 & 1\\
0 & 0 & 2 & 2\\
\end{array}\right]
& 2R_3 \to R_3 \\
& \rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & 1 & 0 & 0\\
0 & 1 & \frac{1}{2} & -\frac{1}{2}\\
0 & 0 & 1 & 1\\
0 & 0 & 0 & 0\\
\end{array}\right]
& R_4 - 2R_3 \to R_4 \\
& \rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & 1 & 0 & 0\\
0 & 1 & 0 & -1\\
0 & 0 & 1 & 1\\
0 & 0 & 0 & 0\\
\end{array}\right]
& R_2 - \frac{1}{2}R_3 \to R_2 \\
& \rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & 0 & 0 & 1\\
0 & 1 & 0 & -1\\
0 & 0 & 1 & 1\\
0 & 0 & 0 & 0\\
\end{array}\right]
& R_1 - R_2 \to R_1 
\end{align*}
from which we can readily see that $[\vec{x}]_B = (1,-1,1)_B^T$. To check the answer, we can simply calculate $(1)(1,0,1,1)^T +(-1)(1,2,0,-1)^T + (1)(0,1,0,1)^T = (0,-1,1,3)^T$.
\end{solution}

Like the above example, in general, if given a generating set $\mathcal{G}$ made of $\vec{u}_j$ ($\in \mathbb{R}^n$), to reduce it into a basis, we can apply Gaussian Elimination on $A = [\vec{u}_1|\vec{u}_2|\cdots|\vec{u}_q]$ to see if there is any non-pivotal column. The components of such a column represents the coefficients of other vectors to make a linear combination of that column vector in question. In the above example, the third column in the reduced row echelon form is $(1,1,0,0)^T$, implying that the third vector in $\mathcal{G}$ equals to $1(\text{first vector}) + 1(\text{second vector})$. Such a relation is called a \index{Dependence Relation}\keywordhl{dependence relation}. Getting rid of all vectors corresponding to non-pivotal columns then leads to the desired basis.

Finally, we expand Theorem \ref{thm:linindspan} (Equivalent requirements of a basis) to any finite-dimensional vector (sub)space. The results are simply stated below.
\begin{proper}
\label{proper:linindspanbasisnewver}
If $\mathcal{V}$ is a vector space with $\text{dim}(\mathcal{V}) = n$, then
\begin{enumerate}[label=(\alph*)]
    \item Any generating set for $\mathcal{V}$ contains at least $n$ vectors. If, furthermore, it is made of exactly $n$ vectors, then it is also a basis for $\mathcal{V}$,
    \item Any linearly independent subset of $\mathcal{V}$ that has exactly $n$ vectors is a basis for $\mathcal{V}$,
    \item Every linearly independent subset $\mathcal{G}_1$ of $\mathcal{V}$ with $m \leq n$ vectors can be extended to a basis for $\mathcal{V}$, i.e. there exists another subset $\mathcal{G}_2$ of $\mathcal{V}$ with $n-m$ vectors such that $\mathcal{B} = \mathcal{G}_1 \cup \mathcal{G}_2$ is a basis for $\mathcal{V}$.
\end{enumerate}
\end{proper}
%\begin{proof}
%\begin{enumerate}[label=(\alph*)]
    %\item Let $\mathcal{G}$ be a generating set for $\mathcal{V}$. By Theorem \ref{thm:finitebasissubset}, there exists a subset $\mathcal{H}$ of $\mathcal{G}$ as a basis for $\mathcal{V}$. By Properties \ref{proper:samenvecsbases}, $\mathcal{H}$ has exactly $n$ vectors, and $\mathcal{G}$, having $\mathcal{H}$ as a subset, must have at least $n$ vectors. If $\mathcal{G}$ happens to have $n$ vectors as well, then $\mathcal{G} = \mathcal{H}$ is a basis for $\mathcal{V}$.
    %\item Use Theorem \ref{thm:Steinitz} with $\mathcal{S}$ being a linearly independent subset of $\mathcal{V}$ that has exactly $n$ vectors. $\mathcal{G}$ can any basis for $\mathcal{V}$, and in particular it generates $\mathcal{V}$ and is consisted of $n$ vectors as well. Then the Replacement Theorem implies that there exists a subset $\mathcal{H}$ of $\mathcal{G}$ with $n-n = 0$ vectors such that $\mathcal{S} \cup \mathcal{H}$ spans $\mathcal{V}$. But $\mathcal{H}$ having $0$ vectors means it is the empty set $\varnothing$, and it reduces to $\mathcal{S}$ spanning $\mathcal{V}$. Hence $\mathcal{S}$, being linearly independent, is a basis for $\mathcal{V}$.
    %\item Again use Theorem \ref{thm:Steinitz} with $\mathcal{S}$ this time being a linearly independent subset of $\mathcal{V}$ having $m \leq n$ vectors. Subsequently, there is a subset $\mathcal{H}$ of $\mathcal{V}$ (since any generating set $\mathcal{G} \subset \mathcal{V}$ is within the vector space) made up of $n-m$ vectors such that $\mathcal{S} \cup \mathcal{H}$ spans $\mathcal{V}$. As $\mathcal{S}$ and $\mathcal{H}$ have $m$ and $n-m$ vectors, the union $\mathcal{S} \cup \mathcal{H}$ can have at most $m + (n-m) = n$ vectors. Because $\mathcal{S} \cup \mathcal{H}$ generates $\mathcal{V}$, part (a) of the properties then means that $\mathcal{S} \cup \mathcal{H}$ has exactly $n$ vectors and it is a basis for $\mathcal{V}$ as well.
%\end{enumerate}
A point worth mentioning is that part (c) of the properties above allows the possibility of completing a basis from its fragment, which will be used in many arguments from time to time.
%\end{proof}

\subsection{Direct Sum Representation}

Since we can create subspaces from multiple individual vectors, we may like to know if we can go one step further and make a larger vector space from smaller subspaces by composing them together. This then leads to the \textit{direct sum} representation. Let's begin with the definition of \index{Subspace Sum}\keywordhl{sum of subspaces} first.
\begin{defn}[Subspace Sum]
\label{defn:subspacesum}
Given two subspaces $\mathcal{W}_1, \mathcal{W}_2$, of a vector space $\mathcal{V}$, their subspace sum is
\begin{align*}
\mathcal{W}_1 + \mathcal{W}_2 = \{\vec{w}_1 + \vec{w}_2 \mid \vec{w}_1 \in \mathcal{W}_1, \vec{w}_2 \in \mathcal{W}_2\}    
\end{align*}
consisted of all possible vectors resulted from addition between any pair of vectors from $\mathcal{W}_1, \mathcal{W}_2 \subseteq \mathcal{V}$ respectively. Note that $(\mathcal{W}_1 + \mathcal{W}_2) \subseteq \mathcal{V}$ is a subspace of $\mathcal{V}$.
\end{defn}
For example, if $\mathcal{W}_1 = \text{span}(\{(1,0,1)^T\})$ and $\mathcal{W}_2 = \text{span}(\{(1,1,0)^T, (0,1,1)^T\})$, then according to the definition of span in Definition \ref{defn:span} and that of subspace sum above, $\mathcal{W}_1 + \mathcal{W}_2 = \text{span}(\{(1,0,1)^T, (1,1,0)^T, (0,1,1)^T\})$, which is just the span of union of generating vectors from the two smaller spans, and can be shown to be equal to $\mathbb{R}^3$ following the same idea from Example \ref{exmp:basisR3}. Extending this, we have
\begin{align*}
\mathcal{W}_1 + \mathcal{W}_2 + \cdots + \mathcal{W}_n = \{\vec{w}_1 + \vec{w}_2 + \cdots \vec{w}_n \mid \vec{w}_j \in \mathcal{W}_j, 1 \leq j \leq n\}    
\end{align*}
In the small example above, $\dim(\mathcal{W}_1) + \dim(\mathcal{W}_2) = 1 + 2 = 3 = \dim(\mathcal{W}_1 + \mathcal{W}_2)$, as the spanning vectors collected from the two subspaces are linearly independent of each other, i.e. the basis in $\mathcal{W}_1$ cannot be expressed as the linear combination of that in $\mathcal{W}_2$ and vice versa. In this case, the dimensions of the two subspaces can be \textit{directly} added together, and hence it constitutes a \index{Direct Sum}\keywordhl{direct sum}, whose requirement is given below.
\begin{defn}[Direct Sum]
\label{defn:directsum}
A direct sum between two subspaces $\mathcal{W}_1, \mathcal{W}_2$ is their subspace sum $\mathcal{W}_1 + \mathcal{W}_2$ as defined in Definition \ref{defn:subspacesum} which additionally satisfies $\mathcal{W}_1 \cap \mathcal{W}_2 = \{\textbf{0}\}$, and is denoted as $\mathcal{W}_1 \oplus \mathcal{W}_2$, and we have $\dim(\mathcal{W}_1 \oplus \mathcal{W}_2) = \dim(\mathcal{W}_1) + \dim(\mathcal{W}_2)$.
\end{defn}
Here we show the condition of $\mathcal{W}_1 \cap \mathcal{W}_2 = \{\textbf{0}\}$ is equivalent to the above condition that the basis vectors from $\mathcal{W}_1$ and $\mathcal{W}_2$ combined are linearly independent. Let $\vec{u}_1, \vec{u}_2, \ldots, \vec{u}_p$ and $\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_q$ be the basis vectors for $\mathcal{W}_1$ and $\mathcal{W}_2$ respectively. If these basis vectors are linearly independent, then by Theorem \ref{thm:linearindep}, the equation
\begin{align*}
c_1\vec{u}_1 + c_2\vec{u}_2 + \cdots + c_p\vec{u}_p + c_{p+1}\vec{v}_1 + \cdots + c_{p+q}\vec{v}_q = \textbf{0}
\end{align*}
only has $c_j = 0$ as the trivial solution, $1 \leq j \leq p+q$. Rearranging, we have
\begin{align*}
&\quad c_1\vec{u}_1 + c_2\vec{u}_2 + \cdots + c_p\vec{u}_p \in \mathcal{W}_1 \\
&= -(c_{p+1}\vec{v}_1 + \cdots + c_{p+q}\vec{v}_q) \in \mathcal{W}_2
\end{align*}
But since $c_j = 0$ is the only solution to this, it shows that there is only the zero vector in both $\mathcal{W}_1$ and $\mathcal{W}_2$ at the same time. The converse essentially follows the same argument in reverse. We say that $\mathcal{W}_1 = \mathcal{W}_2^C$ and $\mathcal{W}_2 = \mathcal{W}_1^C$ are a \textit{complement} to each other in $\mathcal{W}_1 \oplus \mathcal{W}_2$.\\
\\
As a counter-example, consider Example \ref{exmp:gentrimbasis}, suppose $\mathcal{W}_1 = \text{span}(\mathcal{S}_1) = \text{span}(\{(1,0,1,1)^T, (1,2,0,-1)^T\})$ and $\mathcal{W}_2 = \text{span}(\mathcal{S}_2) = \text{span}(\{((2,2,1,0)^T, \\ (0,1,0,1)^T\})$ be the subspaces spanned the first/last two vectors in $\mathcal{G}$ respectively. It is not hard to see that $\mathcal{S}_1$ and $\mathcal{S}_2$ are themselves linearly independent (and hence bases for $\mathcal{W}_1$ and $\mathcal{W}_2$), and $\dim(\mathcal{W}_1) = \dim(\mathcal{W}_2) = 2$. Nevertheless, in that example, we already know that the four vectors when viewed together are not linearly independent ($(2,2,1,0)^T$ in $\mathcal{S}_2$ is equal to $(1,0,1,1)^T + (1,2,0,-1)^T$ in $\mathcal{S}_1$), and $\dim(\mathcal{W}_1 + \mathcal{W}_2) = \dim(\text{span}(\mathcal{G})) = 3 \neq 4 = 2+2 = \dim(\mathcal{W}_1) + \dim(\mathcal{W}_2)$, and therefore they cannot form a direct sum.\\
\\
The direct sum of multiple subspaces are then recursively defined as
\begin{align*}
&\quad \mathcal{W}_1 \oplus \mathcal{W}_2 \oplus \mathcal{W}_3 \oplus \cdots \oplus \mathcal{W}_{n-1} \oplus \mathcal{W}_n \\
&= (\cdots((\mathcal{W}_1 \oplus \mathcal{W}_2) \oplus \mathcal{W}_3) \oplus \cdots \oplus \mathcal{W}_{n-1}) \oplus \mathcal{W}_n
\end{align*}
where we add up the subspaces one by one. Below shows an example of this.
\begin{exmp}
\label{exmp:directsum}
Given $\mathcal{W}_1 = \text{span}\{(1,0,2,1,0)^T, (2,1,0,0,-1)^T\}$, $\mathcal{W}_2 = \text{span}\{(0,3,1,0,0)^T, (0,0,-1,-2,1)^T\}$, $\mathcal{W}_3 = \text{span}\{(1,1,-3,0,-1)^T\}$, show that $\mathcal{W}_1 \oplus \mathcal{W}_2 \oplus \mathcal{W}_3$ is a valid direct sum which equals to $\mathbb{R}^5$.
\end{exmp}
\begin{solution}
First, let's derive $\mathcal{W}_1 \oplus \mathcal{W}_2$. It is obvious that the two generating vectors from each of $\mathcal{W}_1$ and $\mathcal{W}_2$ are linearly independent themselves as they are not constant multiples of another. Now following similar ideas in Example \ref{exmp:gentrimbasis}, we are going to show every column in the matrix, which comes from the basis vectors of both $\mathcal{W}_1$ and $\mathcal{W}_2$
\begin{align*}
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}@{\;}}
1 & 2 & 0 & 0\\
0 & 1 & 3 & 0\\
2 & 0 & 1 & -1\\
1 & 0 & 0 & -2\\
0 & -1 & 0 & 1
\end{array}\right]    
\end{align*}
is pivotal after Gaussian Elimination, as follows.
\begin{align*}
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}@{\;}}
1 & 2 & 0 & 0\\
0 & 1 & 3 & 0\\
2 & 0 & 1 & -1\\
1 & 0 & 0 & -2\\
0 & -1 & 0 & 1
\end{array}\right]  
&\rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}@{\;}}
1 & 2 & 0 & 0\\
0 & 1 & 3 & 0\\
0 & -4 & 1 & -1\\
0 & -2 & 0 & -2\\
0 & -1 & 0 & 1
\end{array}\right] 
&
\begin{aligned}
R_3 - 2R_1 &\to R_3 \\
R_4 - R_1 &\to R_4
\end{aligned} \\
&\rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}@{\;}}
1 & 2 & 0 & 0\\
0 & 1 & 3 & 0\\
0 & 0 & 13 & -1\\
0 & 0 & 6 & -2\\
0 & 0 & 3 & 1
\end{array}\right] 
&
\begin{aligned}
R_3 + 4R_2 &\to R_3 \\
R_4 + 2R_2 &\to R_4 \\
R_5 + R_2 &\to R_5
\end{aligned} \\
&\rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}@{\;}}
1 & 2 & 0 & 0\\
0 & 1 & 3 & 0\\
0 & 0 & 3 & 1\\
0 & 0 & 6 & -2\\
0 & 0 & 13 & -1
\end{array}\right] 
& R_3 \leftrightarrow R_5 \\
&\rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}@{\;}}
1 & 2 & 0 & 0\\
0 & 1 & 3 & 0\\
0 & 0 & 1 & \frac{1}{3}\\
0 & 0 & 6 & -2\\
0 & 0 & 13 & -1
\end{array}\right] 
& \frac{1}{3}R_3 \to R_3 \\
&\rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}@{\;}}
1 & 2 & 0 & 0\\
0 & 1 & 3 & 0\\
0 & 0 & 1 & \frac{1}{3}\\
0 & 0 & 0 & -4\\
0 & 0 & 0 & -\frac{16}{3}
\end{array}\right] 
& \begin{aligned}
R_4 - 6R_3 &\to R_4 \\
R_5 - 13R_3 &\to R_5
\end{aligned} \\
&\rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}@{\;}}
1 & 2 & 0 & 0\\
0 & 1 & 3 & 0\\
0 & 0 & 1 & \frac{1}{3}\\
0 & 0 & 0 & 1\\
0 & 0 & 0 & -\frac{16}{3}
\end{array}\right] 
& -\frac{1}{4}R_4 \to R_4 \\
&\rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}@{\;}}
1 & 2 & 0 & 0\\
0 & 1 & 3 & 0\\
0 & 0 & 1 & \frac{1}{3}\\
0 & 0 & 0 & 1\\
0 & 0 & 0 & 0
\end{array}\right] 
& R_5 + \frac{16}{3}R_4 \to R_5
\end{align*}
and we are done. Therefore, the four column vectors are linearly independent when considered as a whole and the direct sum $\mathcal{W}_1 \oplus \mathcal{W}_2 = \text{span}(\{(1,0,2,1,0)^T, \\(2,1,0,0,-1)^T, (0,3,1,0,0)^T, (0,0,-1,-2,1)^T\})$ makes sense, with $\dim(\mathcal{W}_1 \oplus \mathcal{W}_2) = \dim(\mathcal{W}_1) + \dim(\mathcal{W}_2) = 2+2 = 4$, $\mathcal{W}_1 \oplus \mathcal{W}_2 \subset \mathbb{R}^5$. Now, we attempt to compose $\mathcal{W}_1 \oplus \mathcal{W}_2 \oplus \mathcal{W}_3 = (\mathcal{W}_1 \oplus \mathcal{W}_2) \oplus \mathcal{W}_3$, which requires showing that the only generating vector $(1,1,-3,0,-1)^T$ in $\mathcal{W}_3$ is linearly independent from $\mathcal{W}_1 \oplus \mathcal{W}_2$. One way to do this is to show that the augmented system formed by appending $(1,1,-3,0,-1)^T$ to the matrix above
\begin{align*}
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & 2 & 0 & 0 & 1\\
0 & 1 & 3 & 0 & 1\\
2 & 0 & 1 & -1 & -3\\
1 & 0 & 0 & -2 & 0\\
0 & -1 & 0 & 1 & -1
\end{array}\right]    
\end{align*}
has no solution and thus $(1,1,-3,0,-1)^T$ cannot be written as the linear combination of the previous four vectors (see Properties \ref{proper:linearcombmatrix}). We can simply repeat the exactly same steps of Gaussian Elimination performed above, which would lead to
\begin{align*}
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}@{\;}|wc{10pt}@{\;}}
1 & 2 & 0 & 0 & 1\\
0 & 1 & 3 & 0 & 1\\
0 & 0 & 1 & \frac{1}{3} & 0\\
0 & 0 & 0 & 1 & -\frac{1}{4} \\
0 & 0 & 0 & 0 & -\frac{7}{3}
\end{array}\right]     
\end{align*}
where the last row is inconsistent. Therefore $(1,1,-3,0,-1)^T$ is linear independent from the first four vectors and $\mathcal{W}_1 \oplus \mathcal{W}_2 \oplus \mathcal{W}_3$ is a valid direct sum, and $\dim(\mathcal{W}_1 \oplus \mathcal{W}_2 \oplus \mathcal{W}_3) = \dim(\mathcal{W}_1 \oplus \mathcal{W}_2) + \dim(\mathcal{W}_3) = 4+1=5$. By Properties \ref{proper:dimWleqV}, $\mathcal{W}_1 \oplus \mathcal{W}_2 \oplus \mathcal{W}_3 = \mathbb{R}^5$. 
\end{solution}
The importance of direct sum is that the coordinates of two vectors in respective bases from the two subspaces can be simply concatenated when we add up both the vectors and bases, and \textit{this representation will be unique}. Going in the opposite direction, we can also split the coordinates of a direct sum back into the respective subspaces. Let's illustrate this with $\mathcal{W}_1$ and $\mathcal{W}_2$ in the above example. Using the given sets of generating vectors $\mathcal{X} = \{(1,0,2,1,0)^T, (2,1,0,0,-1)^T\}$ and $\mathcal{Y} = \{(0,3,1,0,0)^T, (0,0,-1,-2,1)^T\}$ as bases for $\mathcal{W}_1$ and $\mathcal{W}_2$, the coordinates $(1,2)_X^T$ and $(1,-1)^T_Y$ in the $\mathcal{X}$ and $\mathcal{Y}$ system, represent the vectors $1(1,0,2,1,0)^T + 2(2,1,0,0,-1)^T = (5,2,2,1,-2)^T$ and $1(0,3,1,0,0)^T + (-1)(0,0,-1,-2,1)^T = (0,3,2,2,-1)^T$ in $\mathbb{R}^5$ respectively. When they are summed, it yields $(5,2,2,1,-2)^T + (0,3,2,2,-1)^T = (5,5,4,3,-3)^T$. The basis formed by combining $\mathcal{X}$ and $\mathcal{Y}$ will be $\mathcal{X} \cup \mathcal{Y} = \{(1,0,2,1,0)^T, (2,1,0,0,-1)^T, (0,3,1,0,0)^T, (0,0,-1,-2,1)^T\}$, and the merged coordinates $(1,2,1,-1)_{X+Y}^T$ then correspond exactly to \\ $1(1,0,2,1,0)^T + 2(2,1,0,0,-1)^T + 1(0,3,1,0,0)^T + (-1)(0,0,-1,-2,1)^T = (5,5,4,3,-3)^T \in \mathcal{W}_1 \oplus \mathcal{W}_2 \subset \mathbb{R}^5$. The new coordinate representation $(1,2,1,-1)_{X+Y}^T$ is unique as $\mathcal{X} \cup \mathcal{Y}$ has been shown to be linearly independent in Example \ref{exmp:directsum} and Properties \ref{proper:lincombofspan} applies over the direct sum $\mathcal{W}_1 \oplus \mathcal{W}_2$, and it can be partitioned cleanly as $(1,2,1,-1)_{X+Y}^T = (1,2)_X^T + (1,-1)_Y^T$.

On the other hand, the uniqueness property will not hold if the subspace sum is not a direct sum. Let's use Example \ref{exmp:gentrimbasis} again as an illustration, where $\mathcal{X} = \mathcal{S}_1 = \{(1,0,1,1)^T, (1,2,0,-1)^T\}$ and $\mathcal{Y} = \mathcal{S}_2 = \{(2,2,1,0)^T, (0,1,0,1)^T\}$ and we have already shown that they are not linearly independent when combined. Take $(2,1)_X^T = 2(1,0,1,1)^T + 1(1,2,0,-1)^T = (3,2,2,1)^T$ and $(-1,1)_Y^T = (-1)(2,2,1,0)^T + 1(0,1,0,1)^T = (-2,-1,-1,1)^T$. Their concatenated sum will be $(2,1,-1,1)_{X+Y}^T = 2(1,0,1,1)^T + 1(1,2,0,-1)^T + (-1)(2,2,1,0)^T + 1(0,1,0,1)^T = (1,1,1,2)^T = (3,2,2,1)^T + (-2,-1,-1,1)^T$. But $(1,0,0,1)_{X+Y}^T = 1(1,0,1,1)^T + 0(1,2,0,-1)^T + 0(2,2,1,0)^T + 1(0,1,0,1)^T = (1,1,1,2)^T = (2,1,-1,1)_{X+Y}^T$ is aptly an alternative representation.

%This also completes our previous proof mentioned in Definition \ref{inverseidentity}.
%\begin{thm}
%If $AP = A$, and $A$ is a invertible square matrix, then $P$ must be $I$.
%\paragraph{Proof}
%The assumption implies that $A$ has linearly independent column vectors. As a result, they cannot be expressed by other vectors. Consider any one of the column vector, like $\vec{u_i}$, then the linear system
%\begin{align*}
%A\vec{x} &= ([\vec{u_1}|\cdots|\vec{u_i}|\cdots|\vec{u_n}])\vec{x} = x_1\vec{u_1} + \cdots + x_i\vec{u_i} + \cdots + x_n\vec{u_n} \\
%&= \vec{u_i}
%\end{align*}
%will only have the solution
%\begin{align*}
%x_j &= 1 & \text{if $j = i$} \\
%x_j &= 0 & \text{if $j \neq i$}
%\end{align*}
%This means that $\vec{x} = \hat{e_i}$. Now if we expand $P = [\vec{p_1}|\cdots|\vec{p_i}|\cdots|\vec{p_n}]$, then we can write $AP = A$ as
%\begin{align*}
%AP &= [A\vec{p_1}|\cdots|A\vec{p_i}|\cdots|A\vec{p_n}] \\
%&= A = [\vec{u_1}|\cdots|\vec{u_i}|\cdots|\vec{u_n}]
%\end{align*}
%The readers are encouraged to verify the expression of $AP = [A\vec{p_1}|\cdots|A\vec{p_i}|\cdots|A\vec{p_n}]$ as a mental exercise, as from time to time we will partition such matrix product into columns. We have just found that for $A\vec{p_i} = \vec{u_i}$ to hold, $\vec{p_i}$ must be $\hat{e_i}$. This implies $P = [\hat{e_1}|...|\hat{e_i}|...|\hat{e_n}] = I$.
%\end{thm}

\section{The Four Fundamental Subspaces Induced by Matrices}

\subsection{Row Space, Column Space}

From the last section, we know that for a matrix $A$ with $m$ rows and $n$ columns, it can be expressed in the form of $A = [\vec{u}_1|\vec{u}_2|\cdots|\vec{u}_n]$, with $\vec{u}_1, \vec{u}_2, \ldots, \vec{u}_n \in \mathbb{R}^m$. And the matrix product $A\vec{x}$ where $\vec{x} \in \mathbb{R}^n$ will generate a subspace that is just the span of these $n$ column vectors according to Definition \ref{defn:span}. This subspace is therefore known as the \index{Column Space}\keywordhl{column space} of $A$. Similarly, we can also define the \index{Row Space}\keywordhl{row space} of a matrix, as below.

\begin{defn}[Column/Row Space]
\label{defn:colrowspace}
For an $m \times n$ real matrix $A$, its column space $\mathcal{C}(A)$ is the subspace spanned by its $n$ column vectors, $\vec{u}_1, \vec{u}_2, \ldots, \vec{u}_n \in \mathbb{R}^m$ as in $A = [\vec{u}_1|\vec{u}_2|\cdots|\vec{u}_m]$; Meanwhile its row space $\mathcal{R}(A)$ is the subspace spanned by its $m$ row vectors $\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_m \in \mathbb{R}^n$ as in
\begin{align*}
A = 
\left[\begin{array}{c}
\vec{v}_1^T \\
\hline
\vec{v}_2^T \\
\hline
\vdots \\
\hline
\vec{v}_m^T
\end{array}\right]
\end{align*}
Notice that the row (column) space of a matrix $\mathcal{R}(A) = \mathcal{C}(A^T)$ ($\mathcal{C}(A) = \mathcal{R}(A^T)$) is just the column (row) space of its transpose. 
\end{defn}

For instance, in Example \ref{exmp:gentrimbasis}, the matrix
\begin{align*}
A &= 
\begin{bmatrix}
1 & 1 & 2 & 0\\
0 & 2 & 2 & 1\\
1 & 0 & 1 & 0\\
1 & -1 & 0 & 1
\end{bmatrix}
\end{align*}
actually has a column space of $\mathcal{C}(A) = \text{span}(\{(1,0,1,1)^T, (1,2,0,-1)^T, (0,1,0,1)^T\})$ of dimension $3$ despite the vectors are in $\mathbb{R}^4$. In deriving this result we have produced the reduced row echelon form of $A$, which is 
\begin{align*}
\begin{bmatrix}
1 & 0 & 1 & 0 \\
0 & 1 & 1 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 
\end{bmatrix}
\end{align*}
from which we can see the number of pivots, or \textit{rank}, is also $3$. In fact, just like the case above, the \index{Rank}\keywordhl{rank} of a matrix always indicates the dimension of its column space, and we have the following equivalent definitions.
\begin{defn}[Rank]
\label{defn:rank}
The rank of a matrix $A$ is the number of leading 1s in its reduced row echelon form, which is also the amount of linearly independent vectors in any basis of its column space, i.e. the dimension of the column space.
\end{defn}
The above statement makes sense because of the following property.
\begin{proper}
\label{proper:elemrowopcolrank}
Elementary row operations does not change the number of dimensions in the column space of a matrix.
\end{proper}
\begin{proof}
Let $A$ be an $m \times n$ matrix with a column space of the dimension $\dim(\mathcal{C}(A)) = r \leq n$. This implies that $n-r$ column vectors in $A$ are redundant, and thus can be written as the linear combination of other $r$ column vectors that are themselves linearly independent. Denote these $r$ linearly independent vectors as $\vec{u}_1, \vec{u}_2, \ldots, \vec{u}_r$, and the remaining redundant column vectors as $\vec{u}_{r+1}, \ldots, \vec{u}_n$. We can take any one of the last $n-r$ dependent column vectors, let's say $\vec{u}_{r+1} = c_1^{(r+1)}\vec{u}_1 + c_2^{(r+1)}\vec{u}_2 + \cdots + c_n^{(r+1)}\vec{u}_r$, written in a dependence relation of the first $r$ vectors. When we apply any elementary row operation on the entirety of $A$ and hence all $\vec{u}_j$, denote the new matrix as $A'$ and its new column vectors as $\vec{u}_j'$. It is not hard to show that the $\vec{u}_j'$ carries the same dependence relation for $\vec{u}_{r+1}', \cdots, \vec{u}_n'$\footnote{We will show this for the case of row addition/subtraction and leave the other two types of elementary row operations to the interested readers. Given \begin{align*}
\vec{u}_{r+1} = c_1^{(r+1)}\vec{u}_1 + c_2^{(r+1)}\vec{u}_2 + \cdots + c_r^{(r+1)}\vec{u}_r
\end{align*}i.e.
\begin{align*}
\begin{bmatrix}
\vdots \\
\vec{u}_{p, r+1} \\
\vdots \\
\vec{u}_{q, r+1} \\
\vdots
\end{bmatrix}
= c_1^{(r+1)}
\begin{bmatrix}
\vdots \\
\vec{u}_{p, 1} \\
\vdots \\
\vec{u}_{q, 1} \\
\vdots
\end{bmatrix}
+ c_2^{(r+1)}
\begin{bmatrix}
\vdots \\
\vec{u}_{p, 2} \\
\vdots \\
\vec{u}_{q, 2} \\
\vdots
\end{bmatrix} + \cdots + 
c_n^{(r+1)}
\begin{bmatrix}
\vdots \\
\vec{u}_{p, r} \\
\vdots \\
\vec{u}_{q, r} \\
\vdots
\end{bmatrix}
\end{align*}
The elementary row operation of adding $c_q$ times row $R_q$ to row $R_p$ produces a new matrix $A'$ with column vectors
\begin{align*}
\vec{u}_j'
=
\begin{bmatrix}
\vdots \\
\vec{u}_{p, j} + c_q(\vec{u}_{q,j}) \\
\vdots \\
\vec{u}_{q, j} \\
\vdots
\end{bmatrix}
\end{align*}
for all $j$. But adding $c_q$ times the $q$-th line: $c_q[\vec{u}_{q,r+1} = c_1^{(r+1)}\vec{u}_{q,1} + c_2^{(r+1)}\vec{u}_{q,2} + \cdots + c_r^{(r+1)}\vec{u}_{q,r}]$ to the $p$-th line in the previous system matrix equation also leads to
\begin{align*}
\begin{bmatrix}
\vdots \\
\vec{u}_{p, r+1} + c_q\vec{u}_{q, r+1} \\
\vdots \\
\vec{u}_{q, r+1} \\
\vdots
\end{bmatrix}
&= c_1^{(r+1)}
\begin{bmatrix}
\vdots \\
\vec{u}_{p, 1} + c_q\vec{u}_{q, 1} \\
\vdots \\
\vec{u}_{q, 1} \\
\vdots
\end{bmatrix}
+
c_2^{(r+1)}
\begin{bmatrix}
\vdots \\
\vec{u}_{p, 2} + c_q\vec{u}_{q, 2} \\
\vdots \\
\vec{u}_{q, 2} \\
\vdots
\end{bmatrix} \\
&\quad + \cdots + 
c_r^{(r+1)}
\begin{bmatrix}
\vdots \\
\vec{u}_{p, r} + c_q\vec{u}_{q, r} \\
\vdots \\
\vec{u}_{q, r} \\
\vdots
\end{bmatrix}
\end{align*}
which is just $\vec{u}_{r+1}' = c_1^{(r+1)}\vec{u}_1' + \cdots + c_r^{(r+1)}\vec{u}_r'$, showing that the same dependence relation holds for the new column vectors $\vec{u}_{r+1}'$, and similarly for up to $\vec{u}_n'$ in $A'$.} 
(and by extension the independence of $\vec{u}_1', \vec{u}_2', \cdots, \vec{u}_r'$\footnote{To see this, we can show the contrapositive, and use the same argument for the last footnote in the opposite direction, that is, if the new column vectors are linearly dependent, then the corresponding old column vectors have to be linearly dependent as well.}). Therefore, after the elementary row operation the new matrix also has these $r$ and $n-r$ linearly independent/redundant column vectors, and the dimension of $A'$ is still $r$.
\end{proof}
As a result, the matrix $A$ has the same number of dimensions in its column space throughout the Gaussian Elimination procedure, which coincides with the number of linearly independent vectors and thus pivots in the final reduced row echelon form, establishing the equivalence in Definition \ref{defn:rank}. However, notice that elementary row operations do change the actual column space. On the other hand, for row space, we have an even stronger result.
\begin{proper}
\label{proper:elemrowoprowrank}
Elementary row operations does not change the row space of a matrix, and thus its dimension.
\end{proper}
which is not hard to accept. Swapping rows, and multiplying a row by some constant obviously does not affect the span of rows in the matrix. Adding to/subtracting from a row $R_p$ (also as a row vector $\vec{v}_r^T$) by the constant multiple of another row $R_q$ ($\vec{v}_q^T$) also will not alter it. To see this, observe that the newly resulted row vector is just a linear combination of the two input rows, i.e. the new $R_p$ becomes $\vec{v}_{r} = \vec{v}_p + c\vec{v}_q$ (and hence $\vec{v}_p = \vec{v}_r - c\vec{v}_q$). Using part (b) of Theorem \ref{thm:plusminus} twice, we have
\begin{align*}
&= \text{span}(\{\ldots, \vec{v}_p, \ldots, \vec{v}_q, \ldots, \vec{v}_r \}) \\
\mathcal{R}(A) &= \text{span}(\{\ldots, \vec{v}_p, \ldots, \vec{v}_q, \ldots\}) \\
&= \text{span}(\{\ldots, \vec{v}_r, \ldots, \vec{v}_q, \ldots\}) = \mathcal{R}(A')
\end{align*}
where $A'$ denotes the matrix after the addition/subtraction elementary row operation. Our next key theorem relies on the observation that the dimensions of row and column space of a matrix in its reduced row echelon form are the same, or in other words,
\begin{proper}
\label{proper:rrefcolrowrank}
A matrix in reduced row echelon form has the same amount of (linearly independent) vectors in the basis of its row and column space.
\end{proper}
We will not read off the detailed arguments in the proof, but instead note that it is essentially an analysis of positions of the leading 1s and zeros in any reduced row echelon form. However, we will give an example to elucidate how it holds. Take a reduced row echelon form of
\begin{align*}
\begin{bmatrix}
1 & 1 & 0 & 0 & 1 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 1 \\
0 & 0 & 0 & 0 & 0 
\end{bmatrix}
\end{align*}
It is obvious that its column space is spanned by the basis $\{(1,0,0,0)^T, (0,1,0,0)^T, \\ (0,0,1,0)^T\}$, while a basis of its row space can be simply formed by the first three non-zero row vectors $\{(1,1,0,0,1), (0,0,1,0,0), (0,0,0,1,1)\}$. In this case, the dimension of row/column space of the reduced row echelon form is both $3$. With these observations, we can derive the desired result, sometimes referred to as \textit{"Column rank equals to row rank"}.
\begin{proper}
\label{proper:samecolrowrank}
For any matrix, the dimension of its column space is equal to that of its row space, i.e.
\begin{align*}
\dim(\mathcal{C}(A)) = \dim(\mathcal{R}(A)) = \dim(\mathcal{C}(A^T))
\end{align*}
\end{proper}
\begin{proof}
Any matrix has a unique reduced row echelon form due to Theorem \ref{thm:uniquerref}, whose  row/column space has the same number of dimensions by Properties \ref{proper:rrefcolrowrank}. According to Properties \ref{proper:elemrowopcolrank} and \ref{proper:elemrowoprowrank}, the elementary row operations done to convert the matrix to its reduced row echelon form conserves both the column rank and row rank, and thus the starting matrix must also has the same dimension in its row and column space.
\end{proof}
\begin{exmp}
\label{exmp:colrowspace}
Given a matrix
\begin{align*}
A = 
\begin{bmatrix}
1 & 1 & -2 & 1 \\
1 & 2 & 1 & -1 \\
1 & 0 & -5 & 3
\end{bmatrix}
\end{align*}
find a basis for its column/row space $\mathcal{C}(A)$ and $\mathcal{R}(A)$ and check if Properties \ref{proper:samecolrowrank} holds.
\end{exmp}
\begin{solution}
We first apply Gaussian Elimination to $A$, which leads to
\begin{align*}
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}@{\;}}
1 & 1 & -2 & 1 \\
1 & 2 & 1 & -1 \\
1 & 0 & -5 & 3
\end{array}\right]
& \rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}@{\;}}
1 & 1 & -2 & 1 \\
0 & 1 & 3 & -2 \\
0 & -1 & -3 & 2
\end{array}\right]
& \begin{aligned}
R_2 - R_1 &\to R_2 \\
R_3 - R_1 &\to R_3
\end{aligned} \\
& \rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}@{\;}}
1 & 1 & -2 & 1 \\
0 & 1 & 3 & -2 \\
0 & 0 & 0 & 0
\end{array}\right]
& R_3 - R_2 \to R_3 \\
& \rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}@{\;}}
1 & 0 & -5 & 3 \\
0 & 1 & 3 & -2 \\
0 & 0 & 0 & 0
\end{array}\right]
& R_1 - R_2 \to R_1
\end{align*}
The number of pivotal columns is $2$, and from the dependence relations we know that in the original matrix the first two column vectors $(1,1,1)^T$ and $(1,2,0)^T$ are linearly independent while the last two column vectors $(-2,1,-5)^T = -5(1,1,1)^T + 3(1,2,0)^T$ and $(1,-1,3)^T = 3(1,1,1)^T - 2(1,2,0)^T$ are linear combinations of the previous two. Hence $\mathcal{C}(A)$ has a basis of $\{(1,1,1)^T, (1,2,0)^T\}$ and $\dim(\mathcal{C}(A)) = 2$. On the other hand, to find the row space we consider $A^T$ and repeat the elimination process again as follows. However, notice that according to the dependence relations for the column vectors in $A$ above, we can immediately do the corresponding addition/subtraction operations for the rows in $A^T$, to reduce the third/fourth rows, obtaining
\begin{align*}
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}@{\;}}
1 & 1 & 1 \\
1 & 2 & 0 \\
-2 & 1 & -5 \\
1 & -1 & 3
\end{array}\right] 
&\rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}@{\;}}
1 & 1 & 1 \\
1 & 2 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right] 
& \begin{aligned}
R_3 + 5R_1 - 3R_2 &\to R_3, \\
R_4 - 3R_1 + 2R_2 &\to R_4
\end{aligned}
\end{align*}
and the next step is straight-forward:
\begin{align*}
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}@{\;}}
1 & 1 & 1 \\
1 & 2 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right] 
&\rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}@{\;}}
1 & 1 & 1 \\
0 & 1 & -1 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right] 
& R_2 - R_1 \to R_2 \\
&\rightarrow
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}@{\;}}
1 & 0 & 2 \\
0 & 1 & -1 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right] 
& R_1 - R_2 \to R_1
\end{align*}
which reveals that the first two columns (representing the first two row vectors in $A$) are linearly independent and the third column (the last row vector in $A$) is redundant ($(1,0,-5,3)^T = 2(1,1,-2,1)^T-(1,2,1,-1)^T$). Therefore $\mathcal{R}(A)$ has a basis of $\{(1,1,-2,1)^T, (1,2,1,-1)^T\}$, and $\dim(\mathcal{R}(A)) = 2 = \dim(\mathcal{C}(A))$, and Properties \ref{proper:samecolrowrank} is true in this case.
\end{solution}
Finally, in view of Definitions \ref{defn:span} and \ref{defn:colrowspace}, we recast the analysis in Section \ref{section:SolveLinSys} as 
\begin{proper}
A linear system $A\vec{x} = \vec{h}$ is consistent if and only if $\vec{h}$ is in the column space of $A$.
\end{proper}

\subsection{Null Space, Rank-Nullity Theorem}
\label{section:null}

As we have briefly mentioned in the end of last chapter, the solution of a linear system $A\vec{x} = \vec{h}$, where $A$ is an $m \times n$ matrix and $\vec{x} \in \mathbb{R}^n$, can be viewed as some sort of a solution space. In Section \ref{subsection:SolLinSysGauss} we know that it is made up of the particular and complementary solution, where the latter corresponding to the family of $\vec{x} = \vec{x}_0$ that satisfies the homogeneous part $A\vec{x} = \textbf{0}$. The set $\vec{x}_0 \in \mathbb{R}^n$ can be shown to form a subspace of $\mathbb{R}^n$\footnote{To show this we check the two conditions in Theorem \ref{thm:subspacecriteria}. Let $\vec{x}_0^{(1)}$ and $\vec{x}_0^{(2)}$ be two vectors in the null space $\vec{x}_0$. Then we have: 1. $A(\vec{x}_0^{(1)} + \vec{x}_0^{(2)}) = A\vec{x}_0^{(1)} + A\vec{x}_0^{(2)} = \textbf{0} + \textbf{0} = \textbf{0}$, so $\vec{x}_0^{(1)} + \vec{x}_0^{(2)} \in \vec{x}_0$, and 2. $A(a\vec{x}_0^{(1)}) = a(A\vec{x}_0^{(1)}) = a\textbf{0} = \textbf{0}$, hence $a\vec{x}_0^{(1)} \in \vec{x}_0$.}, and this subspace is then called the \index{Null Space}\keywordhl{null space} of $A$.
\begin{defn}[Null Space]
\label{defn:nullspace}
For an $m \times n$ real matrix $A$, its null space $\mathcal{N}(A)$ is the subspace consisted of all solution vectors $\vec{x} = \vec{x}_0 \in \mathbb{R}^n$ to the matrix equation $A\vec{x} = \textbf{0}$. The dimension of null space is called \index{Nullity}\keywordhl{nullity}.
\end{defn}

A notable relationship between row space and null space is that any pair of two vectors coming from the respective subspaces will be orthogonal to each other. This means that the two subspaces are the \index{Orthogonal Complement}\keywordhl{orthogonal complement} (denoted by the superscript $^\perp$) of each other.
\begin{proper}
\label{proper:rownullortho}
Given a real matrix $A$, any vector in its row space $\mathcal{R}(A)$ is orthogonal to all vectors in its null space $\mathcal{N}(A)$ and vice versa, such that $\mathcal{R}(A)^\perp = \mathcal{N}(A)$ and $\mathcal{N}(A)^\perp = \mathcal{R}(A)$.
\end{proper}
\begin{proof}
Let the shape of $A$ be $m \times n$, we can express $A$ in the form of its row vectors as
\begin{align*}
A = 
\left[\begin{array}{c}
\vec{v}_1^T \\
\hline
\vec{v}_2^T \\
\hline
\vdots \\
\hline
\vec{v}_m^T
\end{array}\right]
\end{align*}
and the corresponding homogeneous system $A\vec{x} = \textbf{0}$ then can be written as
\begin{align*}
A\vec{x} =
\left[\begin{array}{c}
\vec{v}_1^T \\
\hline
\vec{v}_2^T \\
\hline
\vdots \\
\hline
\vec{v}_m^T
\end{array}\right]
\vec{x}
=
\left[\begin{array}{c}
\vec{v}_1^T\vec{x} \\
\vec{v}_2^T\vec{x} \\
\vdots \\
\vec{v}_m^T\vec{x}
\end{array}\right]
= \textbf{0}
=
\left[\begin{array}{c}
0 \\
0 \\
\vdots \\
0
\end{array}\right]
\end{align*}
where for a solution $\vec{x} = \vec{x}_0$ in the null space of $A$, each of the dot products $\vec{v}_i^T\vec{x}_0 = 0$, $i = 1, 2, \ldots, m$, has to be equal to zero. Any vector in the row space of $A$ can be expressed as $\vec{v} = c_1\vec{v}_1 + c_2\vec{v}_2 + \cdots + c_m\vec{v}_m$ by Definitions \ref{defn:colrowspace} and \ref{defn:span}, and subsequently, its dot product with $\vec{x}_0$
\begin{align*}
\vec{v}^T\vec{x}_0 &= (c_1\vec{v}_1 + c_2\vec{v}_2 + \cdots + c_m\vec{v}_m)^T\vec{x}_0 \\
&= c_1(\vec{v}_1^T\vec{x}_0) + c_2(\vec{v}_2^T\vec{x}_0) + \cdots + c_m(\vec{v}_m^T\vec{x}_0) \\
&= c_1(0) + c_2(0) + \cdots + c_m(0) = 0
\end{align*}
is also zero, therefore they are orthogonal by Properties \ref{proper:dotorth}, which implies that any vector in $\mathcal{R}(A)$ is orthogonal to any another vector in $\mathcal{N}(A)$.
\end{proof}
As a corollary, this is equivalent to all vectors in the generating set or basis for the row space for a matrix being orthogonal to all vectors in those for its null space. The following additional observation will be useful later.
\begin{proper}
\label{proper:ortholinind}
Non-zero orthogonal vectors are linearly independent.
\end{proper}
\begin{proof}
We will only prove the case with two vectors in $\mathbb{R}^n$ but those with multiple vectors can be derived in the same essence. Consider $c_1\vec{u}_1 + c_2\vec{u}_2 = \textbf{0}$ where $\vec{u}_1$ and $\vec{u}_2$ are orthogonal, i.e. $\vec{u}_1 \cdot \vec{u}_2 = 0$. Taking dot product with $\vec{u}_1$ on both sides gives
\begin{align*}
\vec{u}_1 \cdot (c_1\vec{u}_1 + c_2\vec{u}_2) = c_1(\vec{u}_1 \cdot \vec{u}_1) + c_2(\vec{u}_1 \cdot \vec{u}_2) &= \vec{u}_1 \cdot \textbf{0} \\
c_1 \norm{\vec{u}_1}^2 + c_2 (0) = c_1 \norm{\vec{u}_1}^2 &= 0
\end{align*}
Since $\vec{u}_1$ is non-zero, $\norm{\vec{u}_1}^2 > 0$, and $c_1$ must be zero. Similarly, $c_2$ is zero as well. Therefore the only solution to the equation $c_1\vec{u}_1 + c_2\vec{u}_2 = \textbf{0}$ is the trivial solution $c_1 = c_2 = 0$. By Theorem \ref{thm:linearindep}, the two vectors are linearly independent.
\end{proof}
\begin{exmp}
\label{exmp:colrowspace2}
For the matrix in Example \ref{exmp:colrowspace}, find its null space and check if Properties \ref{proper:rownullortho} holds.
\end{exmp}
\begin{solution}
The homogeneous system corresponding to the matrix is
\begin{align*}
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & 1 & -2 & 1 & 0\\
1 & 2 & 1 & -1 & 0\\
1 & 0 & -5 & 3 & 0
\end{array}\right]
\end{align*}
which can be reduced, following the same steps in Example \ref{exmp:colrowspace}, to
\begin{align*}
\left[\begin{array}{@{\;}wc{10pt}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}@{\;}}
1 & 0 & -5 & 3 & 0 \\
0 & 1 & 3 & -2 & 0\\
0 & 0 & 0 & 0 & 0
\end{array}\right]
\end{align*}
where there are two non-pivotal columns and hence two free parameters can be assigned to them. Let $x_3 = s$ and $x_4 = t$, then $x_1 = 5s - 3t$ and $x_2 = -3s + 2t$. So the solution to the system is
\begin{align*}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4
\end{bmatrix}
=
\begin{bmatrix}
5s-3t \\
-3s+2t \\
s \\
t
\end{bmatrix}
=
s
\begin{bmatrix}
5\\
-3\\
1\\
0
\end{bmatrix}
+ t
\begin{bmatrix}
-3 \\
2 \\
0 \\
1
\end{bmatrix}
\end{align*}
and thus a basis for the null space is $\{(5,-3,1,0)^T, (-3,2,0,1)^T\}$ where these two vectors are clearly linearly independent. As found in Example \ref{exmp:colrowspace}, the basis for its row space is $\{(1,1,-2,1)^T, (1,2,1,-1)^T\}$. Subsequently, checking orthogonality between the two bases is straight-forward, and we will only do this for the first vector in the row space basis against the null space basis.
\begin{align*}
(5,-3,1,0)^T \cdot (1,1,-2,1)^T &= (5)(1)+(-3)(1)+(1)(-2)+(0)(1) = 0 \\
(-3,2,0,1)^T \cdot (1,1,-2,1)^T &= (-3)(1)+(2)(1)+(0)(-2)+(1)(1) = 0
\end{align*}
Furthermore, the dimension of null space, or the nullity, is $\dim{\mathcal{N}(A)} = 2$.
\end{solution}
Notice that like in the example above, or solving other homogeneous linear systems with Gaussian Elimination back in Section \ref{section:SolveLinSys}, the number of free variables is always equal to the number of columns subtracted by that of leading 1s. Rephrasing this statement using Definitions \ref{defn:colrowspace}, \ref{defn:rank}, and \ref{defn:nullspace}, it means that the rank of a matrix plus its nullity equals to its number of columns, which leads to the so-called \index{Rank-nullity Theorem}\keywordhl{Rank-nullity Theorem}.
\begin{thm}[Rank-nullity Theorem]
\label{thm:ranknullity}
For a real $m \times n$ matrix $A$, we have
\begin{align*}
\dim(\mathcal{C}(A)) + \dim(\mathcal{N}(A)) &= \text{rank}(A) + \text{nullity}(A) = n \\
&= \dim(\mathcal{R}(A)) + \dim(\mathcal{N}(A))
\end{align*}
\end{thm}
For instance, in Examples \ref{exmp:colrowspace} and  \ref{exmp:colrowspace2}, we can see that $\text{rank}(A) + \text{nullity}(A) = 2+2 = 4$.\\
\\
Short Exercise: Show that \footnote{Replace $A$ by $A^T$ in the theorem above to get $\dim(\mathcal{C}(A^T)) + \dim(\mathcal{N}(A^T)) = m$ and note that $\mathcal{C}(A^T) = \mathcal{R}(A)$.}
\begin{align*}
\dim(\mathcal{R}(A)) + \dim(\mathcal{N}(A^T)) = \dim(\mathcal{C}(A)) + \dim(\mathcal{N}(A^T)) = m   
\end{align*} $\mathcal{N}(A^T)$ is also known as the \index{Left Null Space}\keywordhl{left null space} of $A$.

Since the row space and null space of a matrix are orthogonal complements by Properties \ref{proper:rownullortho}, and Properties \ref{proper:ortholinind} shows that vectors in the two subspaces are linearly independent with respect to each other, they can form a direct sum according to Definition \ref{defn:directsum}. For an $m \times n$ matrix $A$, it is expressed as $\mathcal{R}(A) \oplus \mathcal{N}(A)$. Notice that it is a subspace of $\mathbb{R}^n$. From the Rank-nullity Theorem, $\dim(\mathcal{R}(A)) + \dim(\mathcal{N}(A)) = n$, and by Properties \ref{proper:dimWleqV}, we then must have $\mathcal{R}(A) \oplus \mathcal{N}(A) = \mathbb{R}^n$, in other words, the row space and null space of a matrix can reconstruct the real $n$-space by composing their direct sum. The similar can be said for its column and left null space.
\begin{proper}
\label{proper:funsubsortho}
For a real $m \times n$ matrix $A$, we have
\begin{align*}
& \mathcal{R}(A) \oplus \mathcal{N}(A) = \mathbb{R}^n & & \mathcal{C}(A) \oplus \mathcal{N}(A^T) = \mathbb{R}^m
\end{align*}
\end{proper}

We conclude the relationships between the column, row, null, and left null space, a.k.a the \index{The Four Fundamental Subspaces}\keywordhl{Four fundamental subspaces} induced by a matrix, with a diagram (Figure \ref{fig:foursubspaces}).
\begin{figure}
    \centering
    \begin{tikzpicture}[>={Stealth[length=5pt]}]
    \node[opacity=0.1,scale=5] at (0,0) {$\mathbb{R}^n$}; 
    \node[opacity=0.1,scale=5] at (8,0) {$\mathbb{R}^m$};
    \draw[SkyBlue] (0,0) -- (2,2.8) -- (-0.1,4.3) -- (-2.1,1.5) -- cycle; 
    \node[align=left, scale=0.7] at (0,2.3) {Row space \\ $\mathcal{R}(A) = \mathcal{C}(A^T)$};
    \draw[SkyBlue] (0,0) -- (-1, -1.4) -- (1.8,-3.4) -- (2.8,-2) -- cycle;
    \node[align=left, scale=0.7] at (0.9,-1.7) {Null space \\ $\mathcal{N}(A)$};
    \draw (0.2, 0.28) -- (0.48, 0.08) -- (0.28, -0.2);
    \node at (0,5) {$\dim = r$};
    \node at (-1,-3) {$\dim = n-r$};
    \draw[CarnationPink] (8,0) -- (6,2.8) -- (8.1,4.3) -- (10.1,1.5) -- cycle;
    \node[align=left,scale=0.7] at (8.1,2) {Column space \\ $\mathcal{C}(A)$};
    \draw[CarnationPink] (8,0) -- (8.8,-8/7) -- (4.8,-2.8-8/7) -- (4,-2.8) -- cycle;
    \node[align=left,scale=0.7] at (6.6,-2) {Left null space \\ $\mathcal{N}(A^T)$};
    \draw (7.8, 0.28) -- (7.52, 0.08) -- (7.72, -0.2);
    \node at (6.5,4.5) {$\dim = r$};
    \node at (9,-2.5) {$\dim = m-r$};
    \node[scale=2.5, opacity=0.5] at (3.5,4) {$A_{m \times n}$};
    \node[circle,fill,inner sep=2pt,Green] at (0,0) {};
    \node at (-0.5,0) {$\textbf{0}$};
    \node[circle,fill,inner sep=2pt,yellow] at (8,0) {};
    \node at (8.5,0) {$\textbf{0}$};
    \draw[->] (1,0.5) -- (7,2.5) node[sloped, midway, below]{$A\vec{x} = \vec{h}$};
    \draw[dashed, -{Circle}] (1,0.5) -- (-0.4,1.5) node[left]{$\vec{x}_r$};
    \draw[dashed, -{Circle}] (1,0.5) -- (-0.2,-1.18) node[below]{$\vec{x}_n$};
    \node[circle,fill,inner sep=2pt] at (1,0.5) {};
    \node at (1,0.5) [below right]{$\vec{x} = \vec{x}_r + \vec{x}_n$};
    \draw[dashed, ->] (-0.2,-1.18) -- (8,0) node[sloped, pos=0.7, above]{$A\vec{x}_n = \textbf{0}$};
    \draw[dashed, ->] (-0.4,1.5) -- (7,2.5) node[sloped, midway, above]{$A\vec{x}_r = \vec{h}$};
    \node[above] at (7,2.6) {$\vec{h}$};
    \end{tikzpicture}
    \caption{The relationships between the four fundamental subspaces for an $m \times n$ real matrix $A$ of rank $r$: the row space $\mathcal{R}(A) = \mathcal{C}(A^T)$, null space $\mathcal{N}(A)$, column space $\mathcal{C}(A)$, left null space $\mathcal{N}(A^T)$. Any vector $\vec{x} \in \mathbb{R}^n$ can be partitioned into $\vec{x} = \vec{x}_r + \vec{x}_n$ uniquely, where $\vec{x}_r \in \mathcal{R}(A) \subseteq \mathbb{R}^n$ and $\vec{x}_n \in \mathcal{N}(A) \subseteq \mathbb{R}^n$ are in the row/null space of $A$ respectively. The matrix $A$ maps $\vec{x}_n$ to the zero vector in $\mathbb{R}^m$ and $\vec{x}_r$ to some vector $\vec{h} \in \mathcal{C}(A) \subseteq \mathbb{R}^m$ in the column space of $A$. The total effect on $\vec{x}$ multiplied by $A$, is the sum of the two responses: $A\vec{x} = A(\vec{x}_r + \vec{x}_n) = A\vec{x}_r + A\vec{x}_n = \vec{h} + \textbf{0} = \vec{h}$.}
    \label{fig:foursubspaces}
\end{figure}

\section{Python Programming}
To check linear independence and find a basis for columns in a matrix, we can use the \verb|columnspace| method in \verb|sympy|. Let's test it with the matrix in Example \ref{exmp:colrowspace}.
\begin{lstlisting}
import sympy

myMatrix = sympy.Matrix([[1., 1., -2., 1.],
                         [1., 2., 1., -1.],
                         [1., 0., -5., 3.]])
print(myMatrix.columnspace())
\end{lstlisting}
which gives
\begin{lstlisting}
[Matrix([        
[1.0],
[1.0],
[1.0]]), 
Matrix([
[1.0],
[2.0],
[  0]])]   
\end{lstlisting}
as expected. The rank can be found in two ways.
\begin{lstlisting}
print(myMatrix.rank()) # or len(myMatrix.columnspace())
\end{lstlisting}
This returns \verb|2| correctly. We can make a basis for the row space similarly by the \verb|rowspace| method. In the same manner, the null space is computed by the \verb|nullspace| method:
\begin{lstlisting}
print(myMatrix.nullspace())
\end{lstlisting}
producing an output of
\begin{lstlisting}
[Matrix([
[ 5.0],
[-3.0],
[   1],
[   0]]), 
Matrix([
[-3.0],
[ 2.0],
[   0],
[   1]])]
\end{lstlisting}
The nullity is then simply calculated by \verb|len(myMatrix.nullspace())|, which gives a right answer of \verb|2|.

\section{Exercises}

\begin{Exercise}
For $\vec{u}_1 =
\begin{bmatrix}
1\\
1\\
0
\end{bmatrix}$,
$\vec{u}_2 =
\begin{bmatrix}
1\\
0\\
1
\end{bmatrix}$,
$\vec{u}_3 =
\begin{bmatrix}
0\\
1\\
1
\end{bmatrix}$,
find the constants $a$, $b$, $c$ such that their linear combination $a\vec{u}_1 + b\vec{u}_2 + c\vec{u}_3$ equals to 
\begin{enumerate}[label=(\alph*)]
\item $(3,2,9)^T$, 
\item $(9,1,5)^T$.
\end{enumerate}
\end{Exercise}

\begin{Exercise}
Determine if the following sets of vectors are linearly independent.
\begin{enumerate}[label=(\alph*)]
\item $\vec{u} = (2,-1)^T$, $\vec{v} = (-4,2)^T$,
\item $\vec{u} = (1,2,3)^T$, $\vec{v} = (6,7,9)^T$, $\vec{w} = (4,8,5)^T$, and
\item $\vec{u} = (1,3,3)^T$, $\vec{v}=(3,2,9)^T$, $\vec{w} = (1,-4,3)^T$.
\end{enumerate}
\end{Exercise}

\begin{Exercise}
For the basis $\mathcal{B}$: $\vec{u}_1 = 
\begin{bmatrix}
6\\
1\\
2
\end{bmatrix}$,
$\vec{u}_2 = 
\begin{bmatrix}
1\\
0\\
1
\end{bmatrix}$,
$\vec{u}_3 = 
\begin{bmatrix}
2\\
3\\
3
\end{bmatrix}$
(relative to the standard basis $\mathcal{E}$), do the following coordinate conversion.
\begin{enumerate}[label=(\alph*)]
\item Express $(5, 2, 3)^T$ of $\mathcal{E}$ in $\mathcal{B}$,
\item Recover $(1, -1, 1)^T_B$ from $\mathcal{B}$ back to the the standard basis $\mathcal{E}$.
\end{enumerate}
\end{Exercise}

\begin{Exercise}
Prove that for any two subspaces $\mathcal{W}_1, \mathcal{W}_2 \subseteq \mathcal{V}$. Their intersection $\mathcal{W}_1 \cap \mathcal{W}_2$ is also a subspace of $\mathcal{V}$. How about their union?
\end{Exercise}

\begin{Exercise}
Show that $\mathcal{W}_1 = \text{span}(\{(1,0,0,1)^T, (0,1,-1,1)^T\})$ and $\mathcal{W}_2 = \text{span}(\{(1,0,1,-1)^T\})$ can be composed to produce a direct sum $\mathcal{W}_1 \oplus \mathcal{W}_2$. Find bases for $\mathcal{W}_1$, $\mathcal{W}_2$ and hence this direct sum. Express $(2,0,1,0)^T$ in the direct sum basis, and partition it as coordinates of the two smaller subspaces.
\end{Exercise}

\begin{Exercise}
Find (bases for) the column, row, null and left null space of 
\begin{align*}
A = 
\begin{bmatrix}
1 & 0 & 1 & 1 \\
0 & 1 & -1 & 1 \\
1 & 2 & -1 & 0 \\
1 & 0 & 1 & 0
\end{bmatrix}
\end{align*}
and check if Theorem \ref{thm:ranknullity} and Properties \ref{proper:funsubsortho} hold in this case.
\end{Exercise}