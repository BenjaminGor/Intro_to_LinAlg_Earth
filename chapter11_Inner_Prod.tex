\chapter{Inner Product Spaces}
\label{chap:innerchap}

In Chapter \ref{chap:vec_space}, we have discussed what it means to be a vector space. Previously, it is accompanied by the dot product operation as defined in Section \ref{section:dotprod} that gives rise to the notion of \textit{Euclidean} distance in $\mathbb{R}^n$ (complex dot product for $\mathbb{C}^n$). In this chapter, we will show that the usual dot product is not the only way to measure the distance between vectors: we can equip any vector space with a so-called \textit{inner product} that fulfills certain criteria and leads to an alternative expression for the length of vectors, in place of the dot product. This generalizes a vector space to an \textit{inner product space}, and many concepts for a vector space, like orthogonality, can be adapted to be applied in inner product spaces. Particularly, we will have the \textit{adjoint} as an inner product space equivalent to the usual transpose. Finally, we will talk about \textit{special polynomials} which are generated by considering suitable inner products and are often used in Earth Science applications, such as solving \textit{partial differential equations (PDEs)}.

\section{Definition and Properties of Inner Product Spaces}

\subsection{Requirements of Inner Products}

As introduced in the beginning, an \index{Inner Product Space}\keywordhl{inner product space} is a vector space (either real or complex) that comes with an \index{Inner Product}\keywordhl{inner product} operation that is akin to the usual dot product. To qualify as a valid inner product, it has to fulfill four requirements as suggested below.
\begin{defn}[Inner Product (Space)]
\label{defn:innerprod}
An inner product on a vector space $\mathcal{V}$ over $\mathbb{R}$ [$\mathbb{C}$] is a function that takes a pair of vectors $\vec{u}, \vec{v} \in \mathcal{V}$ as the input and returns an $\mathbb{R}$ [$\mathbb{C}$] scalar, denoted by $\langle \vec{u}, \vec{v} \rangle$. Then, for any $\vec{u}, \vec{v}, \vec{w} \in \mathcal{V}$ and scalar $a \in \mathbb{R}$ [$\mathbb{C}$], the following four axioms have to hold.
\begin{enumerate}
    \item $\langle \vec{u}, \vec{v} \rangle = \langle \vec{v}, \vec{u} \rangle$ [$\langle \vec{u}, \vec{v} \rangle = \overline{\langle \vec{v}, \vec{u} \rangle}$] ([Conjugate] Symmetry);
    \item $\langle \vec{u}+\vec{v}, \vec{w} \rangle = \langle \vec{u}, \vec{w} \rangle + \langle \vec{v}, \vec{w} \rangle$ (Additivity);
    \item $\langle a\vec{u}, \vec{v} \rangle = a\langle \vec{u}, \vec{v} \rangle$ (Homogeneity); and
    \item $\langle \vec{v}, \vec{v} \rangle \geq 0$, and $\langle \vec{v}, \vec{v} \rangle = 0$ if and only if $\vec{v} = \textbf{0}$ is the zero vector. (Positivity)
\end{enumerate}
The second and third condition can be combined into \textit{linearity} (in the first argument): given another scalar $b \in \mathbb{R}$ [$\mathbb{C}$], we have $\langle a\vec{u}+b\vec{v}, \vec{w} \rangle = a\langle \vec{u}, \vec{w} \rangle + b\langle \vec{v}, \vec{w} \rangle$. A real [complex] vector space equipped with an inner product is then known as a real [complex] inner product space. 
\end{defn}
$\blacktriangleright$ Short Exercise: Show that $\langle a\vec{u}, b\vec{v} \rangle = a\overline{b}\langle \vec{u}, \vec{v} \rangle$ for any complex inner product space.\footnotemark\par
For now, we will limit ourselves to finite-dimensional vector/inner product spaces until Section \ref{section:infinner}. It is not hard to verify that the above axioms hold for the usual dot product, so $\mathbb{R}^n$ [as well as $\mathbb{C}^n$] is automatically an inner product space when the [complex] dot product is equipped. In this case, they are known as the \index{Standard Inner Product}\keywordhl{standard inner product} and \index{Euclidean $n$-space}\keywordhl{Euclidean $n$-space}.\footnotetext{$\langle a\vec{u}, b\vec{v} \rangle = a\langle \vec{u}, b\vec{v} \rangle = a\overline{\langle b\vec{v}, \vec{u} \rangle} = a\overline{b\langle \vec{v}, \vec{u} \rangle} = a(\overline{b}) \overline{\langle \vec{v}, \vec{u} \rangle} = a\overline{b}\langle \vec{u}, \vec{v} \rangle$ by the first and third axiom.} However, $\mathbb{R}^n$ will be a different inner product space when another inner product is used. It can be shown that all alternative inner products that can be applied to $\mathbb{R}^n$ are precisely positive-definite symmetric bilinear forms as the extension of positive-definite quadratic forms introduced in the last chapter. Given any positive-definite symmetric bilinear form $B$, it can be verified that $\langle \vec{u}, \vec{v} \rangle = \textbf{u}^TB\textbf{v}$ will satisfy the real inner product axioms.\footnote{For (1): $\langle \vec{u}, \vec{v} \rangle = \textbf{u}^TB\textbf{v} = (\textbf{u}^TB\textbf{v})^T$ since it is only a scalar, and then $(\textbf{u}^TB\textbf{v})^T = \textbf{v}^TB^T\textbf{u} = \textbf{v}^TB\textbf{u} = \langle \vec{v}, \vec{u} \rangle$ due to Properties \ref{proper:transp} and because $B$ is symmetric. (2) and (3) are obvious. For (4), as $B$ is required to be positive-definite, $\langle \vec{v}, \vec{v} \rangle = \textbf{v}^TB\textbf{v} > 0$ by Definition \ref{defn:quaddefinite} as long as $\vec{v} \neq \textbf{0}$ and it is apparent that $\langle \vec{v}, \vec{v} \rangle = \textbf{0}^TB\textbf{0} = 0$ when $\vec{v} = \textbf{0}$.} Below are some other properties of inner products extended from the axioms that can be compared to Properties \ref{proper:dotproper} and \ref{proper:complexdot}.
\begin{proper}
\label{proper:innerprod2}
For vectors $\vec{u}, \vec{v}, \vec{w} \in \mathcal{V}$ and scalar $b \in \mathbb{R}$ [$\mathbb{C}$] in a real [complex] inner product space, we have
\begin{enumerate}
    \item $\langle \vec{u} \pm \vec{v}, \vec{w} \rangle = \langle \vec{u}, \vec{w} \rangle \pm \langle \vec{v}, \vec{w} \rangle$ (Distributive Property);
    \item $\langle \vec{u}, \vec{v} \pm \vec{w} \rangle = \langle \vec{u}, \vec{v} \rangle \pm \langle \vec{u}, \vec{w} \rangle$ (Distributive Property);
    \item $\langle \vec{v}, \textbf{0} \rangle = \langle \textbf{0}, \vec{v} \rangle = 0$;
    \item $\langle \vec{u}, b\vec{v} \rangle = b\langle \vec{u}, \vec{v} \rangle$ [$\langle \vec{u}, b\vec{v} \rangle = \bar{b}\langle \vec{u}, \vec{v} \rangle$];
    \item if $\langle \vec{u}, \vec{v} \rangle = \langle \vec{u}, \vec{w} \rangle$ for all $\vec{u}$, then $\vec{v} = \vec{w}$.
\end{enumerate}
\end{proper}
\begin{proof}
We will skip (1). (2): $\langle \vec{u}, \vec{v} \pm \vec{w} \rangle = \overline{\langle \vec{v} \pm \vec{w}, \vec{u} \rangle} = \overline{\langle \vec{v}, \vec{u} \rangle} \pm \overline{\langle \vec{w}, \vec{u} \rangle} = \langle \vec{u}, \vec{v} \rangle \pm \langle \vec{u}, \vec{w} \rangle$ by the first and second axiom. (3): $\langle \textbf{0}, \vec{v} \rangle = \langle 0\vec{u}, \vec{v} \rangle = 0\langle \vec{u}, \vec{v} \rangle = 0$ using arbitrary $\vec{u}$ and the third axiom. (4) simply follows from the last short exercise with $a = 1$. For (5):
\begin{align*}
\langle \vec{u}, \vec{v} \rangle &= \langle \vec{u}, \vec{w} \rangle \\
\langle \vec{u}, \vec{v} - \vec{w} \rangle &= 0 & \text{(By (1))}
\end{align*}
Then by letting $\vec{u} = \vec{v} - \vec{w}$ and using the last axiom we get $\vec{v} - \vec{w} = \textbf{0}$ and thus $\vec{v} = \vec{w}$.
\end{proof}

\subsection{Generalization of Length and Orthogonality via Inner Products}

As noted earlier, the idea of inner products extends the usual dot product, and we may ask how the notion of vector length, which can be expressed via the dot product of the vector with itself (Properties \ref{proper:lengthdot}), is carried over to an inner product space. The most natural generalization is to simply replace the dot product with an inner product in Properties \ref{proper:lengthdot} when computing such a "length", which is now more properly known as a \index{Norm}\keywordhl{norm}. This makes physical sense as the last axiom in Definition \ref{defn:innerprod} forces the norm to always be positive ($0$ if it is the zero vector) just like the usual length.
\begin{proper}[Vector Norm]
\label{proper:norminner}
The norm of a vector in an inner product space is induced by
\begin{align}
\norm{\vec{v}} &= \sqrt{\langle\vec{v} , \vec{v}\rangle} & &\text{or equivalently} &
\norm{\vec{v}}^2 &= \langle\vec{v} , \vec{v}\rangle
\end{align}
\end{proper}
where $\langle \cdot, \cdot \rangle$ is an inner product following Definition \ref{defn:innerprod}.
Unit vectors with respect to a specific inner product are then conveniently created using this definition of norm.
\begin{defn}
\label{defn:unitvecinner}
The unit vector of a non-zero vector $\vec{v} \in \mathcal{V}$ in an inner product space is denoted as $\hat{v}$ and is given by
\begin{align}
\hat{v} &= \frac{1}{\norm{\vec{v}}}\vec{v}
\end{align}
where the norm $\norm{\vec{v}}$ is now defined as in Properties \ref{proper:norminner}. 
\end{defn}
The notion of orthogonality (as well as orthonormality) in Properties \ref{proper:dotorth} is also transferred to an inner product space by the same essence of replacing the dot product with an inner product.
\begin{proper}
\label{proper:orthoinner}
Two vectors $\vec{u}, \vec{v} \in \mathcal{V}$ in an inner product space are said to be orthogonal with respect to the inner product when $\langle\vec{u}, \vec{v}\rangle = \langle\vec{v}, \vec{u}\rangle = 0$.
\end{proper}
Some other related results derived using dot product are also valid when inner products are used instead and we note them below.
\begin{thm}[Cauchy-Schwarz Inequality]
\label{thm:cauchyschinner}
Given two vectors $\vec{u}, \vec{v} \in \mathcal{V}$ in an inner product space, we have
\begin{align}
\abs{\langle\vec{u} , \vec{v}\rangle} \leq \norm{\vec{u}}\norm{\vec{v}} =  \sqrt{\langle\vec{u} , \vec{u}\rangle} \sqrt{\langle\vec{v} , \vec{v}\rangle}
\end{align}
\end{thm}
The proof is essentially the same as the one in Theorem \ref{thm:CauchySch} but with all the dot product expressions replaced by the inner product ones. Also
\begin{proper}
Non-zero orthogonal vectors with respect to any inner product are linearly independent.
\end{proper}
Again, the proof follows the same line of arguments as in Properties \ref{proper:ortholinind}, with the dot product changed to an inner product.

\begin{exmp}
Show that $\vec{u} = (1,2)^T$ and $\vec{v} = (-3,4)^T$ in $\mathbb{R}^2$ are orthogonal to each other if the inner product used is given by
\begin{align*}
\langle\vec{u}, \vec{v}\rangle = \textbf{u}^TB\textbf{v}
\end{align*}
where 
\begin{align*}
B = 
\begin{bmatrix}
2 & 1 \\
1 & 1
\end{bmatrix}
\end{align*}
\end{exmp}
\begin{solution}
First, we have to make sure the inner product defined as a symmetric bilinear form above is indeed valid, particularly it has to be positive-definite. By Theorem \ref{thm:quaddefinite}, it simply amounts to checking if the eigenvalues are all positive. A simple calculation reveals that $\lambda = \smash{\frac{3+\sqrt{5}}{2}, \frac{3-\sqrt{5}}{2}} > 0$ so we can proceed to calculate
\begin{align*}
\langle\vec{u}, \vec{v}\rangle &= \textbf{u}^TB\textbf{v} \\
&= \begin{bmatrix}
1 & 2
\end{bmatrix}
\begin{bmatrix}
2 & 1 \\
1 & 1
\end{bmatrix}
\begin{bmatrix}
-3 \\
4
\end{bmatrix} \\
&= \begin{bmatrix}
1 & 2
\end{bmatrix}
\begin{bmatrix}
-2 \\
1
\end{bmatrix} = 0
\end{align*}
Hence by Properties \ref{proper:orthoinner}, the two vectors are orthogonal with respect to the said inner product. Obviously they will not be orthogonal if the usual dot product is used instead.
\end{solution}

\subsection{Infinite-dimensional Inner Product Spaces}
\label{section:infinner}

We have been staying in the realm of finite-dimensional vector/inner product spaces until now but the utility of inner product spaces only becomes the most significant when they are infinite-dimensional. As suggested by Properties \ref{proper:samenvecsbases}, if there is a basis for an infinite-dimensional vector space\footnote{Actually, there always exists some basis for any infinite-dimensional vector space due to the \textit{Zorn's Lemma}, or equivalently the \textit{Hausdorff's Maximal Principle}.}, it must be infinite. Recall that from Chapter \ref{chap:6x}, a general vector space can be generated in terms of functions, and the same can be said for an inner product space. An example of infinite-dimensional inner product spaces will be $\mathcal{C}^0[a,b]$, the vector space of all continuous functions in one variable over the interval $a \leq x \leq b$, equipped with the frequently used inner product of
\begin{align}
\langle f,g \rangle = \int_a^b f(x) \overline{g(x)} dx \label{eqn:integralinner}
\end{align}
where $a < b$ are some real constants. Checking the validity of this inner product formulation is not hard.\footnote{We will only justify (4) in Definition \ref{defn:innerprod} and leave the remaining axioms to the readers. It is obvious that if $f(x) = 0$ is the zero function then $\langle f,f \rangle = \int_a^b (0)^2 dx = 0$. Assume that $f(x)$ is not everywhere zero, then by continuity from elementary calculus, we know that $f(c) \neq 0$ and hence $\abs{f(c)}^2 > 0$ at some point $a \leq c \leq b$ and there exists $\delta > 0$ such that $\abs{f(x)}^2 > \frac{\abs{f(c)}^2}{2}$ where $c-\delta \leq x \leq c+\delta$, therefore $\langle f,f \rangle \geq \frac{\abs{f(c)}^2}{2} (2\delta) = \abs{f(c)}^2 \delta > 0$.} 
Unfortunately, the above example fails to be a \index{Hilbert Space}\keywordhl{Hilbert space} which we often desire. The detailed explanation of how Hilbert spaces work belongs to the area of Functional Analysis and is very much out of the scope of this book\footnote{To put it shortly, a Hilbert space is a \textit{complete} inner product space where all \textit{Cauchy sequences} are convergent. The meaning of complete here is different from that in Properties \ref{proper:hilbertorthosys}.}, and we will take the liberty to assume that an infinite-dimensional inner product space we work on is a \index{Separable (Hilbert Space)}\keywordhl{separable} Hilbert space from time to time. For instance, the $L^2[a,b]$ space for all square-integrable functions\footnote{A square-integrable function $f$ means that $\int \abs{f}^2 dx < \infty$ is finite, and the $L^2[a,b]$ space is actually the "completed" (in the sense of the footnote above) version of $\mathcal{C}^0[a,b]$.} equipped with the same inner product in Equation (\ref{eqn:integralinner}) is such a Hilbert space. The reason why we care so much about Hilbert spaces, particularly those that are separable, is because they always admit a countable orthonormal basis that is \index{Complete}\textit{complete}.
\begin{proper}
\label{proper:hilbertorthosys}
An infinite-dimensional separable Hilbert space $\mathcal{H}$ always have a countably infinite orthonormal basis $\{\varphi_j\}_{j=1}^{\infty}$ where $\varphi_j$ denotes the $j$-th basis vector/function and $j$ is an integer enumerated from $1$ to infinity. It is complete in the sense that there exists no more other non-zero vector $\tilde{\varphi}$ that can be included in the basis such that $\langle \tilde{\varphi}, \varphi_j \rangle = 0$ for all $j$ without making the set linearly dependent.
\end{proper}
Equivalently, it means that any vector/function in the infinite-dimensional Hilbert space can be expanded into an infinite sum of orthonormal vectors $f = c_1\varphi_1 + c_2\varphi_2 + \cdots = \lim_{n \to \infty} \sum_{j=1}^{n} c_j \varphi_j$.\footnote{(Note: again this only holds if the Hilbert space is \textit{separable}, but we will not go into the details.) If there exists such a vector $\tilde{\varphi}$ that satisfies $\langle \tilde{\varphi}, \varphi_j \rangle = 0$ for all $j$, then consider $f = \tilde{\varphi}$ and take the inner product with $\tilde{\varphi}$ on both sides of $\tilde{\varphi} = c_1\varphi_1 + c_2\varphi_2 + \cdots$. All the terms on the R.H.S. will become zero but the L.H.S. is $\langle \tilde{\varphi}, \tilde{\varphi} \rangle > 0$ by the last axiom in Definition \ref{defn:innerprod}, a contradiction.} For these infinite-dimensional vector spaces, we often loosen the restriction so that any infinite sum of their basis vectors also makes up the span. This is known as a \index{Schuader Basis}\textit{Schuader basis}. \textcolor{red}{As noted before, the formal treatment of Hilbert spaces (or infinite-dimensional spaces in general) is out of our reach, and we will invoke the relevant properties as we see fit to avoid going down the rabbit hole.}\par
Back to infinite-dimensional inner product spaces in general, the properties and theorems given earlier in this section still hold for them as we have defined inner products in a way without regard to the (in)finiteness of dimensions.
\begin{exmp}
Verify that the Cauchy-Schwarz Inequality (Theorem \ref{thm:cauchyschinner}) holds for $\varphi_1 = x$ and $\varphi_2 = x^2$ in the $\mathcal{C}^0[0,1]$ space where the inner product is defined by Equation (\ref{eqn:integralinner}).
\end{exmp}
\begin{solution}
This is to check 
\begin{align*}
\abs{\langle\varphi_1 , \varphi_2\rangle} &\leq \norm{\varphi_1}\norm{\varphi_2} \\
\abs{\langle x , x^2\rangle} &\leq \norm{x}\norm{\smash{x^2}}
\end{align*}
by computing the three quantities in the above inequality:
\begin{align*}
\abs{\langle x , x^2\rangle} &= \abs{\int_0^1 x \overline{(x^2)} dx} \\
&= \abs{\int_0^1 x^3 dx} \\
&= \abs{\left[\frac{1}{4}x^4\right]_0^1} = \abs{\frac{1}{4}} = \frac{1}{4} \\
\norm{x} &= \sqrt{\int_0^1 x \overline{(x)} dx} \\
&= \sqrt{\int_0^1 x^2 dx} \\
&= \sqrt{\left[\frac{1}{3}x^3\right]_0^1} = \frac{1}{\sqrt{3}}
\end{align*}
similarly $\norm{x^2} = \frac{1}{\sqrt{5}}$. Hence $\abs{\langle x , x^2\rangle} = \frac{1}{4} = \frac{1}{\sqrt{16}} < \frac{1}{\sqrt{15}} = (\frac{1}{\sqrt{3}})(\frac{1}{\sqrt{5}}) = \norm{x}\norm{\smash{x^2}}$ and the inequality holds in this case.
\end{solution}
$\blacktriangleright$ Short Exercise: Show that the same\footnote{Actually, they are not the same functions as before since their domain changes from $[0,1]$ to $[-1,1]$, but we use the word for the sake of convenience.} two functions $\varphi_1 = x$ and $\varphi_2 = x^2$ become orthogonal to each other in $\mathcal{C}^0[-1,1]$ where the inner product still takes the same form of Equation (\ref{eqn:integralinner}) but integrated from $-1$ to $1$ instead.\footnotemark

\section{Adjoints and Hermitian/Unitary Operators}
\label{section:adjointherm}

\subsection{Definition of Adjoints}
\label{section:adjointdef}

With the usual [complex] dot product as the inner product for $\mathbb{R}^n$ [$\mathbb{C}^n$], we have $\langle A\vec{u}, \vec{v} \rangle = \langle \vec{u}, A^T\vec{v} \rangle$ [$\langle \vec{u}, A^*\vec{v} \rangle$] by Properties \ref{proper:dotproper} [\ref{proper:complexdotherm}]. Here the square matrix $A$ in the first argument can be moved to the second argument by applying a [conjugate] transpose on it. In this context, $A^T$ [$A^*$] is known as the \index{Adjoint}\keywordhl{adjoint} of $A$ with respect to the dot product. Since any square matrix essentially represents a linear operator behind the scenes (see Section \ref{section:lineartrans}), we can extend this idea for a general vector space $\mathcal{V}$ that has an inner product, where $\vec{u}, \vec{v} \in \mathcal{V}$ and the linear operator is $T: \mathcal{V} \to \mathcal{V}$ now. Subsequently, the adjoint of $T$ will then be another linear operator $T^\dag$ defined by the relationship $\langle T(\vec{u}), \vec{v} \rangle = \langle \vec{u}, T^\dag(\vec{v}) \rangle$. Again, if the standard inner product is employed, then we know that if the matrix representation of $T$ in the basis $\beta$ is $[T]_\beta$, then
\begin{align}
\langle T(\vec{u}), \vec{v} \rangle &\equiv \langle [T]_\beta[\vec{u}]_\beta, [\vec{v}]_\beta \rangle \nonumber \\
&= ([T]_\beta[\vec{u}]_\beta)^T\overline{[\vec{v}]_\beta} \nonumber \\
&= [\vec{u}]_\beta^T[T]_\beta^T\overline{[\vec{v}]_\beta} \nonumber \\
&= [\vec{u}]_\beta^T (\overline{[T]_\beta^*[\vec{v}]_\beta}) \nonumber \\
&= \langle [\vec{u}]_\beta, ([T]_\beta)^*[\vec{v}]_\beta \rangle \stackrel{\text{def}}{\equiv} \langle \vec{u}, T^\dag(\vec{v}) \rangle
\end{align}
by Properties \ref{proper:complexdotherm}, so that we identify the adjoint operator $T^\dag$ with the conjugate transpose matrix representation of $([T]_\beta)^*$ as expected.\footnotetext{$\int_{-1}^1 x \overline{(x^2)} = \int_{-1}^1 x^3 = 0$ as $x^3$ is an odd function and the integration limits are symmetric.}
\begin{defn}[Adjoint]
\label{defn:adjoint}
The adjoint of a linear operator $T: \mathcal{V} \to \mathcal{V}$ with respect to some inner product $\langle \cdot, \cdot \rangle$ is another linear operator denoted by $T^\dag$ such that the relation
\begin{align}
\langle T(\vec{u}), \vec{v} \rangle = \langle \vec{u}, T^\dag(\vec{v}) \rangle    
\end{align}
holds for all $\vec{u}, \vec{v} \in \mathcal{V}$. Such an adjoint $T^\dag$ is always unique for a given $T$. 
\end{defn}
It is easy to check the linearity and uniqueness of an adjoint.\footnote{Linearity (Definition \ref{defn:lintrans}): Consider $\langle \vec{u}, T^\dag(a\vec{v} + b\vec{w}) \rangle = \langle T(\vec{u}), a\vec{v} + b\vec{w} \rangle = \overline{a}\langle T(\vec{u}), \vec{v} \rangle + \overline{b}\langle T(\vec{u}), \vec{w} \rangle$ by Definitions \ref{defn:adjoint} and Properties \ref{proper:innerprod2}. Subsequently $\overline{a}\langle T(\vec{u}), \vec{v} \rangle + \overline{b}\langle T(\vec{u}), \vec{w} \rangle = \overline{a}\langle \vec{u}, T^\dag (\vec{v}) \rangle + \overline{b}\langle \vec{u}, T^\dag(\vec{w}) \rangle = \langle \vec{u}, aT^\dag(\vec{v}) \rangle + \langle \vec{u}, bT^\dag(\vec{w}) \rangle = \langle \vec{u}, aT^\dag(\vec{v}) + bT^\dag(\vec{w}) \rangle$ by those definitions again. Since this has to hold for any $\vec{u}$ (plus $\vec{v}$ and $\vec{w}$), $\langle \vec{u}, T^\dag(a\vec{v} + b\vec{w}) \rangle = \langle \vec{u}, aT^\dag(\vec{v}) + bT^\dag(\vec{w}) \rangle$ means that $T^\dag(a\vec{v} + b\vec{w}) = aT^\dag(\vec{v}) + bT^\dag(\vec{w})$ by Properties \ref{proper:innerprod2}. Uniqueness: Assume that there is another adjoint $S^\dag$ satisfies $\langle T(\vec{u}), \vec{v} \rangle = \langle \vec{u}, S^\dag(\vec{v}) \rangle$ for all $\vec{u}$ and $\vec{v}$, then by Properties \ref{proper:innerprod2} $T^\dag(\vec{v}) = S^\dag(\vec{v})$ for all $\vec{v}$. Hence $T^\dag$ and $S^\dag$ must be the same operator by Properties \ref{proper:sametrans}.} Now let's look at how adjoint is found when inner products other than the standard one are used. First, if $\mathcal{V}$ is a finite-dimensional complex vector space and a new inner product is defined via a positive-definite Hermitian form (see Section \ref{section:hermform}\footnote{The readers should check for themselves that the four inner space axioms hold in this case.}) $\langle \vec{u}, \vec{v} \rangle = \textbf{u}^T\overline{H\textbf{v}} = (H^*\vec{u})\cdot\vec{v}$ where $\cdot$ denotes the usual complex dot product, then for a (complex) linear operator $T$ with a matrix representation of $[T]_\beta$ in the $\beta$ system, by Definition \ref{defn:complexdotproduct} and Properties \ref{proper:complexdotherm}:
\begin{align*}
\langle T(\vec{u}), \vec{v} \rangle &\equiv (H^*[T]_\beta[\vec{u}]_\beta) \cdot ([\vec{v}]_\beta) \\
&= (H^*[T]_\beta (H^*)^{-1} H^* [\vec{u}]_\beta) \cdot ([\vec{v}]_\beta) \\
&= (H^*[\vec{u}]_\beta) \cdot ((H^*[T]_\beta (H^*)^{-1})^*[\vec{v}]_\beta) \\
&\equiv \langle \vec{u}, (H^*[T]_\beta (H^*)^{-1})^* \vec{v} \rangle = \langle \vec{u}, H^{-1} [T]_\beta^* H \vec{v} \rangle
\end{align*}
and thus we identify the matrix representation of the adjoint as 
\begin{align}
T^\dag \equiv H^{-1} [T]_\beta^* H \label{eqn:adjointformatrix}
\end{align} relative to such an inner product. For an infinite-dimensional inner product space, the inner product used is usually in the form of an integral like the one in Equation (\ref{eqn:integralinner}), additionally with a weighting function, and a linear operator $T$ can be more general, including differentiation and multiplication by some function, or a mix of them. As a result, the adjoint is often found via the technique of \index{Integration by Parts}\textit{integration by parts}, producing \index{Boundary Terms}\textit{boundary terms} as a by-product. Conventionally, we will use $\mathcal{L}$ in place of $T$ to denote the operator when the vectors are functions.
\begin{proper}
\label{proper:adjointinnerint}
For a general integral inner product:
\begin{align}
\langle f,g \rangle = \int_a^b w(x) f(x) \overline{g(x)} dx \label{eqn:integralinner2}
\end{align}
where $w(x) > 0$ is a positive-definite \textit{real} weighting function, the unique adjoint $\mathcal{L}^\dagger$ of a linear operator $\mathcal{L}$ is another one that satisfies
\begin{align}
\langle \mathcal{L}[f], g \rangle &= \int_a^b w(x) \mathcal{L}[f(x)] \overline{g(x)} dx \nonumber \\ 
&= \int_a^b w(x) f(x) \overline{(\mathcal{L^\dag}[g(x)])} dx + \text{boundary terms} \nonumber \\
&= \langle f, \mathcal{L}^\dag[g] \rangle + \text{boundary terms}
\end{align}
for all functions $f, g \in \mathcal{V}$ and the boundary terms are evaluated at the end-points $a$ and $b$.
\end{proper}
Equation (\ref{eqn:integralinner}) is then simply a special case of (\ref{eqn:integralinner2}) with a constant weight of $w = 1$.
\begin{exmp}
Find the adjoint of the linear operator $\mathcal{L}[f] = x\frac{d}{dx}[f]$ with respect to the inner product in Equation (\ref{eqn:integralinner}).
\end{exmp}
\begin{solution}
We start with $\langle \mathcal{L}[f], g \rangle = \smash{\int_a^b \left(x\frac{d}{dx}(f(x))\right)} \overline{g(x)} dx$ and aim to rewrite it into the form of $ \smash{\int_a^b f(x)\overline{(\mathcal{L^\dag}[g(x)])}} dx = \langle f, \mathcal{L}^\dag[g] \rangle$, plus possibly some boundary term(s). As suggested above we can try to apply integration by parts:
\begin{align*}
\langle \mathcal{L}[f], g \rangle &=  \int_a^b \left(x\frac{d}{dx}(f(x))\right) \overline{g(x)} dx \\  
&= \int_a^b \frac{d}{dx}(f(x)) x\overline{(g(x))} dx \\
&= [xf(x)\overline{g(x)}]_a^b - \int_a^b f(x)\frac{d}{dx}(x\overline{(g(x))}) dx \\
&= \int_a^b f(x)\overline{\left(-\frac{d}{dx}(x(g(x)))\right)} dx + [xf(x)\overline{(g(x))}]_a^b
\end{align*}
The boundary term is now put after the integral, from which we deduce that $\mathcal{L}^\dag[g] = -\frac{d}{dx}(x[g])$ by comparing it with $\int_a^b f(x) \overline{(\mathcal{L^\dag}[g(x)])} dx$.
\end{solution}
Finally, we note some properties of adjoints that can be compared to Properties \ref{proper:complexmat}.
\begin{proper}
\label{proper:adjoints}
For two linear operators $T$ and $U$ in the same inner product space $\mathcal{V}$ with adjoints $T^\dag$ and $U^\dag$ respectively, we have
\begin{enumerate}
\item $(cT)^\dag = \overline{c}T^\dag$, where $c$ is any complex scalar;
\item $(T^\dag)^\dag = T$ (the adjoint of adjoint is itself);
\item $(T \pm U)^\dag = T^\dag \pm U^\dag$;
\item $(T^\dag)^{-1} = (T^{-1})^\dag$ if $T$ (hence $T^\dag$) is invertible;
\item $(TU)^\dag = U^\dag T^\dag$.
\end{enumerate}
\end{proper}
We will briefly show the last item here. Using Definition \ref{defn:adjoint} twice, we have
\begin{align*}
\langle TU(\vec{u}), \vec{v} \rangle &= \langle U(\vec{u}), T^\dag(\vec{v}) \rangle \\
&= \langle \vec{u}, U^\dag T^\dag(\vec{v}) \rangle
\end{align*}
so we identify that $(TU)^\dag = U^\dag T^\dag$.

\subsection{Hermitian Operators}
As suggested in the last subsection, adjoints are the inner product counterpart of (conjugate) transposes. Therefore, it is natural to ask if the concept of symmetric or Hermitian in the matrix world is also applicable to an adjoint. Correspondingly, when a linear operator $T$ has an adjoint $T^\dag$ which is equal to itself, i.e.\ $T^\dag = T$, it is called \index{Self-adjoint}\keywordhl{self-adjoint}, as the inner product equivalent of Hermitian. If the inner product space is finite-dimensional with a positive-definite Hermitian form $H$ that induces its inner product, then according to what we have just derived, we simply need to check if $T^\dag \equiv H^{-1} [T]_\beta^* H = [T]_\beta \equiv T$. The problem becomes a bit more complicated when we consider the integral inner product in Equation (\ref{eqn:integralinner2}) because even when $\mathcal{L}^* = \mathcal{L}$ is self-adjoint, there can be boundary terms. We need an even stronger condition where there is no boundary term so that $\mathcal{L}$ becomes "nicer" to work with and gives desirable properties. In this situation, the self-adjoint operator $\mathcal{L}$ is further known as a \index{Hermitian (Operator)}\keywordhl{Hermitian} (or \textit{truly self-adjoint}, compared to \textit{formally self-adjoint}) operator.
\begin{defn}[Self-adjointness and Hermiticity for Linear Operators]
\label{defn:selfadjoint}
A linear operator $T$ is self-adjoint if its adjoint $T^\dag = T$ is equal to itself. A linear operator $\mathcal{L}$ is Hermitian if it is self-adjoint and all boundary terms vanish.
\end{defn}
The absence of any boundary term can be due to the structure of $\mathcal{L}$ or $\mathcal{L}^\dag$ itself, or the boundary condition(s) imposed on the input functions. 
\begin{exmp}
Show that $\mathcal{L} = -i\hbar\frac{d}{dx}$, where $\hbar$ is a real constant, is a self-adjoint operator with respect to the inner product in Equation (\ref{eqn:integralinner}) over the entire $x$-axis. Find the form of its eigenfunctions (treating functions as (eigen)vectors).
\end{exmp}
\begin{solution}
\begin{align*}
\langle \mathcal{L}[f], g \rangle &= \int_{-\infty}^\infty \mathcal{L}[f(x)] \overline{g(x)} dx \\
&= \int_{-\infty}^\infty \left(-i \hbar\frac{d}{dx}(f(x))\right) \overline{g(x)} dx \\
&= [-i\hbar f(x)\overline{g(x)}]_{-\infty}^\infty - \int_{-\infty}^\infty f(x) \left(-i\hbar\frac{d}{dx}(\overline{g(x)})\right) dx \\
&\quad \text{(Integration by parts)} \\
&= \int_{-\infty}^\infty f(x) \left(i\hbar\frac{d}{dx}(\overline{g(x)})\right) dx + [-i\hbar f(x)\overline{g(x)}]_{-\infty}^\infty \\
&= \int_{-\infty}^\infty f(x) \overline{\left(-i\hbar\frac{d}{dx}(g(x))\right)} dx + [-i\hbar f(x)\overline{g(x)}]_{-\infty}^\infty \\
&= \int_{-\infty}^\infty f(x) \overline{\mathcal{L}^\dag[g(x)]} dx + [-i\hbar f(x)\overline{g(x)}]_{-\infty}^\infty
\end{align*}
So we see that $\mathcal{L}^\dag = \mathcal{L} = -i\hbar\frac{d}{dx}$ is self-adjoint. The eigenfunctions $\varphi_m$ of $\mathcal{L}$ can be found from solving the ODE:
\begin{align*}
\mathcal{L}[\varphi_m] = -i \hbar\frac{d\varphi_m}{dx} &= m\varphi_m \\
-\int i \hbar \frac{d\varphi_m}{\varphi_m} &= \int m dx \\
- i \hbar \ln{\varphi_m} &= mx + C \\
\therefore \varphi_m &= Ke^{\frac{i}{\hbar}mx}
\end{align*}
where $K = e^{\frac{i}{\hbar} C}$ is any scaling constant. Notice that the eigenvalue $m$ can take any value ranging from $-\infty$ to $\infty$ as there is no boundary condition being enforced. In this case, the eigenvalues form a \textit{continuous spectrum} and we note that the inner product space generated by those eigenfunctions, which are uncountable in this problem, is not separable. In fact, this operator $\mathcal{L} = -i\hbar\frac{d}{dx}$, is famously referred to as the \textit{momentum operator} in Quantum Mechanics.
\end{solution}
The above example shows that eigenfunctions for a self-adjoint linear operator may be uncountable. In this case, a function cannot be represented by an infinite sum $f = c_1\varphi_1 + c_2\varphi_2 + \cdots = \sum_{j=1}^{\infty} c_j \varphi_j$ as suggested by Properties \ref{proper:hilbertorthosys} but rather requires to be expressed by an integral $f(x) = \int_0^\infty c_m\varphi_m(x) dm$ over a continuous index $m$, which is much more troublesome to deal with. This is the exact reason why we have introduced separable Hilbert spaces and Hermitian operators before and now, as they carry the desirable attributes of possessing countably infinite eigenfunctions that form a complete orthonormal basis and thus a \textit{discrete spectrum}. The completeness of the eigenfunctions of a Hermitian operator, again should be delegated to the world of Functional Analysis. But first, we can show an important property of Hermitian operators that their eigenvalues are always real, which can be compared to Properties \ref{proper:hermrealeig}.
\begin{proper}
\label{proper:hermrealeiginner}
The eigenvalues of any Hermitian operator $\mathcal{L}^\dag = \mathcal{L}$ must be real.
\end{proper}
\begin{proof}
Let $\varphi$ be an eigenfunction of $\mathcal{L}$ with an eigenvalue of $\lambda$. Then consider
\begin{align*}
\langle \mathcal{L}[\varphi], \varphi \rangle &= \langle \lambda \varphi, \varphi \rangle & \text{(Definition \ref{defn:eigen})} \\
&= \lambda \langle \varphi, \varphi \rangle & \text{(Definition \ref{defn:innerprod})} \\
&= \lambda \norm{\varphi}^2 & \text{(Properties \ref{proper:norminner})}
\end{align*}
but also
\begin{align*}
\langle \mathcal{L}[\varphi], \varphi \rangle &= \langle \varphi, \mathcal{L}^\dag[\varphi] \rangle + \cdots & \text{(Properties \ref{proper:adjointinnerint})} \\
&= \langle \varphi, \mathcal{L}[\varphi] \rangle & \text{($\mathcal{L}$ is Hermitian, Definition \ref{defn:selfadjoint})} \\
&= \langle \varphi, \lambda \varphi \rangle & \text{(Definition \ref{defn:eigen})} \\
&= \overline{\lambda} \langle \varphi, \varphi \rangle = \overline{\lambda} \norm{\varphi}^2 & \text{(Properties \ref{proper:innerprod2} and  \ref{proper:norminner})} 
\end{align*}
This implies that $\lambda \norm{\varphi}^2 = \overline{\lambda} \norm{\varphi}^2$. Since the eigenfunction $\varphi \neq 0$ cannot be zero, $\norm{\varphi}^2 \neq 0$ and thus it must be $\lambda = \overline{\lambda}$ so the eigenvalue will be real.
\end{proof}
Moreover, we can briefly prove the orthogonality between their eigenfunctions.
\begin{proper}
\label{proper:orthogonalherm}
Any two eigenfunctions of a Hermitian operator $\mathcal{L}^\dag = \mathcal{L}$ corresponding to two distinct eigenvalues are always orthogonal to each other.
\end{proper}
This closely parallels Properties \ref{proper:symortho} and the proof is also very similar.
\begin{proof}
Denote the two eigenfunctions as $\varphi_1$ and $\varphi_2$ where the respective eigenvalues are $m_1$ and $m_2$. Subsequently, we have
\begin{align*}
\langle \mathcal{L}[\varphi_1], \varphi_2 \rangle &= \langle m_1\varphi_1, \varphi_2 \rangle & \text{(Definition \ref{defn:eigen})} \\
&= m_1 \langle \varphi_1, \varphi_2 \rangle 
\end{align*}
but also
\begin{align*}
\langle \mathcal{L}[\varphi_1], \varphi_2 \rangle &= \langle \varphi_1, \mathcal{L}^\dag[\varphi_2] \rangle + \cdots & \text{(Properties \ref{proper:adjointinnerint})} \\
&= \langle \varphi_1, \mathcal{L}[\varphi_2] \rangle & \text{(Definition \ref{defn:selfadjoint})} \\
&= \langle \varphi_1, m_2\varphi_2 \rangle & \text{(Definition \ref{defn:eigen})} \\
&= \overline{m_2} \langle \varphi_1, \varphi_2 \rangle & \text{(Properties \ref{proper:innerprod2})} \\
&= m_2 \langle \varphi_1, \varphi_2 \rangle & \text{(Properties \ref{proper:hermrealeiginner})}
\end{align*}
So
\begin{align*}
m_1 \langle \varphi_1, \varphi_2 \rangle &= m_2 \langle \varphi_1, \varphi_2 \rangle \\
(m_1 - m_2) \langle \varphi_1, \varphi_2 \rangle &= 0
\end{align*}
but the two eigenvalues are taken to be distinct, $m_1 \neq m_2$, hence it must be that $\langle \varphi_1, \varphi_2 \rangle$ = 0 and $\varphi_1, \varphi_2$ are orthogonal with respect to the inner product (Properties \ref{proper:orthoinner}).
\end{proof}
We can then divide the eigenfunctions by their norm as in Definition \ref{defn:unitvecinner} to make them have unit length and hence become orthonormal. Even when some of the $k$ eigenfunctions have the same eigenvalue (in this context they are said to be \textit{$k$-fold degenerate}), we can apply the Gram-Schmidt Orthogonalization process adapted for inner product space (to be discussed soon in Section \ref{section:GSorthinnersec}), over those eigenfunctions, in a fashion very similar to the idea suggested in the discussion below Properties \ref{proper:symortho}. Now let's see a very standard example of a Hermitian operator leading to a complete set of orthonormal eigenfunctions.
\begin{exmp}
\label{exmp:Fourierbasis}
For the separable Hilbert space $L^2[-\pi, \pi]$ of square-integrable functions along $-\pi \leq x \leq \pi$, show that the linear operator $\mathcal{L}[f] = \frac{d^2}{dx^2}[f]$ will be Hermitian with respect to the inner product in Equation (\ref{eqn:integralinner}) if the (eigen)functions are picked in a way so that the boundary terms vanish.
\end{exmp}
\begin{solution}
First, we have to check if $\mathcal{L}$ is self-adjoint, from
\begin{align*}
\langle \mathcal{L}[f], g \rangle &= \int_{-\pi}^\pi \mathcal{L}[f(x)] \overline{g(x)} dx \\
&= \int_{-\pi}^\pi \frac{d^2}{dx^2}(f(x)) \overline{g(x)} dx \\
&= [\frac{d}{dx}(f(x)) \overline{(g(x))}]_{-\pi}^\pi - \int_{-\pi}^\pi \frac{d}{dx}(f(x)) \frac{d}{dx}\overline{(g(x))} dx \\
&= [\frac{d}{dx}(f(x)) \overline{(g(x))}]_{-\pi}^\pi - [(f(x)) \frac{d}{dx}\overline{(g(x))}]_{-\pi}^\pi \\
&\quad + \int_{-\pi}^\pi f(x) \frac{d^2}{dx^2}\overline{(g(x))} dx \\
&= [\frac{d}{dx}(f(x)) \overline{(g(x))}]_{-\pi}^\pi - [(f(x)) \frac{d}{dx}\overline{(g(x))}]_{-\pi}^\pi \\
&\quad + \int_{-\pi}^\pi f(x) \overline{\left(\frac{d^2}{dx^2}(g(x))\right)} dx \\
&= [\frac{d}{dx}(f(x)) \overline{(g(x))}]_{-\pi}^\pi - [(f(x)) \frac{d}{dx}\overline{(g(x))}]_{-\pi}^\pi \\
&\quad + \int_{-\pi}^\pi f(x) \overline{\mathcal{L}[g(x)]} dx
\end{align*}
we thus see $\mathcal{L}^\dag = \smash{\frac{d^2}{dx^2}} = \mathcal{L}$. Again from basic ODE, we know that the eigenfunctions $\varphi$ of $\mathcal{L}[f] = \smash{\frac{d^2}{dx^2}[f]}$ will be in the form of $\sin(mx)$ and $\cos(mx)$ as $\smash{\frac{d^2}{dx^2}}(\sin(mx)) = -m^2\sin(mx)$ and $\smash{\frac{d^2}{dx^2}}(\cos(mx)) = -m^2\cos(mx)$. For the two boundary terms to vanish, $m = 0, 1, 2, \ldots$ must be a non-negative integer\footnote{Without loss of generality, let $f = \sin(m_1 x)$ and $g = \cos(m_2 x)$, then
\begin{align*}
&\quad [\frac{d}{dx}(f(x)) \overline{(g(x))}]_{-\pi}^\pi - [(f(x)) \frac{d}{dx}\overline{([g(x)])}]_{-\pi}^\pi \\
&= [m_1\cos(m_1 x)\cos(m_2 x)]_{-\pi}^\pi - [-m_2\sin(m_1 x)\sin(m_2 x)]_{-\pi}^\pi \\
&= [\frac{m_1}{2}(\cos((m_1-m_2)x) + \cos((m_1+m_2)x))]_{-\pi}^\pi \\
&\quad - [-\frac{m_2}{2}(\cos((m_1-m_2)x) - \cos((m_1+m_2)x))]_{-\pi}^\pi 
\end{align*}
using trigonometric identities. From this, we see that if the two boundary terms have to be zero, $m_1 - m_2$ and $m_1 + m_2$ must both be odd or even at the same time, which implies that $m_1$ and $m_2$ have to be integers. And since $\sin(-m_1 x) = -\sin(m_1 x)$ and $\cos(-m_2 x) = \cos(m_2 x)$, we only need to take the integers that are positive.}, with the eigenvalues being $-m^2$. Hence the countably infinite, orthogonal\footnotemark (Properties \ref{proper:hilbertorthosys}, not yet normalized) basis of eigenfunctions for $\mathcal{L}$ are $\{\sin(x), \sin(2x), \sin(3x), \ldots, 1, \cos(x), \cos(2x), \cos(3x), \ldots\}$. This basis, consisting of sines and cosines with discrete, equally spaced frequencies, is famously known as the \index{Fourier Basis}\keywordhl{Fourier basis}.
\end{solution}

\subsection{Unitary Operators}

Another class of matrices we want to generalize for inner product spaces is the orthogonal/unitary one introduced in Chapter \ref{chap:normalmat}. For those matrices, let's say $A$, two defining properties are that $A^{-1} = A^*$ and the preservation of distance noted in Properties \ref{proper:ortholengthpreserve}. Therefore, we want a unitary operator $T$ to satisfy the same properties where the conjugate transpose is now replaced by its adjoint $T^\dag$ and the notion of distance is relative to the given inner product.
\begin{defn}[Unitary Operator]
\label{defn:unitaryop}
A unitary operator $T$ is a linear operator such that its inverse operator is equal to its adjoint $T^{-1} = T^\dag$ with respect to the inner product used.
\end{defn}
With this definition, the distance-preserving property can be readily derived.
\begin{proper}
Transformation by a unitary operator $T = T^\dag$ on a vector (or function) is length-preserving.
\end{proper}
which should be compared to Properties \ref{proper:ortholengthpreserve}.\footnotetext{Again, without the loss of generality, we let $f = \sin(m_1 x)$ and $g = \cos(m_2 x)$ where $m_1$ and $m_2$ are now positive integers. Then the orthogonality is verified by computing
\begin{align*}
\int_{-\pi}^{\pi} \sin(m_1 x)\cos(m_2 x) dx &= \int_{-\pi}^{\pi} \frac{1}{2} (\sin((m_1 - m_2)x) + \sin((m_1 + m_2)x)) dx \\
&= [\frac{1}{2} (-\frac{\cos((m_1 - m_2)x)}{m_1 - m_2} - \frac{\cos((m_1 + m_2)x)}{m_1 + m_2})]_{-\pi}^{\pi} 
\end{align*}
which yields $0$ when the end-points $-\pi, \pi$ are substituted into the expression.}
\begin{proof}
Denote the original vector as $\vec{v}$ and the newly transformed vector be $T(\vec{v})$, then its length, given as the norm in Properties \ref{proper:norminner},
\begin{align*}
\norm{T(\vec{v})}^2 = \langle T(\vec{v}), T(\vec{v}) \rangle &= \langle \vec{v}, T^\dag(T(\vec{v})) \rangle & \text{(Definition \ref{defn:adjoint})} \\
&= \langle \vec{v}, T^{-1}(T(\vec{v})) \rangle & \text{(Definition \ref{defn:unitaryop})} \\
&= \langle \vec{v}, \text{id}(\vec{v}) \rangle & \text{(Definition \ref{defn:inversetrans})} \\
&= \langle \vec{v}, \vec{v} \rangle = \norm{\vec{v}}^2
\end{align*}
is shown to be equal throughout the unitary transformation.
\end{proof}
\begin{exmp}
Show that for the $L^2[a,b]$ space with the inner product of Equation (\ref{eqn:integralinner}), the linear operator $\mathcal{L}[f] = e^{ikx}[f]$ where $k$ is any real number, is unitary.
\end{exmp}
\begin{solution}
It is apparent that the inverse of $\mathcal{L}$ is $\mathcal{L}^{-1}[f] = e^{-ikx}[f]$ so that $(\mathcal{L}^{-1} \circ \mathcal{L})[f] = e^{-ikx}(e^{ikx}[f]) = (1)f = \text{id}[f]$, and we have to show that it equals to the adjoint $\mathcal{L}^\dag$:
\begin{align*}
\langle \mathcal{L}[f], g \rangle &= \int_a^b \mathcal{L}[f(x)] \overline{g(x)} dx \\
&= \int_a^b (e^{ikx}f(x)) \overline{g(x)} dx \\
&= \int_a^b f(x) \overline{e^{-ikx}g(x)} dx & (\overline{e^{ikx}} = e^{-ikx}) \\
&= \int_a^b f(x) \overline{\mathcal{L}^{-1}[g(x)]} dx
\end{align*}
Comparing with $\langle f, \mathcal{L}^\dag[g] \rangle$, from this we readily infer that $\mathcal{L}^\dag = \mathcal{L}^{-1}$ and thus $\mathcal{L}$ is unitary.
\end{solution}

\section{Revisiting Orthogonal Projections}
\label{section:orthoproj}

\subsection{Orthogonal Projections for an Inner Product Space}

Since we have defined orthogonality and adjoints, we are now ready to derive the form of orthogonal projections for any inner product space, ultimately allowing us to establish the inner product space version of the Spectral Theorem. Remember in Section \ref{section:proj}, we derived the orthogonal projection of a vector onto another vector with the dot product, and in the same essence, we can obtain the expression of any one-dimensional orthogonal projection with respect to an inner product by simply replacing the dot product with that inner product.
\begin{defn}
\label{defn:orthoprojinner}
The orthogonal projection of a vector $\vec{v}$ onto $\vec{u}$ with respect to an inner product $\langle \cdot, \cdot \rangle$ is
\begin{align}
\overrightarrow{\text{proj}}_u v = \frac{\langle \vec{v}, \vec{u}\rangle}{\norm{\vec{u}}^2} \vec{u}
\end{align}
\end{defn}
This definition is consistent in the sense that $\vec{u}$ and the component of $\vec{v}$ normal to $\vec{u}$ are orthogonal:
\begin{align*}
\langle \vec{u}, \vec{v} - \overrightarrow{\text{proj}}_u v \rangle &= \langle \vec{u}, \vec{v} - \frac{\langle \vec{v}, \vec{u}\rangle}{\norm{\vec{u}}^2} \vec{u} \rangle \\
&= \langle \vec{u}, \vec{v} \rangle - \langle \vec{u}, \frac{\langle \vec{v}, \vec{u}\rangle}{\norm{\vec{u}}^2} \vec{u} \rangle \\
&= \langle \vec{u}, \vec{v} \rangle - \overline{\frac{\langle \vec{v}, \vec{u}\rangle}{\norm{\vec{u}}^2}} \langle \vec{u}, \vec{u} \rangle & \text{(Properties \ref{proper:innerprod2})}\\
&= \langle \vec{u}, \vec{v} \rangle - \frac{\langle \vec{u}, \vec{v}\rangle}{\norm{\vec{u}^2}} {\norm{\vec{u}}^2} \\
&= \langle \vec{u}, \vec{v} \rangle -  \langle \vec{u}, \vec{v} \rangle = 0
\end{align*}
Similar to Expression (\ref{eqn:projrank1sum}), the orthogonal projection operator $T$ onto a subspace $\mathcal{W} \subseteq \mathcal{V}$ of an inner product space with an orthonormal basis $\{\vec{w}^{(1)}, \vec{w}^{(2)}, \ldots\}$ that may be finite or countably infinite, is the sum of the one-dimensional projectors onto each of the basis vectors according to the definition above:
\begin{align}
T(\vec{v}) = \frac{\langle \vec{v}, \vec{w}^{(1)}\rangle}{\norm{\vec{w}^{(1)}}^2} \vec{w}^{(1)} + \frac{\langle \vec{v}, \vec{w}^{(2)}\rangle}{\norm{\vec{w}^{(2)}}^2} \vec{w}^{(2)} + \cdots \label{eqn:orthoprojinner}
\end{align}
Just as Properties \ref{proper:matrixproj} and \ref{proper:orthogonalproj}, a linear operator $T$ represents an orthogonal projection if and only if it has an adjoint $T^\dag$ so that $T^2 = T = T^\dag$.
\begin{proper}
\label{proper:orthoprojadjoint}
A linear operator $T: \mathcal{V} \to \mathcal{V}$ over an inner product space, is an orthogonal projection with respect to the inner product if and only if $T^2 = T = T^\dag$ where $T^\dag$ is its adjoint.
\end{proper}
The proof for the first equality $T^2 = T$ is the same one in Properties \ref{proper:matrixproj} except that the $^\dag$ adjoint superscript now replaces that of conjugate transpose $^*$, and $\mathcal{V}$, as well as $\mathcal{W}_1$ and $\mathcal{W}_2$ may be infinite-dimensional now. Meanwhile for the second equality $T = T^\dag$, we need to replace the dot product in the original proof for Properties \ref{proper:orthogonalproj} by the inner product, and additionally show that $\mathcal{N}(T)^\perp = \mathcal{R}(T)$ for the "if" part when $\mathcal{V}$ is infinite-dimensional, which is not too far from our reach.\footnote{We take it for granted that $\mathcal{W} \subseteq \mathcal{W}^{\perp\perp}$ for any subspace $\mathcal{W}$, so that $\mathcal{N}(T)^\perp = \mathcal{R}(T)^{\perp\perp} \supseteq \mathcal{R}(T)$, and the remaining task is to show $\mathcal{N}(T)^\perp \subseteq \mathcal{R}(T)$ so that $\mathcal{N}(T)^\perp = \mathcal{R}(T)$, i.e.\ for any $\vec{w} \in \mathcal{N}(T)^\perp$, $\vec{w} \in \mathcal{R}(T)$. Consider $\norm{\smash{\vec{w} - T(\vec{W})}}^2 = \langle \vec{w} - T(\vec{w}), \vec{w} - T(\vec{w}) \rangle = \langle \vec{w}, \vec{w} - T(\vec{w}) \rangle - \langle T(\vec{w}), \vec{w} - T(\vec{w}) \rangle$. The first term is zero since $\vec{w} - T(\vec{w}) \in \mathcal{N}(T)$ is orthogonal to $\vec{w} \in \mathcal{N}(T)^\perp$, and the second term is also zero as $\langle T(\vec{w}), \vec{w} - T(\vec{w}) \rangle = \langle \vec{w}, T^\dag(\vec{w} - T(\vec{w})) \rangle = \langle \vec{w}, T(\vec{w} - T(\vec{w})) \rangle = \langle \vec{w}, T(\vec{w}) - T^2(\vec{w}) \rangle = \langle \vec{w}, \textbf{0} \rangle = 0$ since $T^\dag = T$ by the "if" condition and $T^2 = T$ from the first part. Thus $\norm{\vec{w} - T(\vec{w})} = 0$ and this implies that $T(\vec{w}) = \vec{w}$ so $\vec{w} \in \mathcal{R}(T)$.} We now show that the expression of the orthogonal projector $T$ given in Equation (\ref{eqn:orthoprojinner}) satisfies the above requirement $T^2 = T = T^\dag$: Using (\ref{eqn:orthoprojinner}) twice, we have
\begin{align*}
T^2(\vec{v}) = T(T(\vec{v})) &= \frac{\langle T(\vec{v}), \vec{w}^{(1)}\rangle}{\norm{\vec{w}^{(1)}}^2} \vec{w}^{(1)} + \frac{\langle T(\vec{v}), \vec{w}^{(2)}\rangle}{\norm{\vec{w}^{(2)}}^2} \vec{w}^{(2)} + \cdots \\
&= \frac{\left\langle \frac{\langle \vec{v}, \vec{w}^{(1)}\rangle}{\norm{\vec{w}^{(1)}}^2} \vec{w}^{(1)} + \frac{\langle \vec{v}, \vec{w}^{(2)}\rangle}{\norm{\vec{w}^{(2)}}^2} \vec{w}^{(2)} + \cdots, \vec{w}^{(1)}\right\rangle}{\norm{\vec{w}^{(1)}}^2} \vec{w}^{(1)} \\
&\quad+ \frac{\left\langle \frac{\langle \vec{v}, \vec{w}^{(1)}\rangle}{\norm{\vec{w}^{(1)}}^2} \vec{w}^{(1)} + \frac{\langle \vec{v}, \vec{w}^{(2)}\rangle}{\norm{\vec{w}^{(2)}}^2} \vec{w}^{(2)} + \cdots, \vec{w}^{(2)}\right\rangle}{\norm{\vec{w}^{(2)}}^2} \vec{w}^{(2)} + \cdots \\
&= \frac{\frac{\langle \vec{v}, \vec{w}^{(1)}\rangle}{\norm{\vec{w}^{(1)}}^2} \norm{\vec{w}^{(1)}}^2 + (0)}{\norm{\vec{w}^{(1)}}^2} \vec{w}^{(1)} \\
&\quad+ \frac{(0) + \frac{\langle \vec{v}, \vec{w}^{(2)}\rangle}{\norm{\vec{w}^{(2)}}^2} \norm{\vec{w}^{(2)}}^2 + (0)}{\norm{\vec{w}^{(2)}}^2} \vec{w}^{(2)} + \cdots \\
&\quad \text{($\vec{w}^{(j)}$ are orthogonal)} \\
&= \frac{\langle \vec{v}, \vec{w}^{(1)}\rangle}{\norm{\vec{w}^{(1)}}^2} \vec{w}^{(1)} + \frac{\langle \vec{v}, \vec{w}^{(2)}\rangle}{\norm{\vec{w}^{(2)}}^2} \vec{w}^{(2)} + \cdots = T(\vec{v})
\end{align*}
and
\begin{align*}
\langle \vec{u}, T(\vec{v}) \rangle &= \langle \vec{u}, \frac{\langle \vec{v}, \vec{w}^{(1)}\rangle}{\norm{\vec{w}^{(1)}}^2} \vec{w}^{(1)} + \frac{\langle \vec{v}, \vec{w}^{(2)}\rangle}{\norm{\vec{w}^{(2)}}^2} \vec{w}^{(2)} + \cdots \rangle \\
&= \frac{\overline{\langle \vec{v}, \vec{w}^{(1)}\rangle}}{\norm{\vec{w}^{(1)}}^2} \langle \vec{u}, \vec{w}^{(1)} \rangle + \frac{\overline{\langle \vec{v}, \vec{w}^{(2)}\rangle}}{\norm{\vec{w}^{(2)}}^2} \langle \vec{u}, \vec{w}^{(2)} \rangle + \cdots \\
&= \frac{\langle \vec{w}^{(1)}, \vec{v} \rangle}{\norm{\vec{w}^{(1)}}^2} \langle \vec{u}, \vec{w}^{(1)} \rangle + \frac{\langle \vec{w}^{(2)}, \vec{v}\rangle}{\norm{\vec{w}^{(2)}}^2} \langle \vec{u}, \vec{w}^{(2)} \rangle + \cdots 
\end{align*}
but
\begin{align*}
\langle T(\vec{u}), \vec{v} \rangle &= \langle \frac{\langle \vec{u}, \vec{w}^{(1)}\rangle}{\norm{\vec{w}^{(1)}}^2} \vec{w}^{(1)} + \frac{\langle \vec{u}, \vec{w}^{(2)}\rangle}{\norm{\vec{w}^{(2)}}^2} \vec{w}^{(2)} + \cdots, \vec{v} \rangle \\
&= \frac{\langle \vec{u}, \vec{w}^{(1)}\rangle}{\norm{\vec{w}^{(1)}}^2} \langle \vec{w}^{(1)}, \vec{v} \rangle + \frac{\langle \vec{u}, \vec{w}^{(2)}\rangle}{\norm{\vec{w}^{(2)}}^2} \langle \vec{w}^{(2)}, \vec{v} \rangle + \cdots \\
&= \langle \vec{u}, T(\vec{v}) \rangle
\end{align*}
Hence we identify $T^\dag$ with $T$.

\subsection{Revisiting Gram-Schmidt Orthogonalization}
\label{section:GSorthinnersec}

After obtaining the formula for orthogonal projections with respect to an inner product space, the next step is to extend the method of Gram-Schmidt Orthogonalization, so that a countably infinite orthonormal basis can be produced for an infinite-dimensional separable Hilbert space as predicted by Properties \ref{proper:hilbertorthosys}, given that the procedure keeps going on indefinitely. Attentive readers should already be able to conceive that we simply have to replace all the dot products in Definition \ref{defn:GSorth} with an appropriate inner product. 
\begin{defn}
\label{defn:GSorthinner}
Given a countably infinite basis $\{\vec{u}^{(1)}, \vec{u}^{(2)}, \vec{u}^{(3)}, \ldots\}$, where $\vec{u}^{(j)} \in \mathcal{V}$ belongs to an inner product space, it is transformed into an orthogonal basis $\{\vec{v}^{(1)}, \vec{v}^{(2)}, \vec{v}^{(3)}, \ldots\}$, $\vec{v}^{(j)} \in \mathcal{V}$, by Gram-Schmidt Orthogonalization according to the following formulae:
\begin{align}
\vec{v}^{(1)} &= \vec{u}^{(1)} \nonumber\\
\vec{v}^{(2)} &= \vec{u}^{(2)} - \text{proj}_{v^{(1)}}\vec{u}^{(2)} = \vec{u}^{(2)} - \frac{\langle \vec{u}^{(2)}, \vec{v}^{(1)}\rangle}{\norm{\vec{v}^{(1)}}^2} \vec{v}^{(1)} \nonumber \\
\vec{v}^{(3)} &= \vec{u}^{(3)} - \text{proj}_{v^{(1)}}\vec{u}^{(3)} - \text{proj}_{v^{(2)}}\vec{u}^{(3)} \nonumber\\
&= \vec{u}^{(3)} - \frac{\langle\vec{u}^{(3)}, \vec{v}^{(1)}\rangle}{\norm{\vec{v}^{(1)}}^2} \vec{v}^{(1)} - \frac{\langle\vec{u}^{(3)}, \vec{v}^{(2)}\rangle}{\norm{\vec{v}^{(2)}}^2} \vec{v}^{(2)} \\
\vdots \nonumber \\
\vec{v}^{(n)} &= \vec{u}^{(n)} - \text{proj}_{v^{(1)}}\vec{u}^{(n)} - \text{proj}_{v^{(2)}}\vec{u}^{(n)} - \cdots - \text{proj}_{v^{(n-1)}}\vec{u}^{(n)} \nonumber \\
&= \vec{u}^{(n)} - \frac{\langle\vec{u}^{(n)}, \vec{v}^{(1)}\rangle}{\norm{\vec{v}^{(1)}}^2} \vec{v}^{(1)} - \frac{\langle\vec{u}^{(n)}, \vec{v}^{(2)}\rangle}{\norm{\vec{v}^{(2)}}^2} \vec{v}^{(2)} - \cdots - \frac{\langle\vec{u}^{(n)}, \vec{v}^{(n-1)}\rangle}{\norm{\vec{v}^{(n-1)}}^2} \vec{v}^{(n-1)} \nonumber
\end{align}
Generally, for $j \geq 2$, the $j$-th new orthogonal basis vector is computed by
\begin{align}
\vec{v}^{(j)} &= \vec{u}^{(j)} - \sum_{k=1}^{j-1}\text{proj}_{v^{(k)}}\vec{u}^{(j)}  = \vec{u}^{(j)} - \sum_{k=1}^{j-1}\frac{\langle \vec{u}^{(j)}, \vec{v}^{(k)} \rangle}{\norm{\vec{v}^{(k)}}^2} \vec{v}^{(k)}
\end{align}
where the expression of a vector projection in an inner product space now follows Definition \ref{defn:orthoprojinner} and $j$ can be arbitrarily large. To make it an orthonormal basis we simply normalize each $\vec{v}^{(j)}$ by its norm as suggested by Definition \ref{defn:unitvecinner}.  
\end{defn}
However, for now, we will go through the example of a general, finite-dimensional inner product space first, in which the procedure truncates at the $n$-th step where $n$ is the dimension, and reserve the infinite-dimensional case until the next section.
\begin{exmp}
\label{exmp:R3innerGS}
For the $\mathbb{R}^3$ space with an inner product defined according to the symmetric bilinear form as
\begin{align*}
\langle \vec{u}, \vec{v} \rangle = \textbf{u}^TB\textbf{v}
\end{align*}
where 
\begin{align*}
B =
\begin{bmatrix}
2&1&0\\ 
1&2&1\\
0&1&2
\end{bmatrix}
\end{align*}
apply Gram-Schmidt Orthogonalization over the standard basis for $\mathbb{R}^3$, $\hat{e}^{(1)} = (1,0,0)^T, \hat{e}^{(2)} = (0,1,0)^T, \hat{e}^{(3)} = (0,0,1)^T$, to transform it into an orthonormal basis with respect to this inner product.
\end{exmp}
\begin{solution}
We leave it to the readers to check that the eigenvalues of $B$ are $\lambda = 2-\sqrt{2},2,2+\sqrt{2}$ all positive so that $B$ is positive-definite by Theorem \ref{thm:quaddefinite} and the inner product makes sense. We calculate each of the expressions appearing in Definition \ref{defn:GSorthinner}:
\begin{align*}
\norm{\vec{v}^{(1)}}^2 &= 
\begin{bmatrix}
1 & 0 & 0
\end{bmatrix}
\begin{bmatrix}
2&1&0\\ 
1&2&1\\
0&1&2
\end{bmatrix}
\begin{bmatrix}
1 \\
0 \\
0
\end{bmatrix} = 2 \\
\vec{v}^{(2)} &= \vec{u}^{(2)} - \frac{\langle \vec{u}^{(2)}, \vec{v}^{(1)}\rangle}{\norm{\vec{v}^{(1)}}^2} \vec{v}^{(1)} \\
&= \begin{bmatrix}
0 \\
1 \\
0
\end{bmatrix}
-
\frac{\left(\begin{bmatrix}
0 & 1 & 0
\end{bmatrix}
\begin{bmatrix}
2&1&0\\ 
1&2&1\\
0&1&2
\end{bmatrix}
\begin{bmatrix}
1 \\
0 \\
0
\end{bmatrix}\right)}{2}
\begin{bmatrix}
1 \\
0 \\
0
\end{bmatrix} \\
&= \begin{bmatrix}
0 \\
1 \\
0
\end{bmatrix}
-\frac{1}{2}
\begin{bmatrix}
1 \\
0 \\
0
\end{bmatrix} =
\begin{bmatrix}
-\frac{1}{2} \\
1 \\
0
\end{bmatrix} \\
\norm{\vec{v}^{(2)}}^2 &= 
\begin{bmatrix}
-\frac{1}{2} & 1 & 0
\end{bmatrix}
\begin{bmatrix}
2&1&0\\ 
1&2&1\\
0&1&2
\end{bmatrix}
\begin{bmatrix}
-\frac{1}{2} \\
1 \\
0
\end{bmatrix} = \frac{3}{2} \\
\vec{v}^{(3)} &= \vec{u}^{(3)} - \frac{\langle\vec{u}^{(3)}, \vec{v}^{(1)}\rangle}{\norm{\vec{v}^{(1)}}^2} \vec{v}^{(1)} - \frac{\langle\vec{u}^{(3)}, \vec{v}^{(2)}\rangle}{\norm{\vec{v}^{(2)}}^2} \vec{v}^{(2)} \\
&= 
\begin{bmatrix}
0\\
0\\
1
\end{bmatrix}
-
\frac{\left(\begin{bmatrix}
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
2&1&0\\ 
1&2&1\\
0&1&2
\end{bmatrix}
\begin{bmatrix}
1 \\
0 \\
0
\end{bmatrix}\right)}{2}
\begin{bmatrix}
1 \\
0 \\
0
\end{bmatrix} \\
&\quad -
\frac{\left(\begin{bmatrix}
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
2&1&0\\ 
1&2&1\\
0&1&2
\end{bmatrix}
\begin{bmatrix}
-\frac{1}{2} \\
1 \\
0
\end{bmatrix}\right)}{\frac{3}{2}}
\begin{bmatrix}
-\frac{1}{2} \\
1 \\
0
\end{bmatrix} \\
&=
\begin{bmatrix}
0 \\
0 \\
1
\end{bmatrix}
- (0)
\begin{bmatrix}
1 \\
0 \\
0
\end{bmatrix}
- \frac{2}{3}
\begin{bmatrix}
-\frac{1}{2} \\
1 \\
0
\end{bmatrix}
=
\begin{bmatrix}
\frac{1}{3}\\
-\frac{2}{3}\\
1
\end{bmatrix} \\
\norm{\vec{v}^{(3)}}^2 &= 
\begin{bmatrix}
\frac{1}{3} & -\frac{2}{3} & 1
\end{bmatrix}
\begin{bmatrix}
2&1&0\\ 
1&2&1\\
0&1&2
\end{bmatrix}
\begin{bmatrix}
\frac{1}{3}\\
-\frac{2}{3}\\
1
\end{bmatrix} = \frac{4}{3}
\end{align*}
Therefore, the required orthonormal basis is 
\begin{align*}
\left\{\left(\frac{1}{\sqrt{2}},0,0\right)^T, \left(-\frac{1}{\sqrt{6}}, \frac{2}{\sqrt{6}}, 0\right)^T, \left(\frac{1}{2\sqrt{3}}, -\frac{1}{\sqrt{3}}, \frac{3}{2\sqrt{3}}\right)^T\right\}    
\end{align*} found by dividing each of the $\vec{v}^{(j)}$ by $\norm{\vec{v}^{(j)}}$. This example shows that the standard unit vectors are usually not orthogonal to each other when an inner product other than the standard one is used. 
\end{solution}

\subsection{Spectral Theorem for Hermitian Operators}
\label{subsection:spectralthmherm}

As in the symmetric matrix case, the Spectral Theorem is the most principal result that can be derived for Hermitian operators, which are the inner product counterparts of symmetric matrices. Its statements are therefore analogous to Theorem \ref{thm:spectral} and are listed below.
\begin{thm}[Spectral Theorem for Hermitian Operators]
\label{thm:spectralinner}
For a Hermitian linear operator $T: \mathcal{V} \to \mathcal{V}$ where $T^\dag = T$ and the inner product space $\mathcal{V}$ is a separable Hilbert space, denote its eigenvalues by $\lambda_j$, $j = 1,2,\ldots$, and $\varphi^{(j)}$ be the corresponding eigenvectors that are orthonormal (by Properties \ref{proper:orthogonalherm}, or made from the Gram-Schmidt process if necessary). The dimension of this Hilbert space, $n$, and hence $j$, may be finite (if so, remove the limit $\lim_{n \to \infty}$) or countably infinite. Refer the one-dimensional eigenspaces generated by each of the $\varphi^{(j)}$ to as $\mathcal{E}_{j}$ and denote the orthogonal projection onto $\mathcal{E}_{j}$ by $T_{j}(\vec{v}) = \langle \vec{v} , \varphi^{(j)} \rangle$, then we have:
\begin{enumerate}[label=(\alph*)]
\item $\mathcal{V} = \mathcal{E}_{1} \oplus \mathcal{E}_{2} \oplus \cdots = \lim_{n \to \infty} \bigoplus_{j=1}^{n} \mathcal{E}_{j}$;
\item $(\bigoplus_{j \in \{J\}} \mathcal{E}_{j})^\perp = \bigoplus_{j \notin \{J\}} \mathcal{E}_{j}$ where $J$ is a countable index set;
\item $T_{j} T_{j'} = 
\begin{cases}
T_{j} & \text{if $j = j'$} \\
0 & \text{if $j \neq j'$}
\end{cases}$;
\item $I = T_{1} + T_{2} + \cdots = \lim_{n \to \infty} \sum_{j=1}^{n} T_{j}$; and
\item $T = \lambda_{1}T_{1} + \lambda_{2}T_{2} + \cdots = \lim_{n \to \infty} \sum_{j=1}^{n} \lambda_{j}T_{j}$, where the eigenvalues $\lambda_j$ are all real due to Properties \ref{proper:hermrealeiginner}.
\end{enumerate}
\end{thm}
Note that (a) and subsequently (d) are simply a restatement of the fact that a Hermitian operator has a complete orthonormal basis formed by countably infinite eigenvectors/eigenfunctions, which we have taken for granted to avoid getting involved with Functional Analysis. (a) essentially means that any vector $\vec{v} = c_1\varphi^{(1)} + c_2\varphi^{(2)} + \cdots = \lim_{n \to \infty}\sum_{j=1}^{n}c_j\varphi^{(j)} \in \mathcal{V}$ in the inner product space can be written as a unique (infinite) sum of the eigenvectors which is exactly what a (Schauder) basis indicates, although it is only in the so-called $L^2$ sense.\footnote{We will demonstrate this using \textit{Bessel's Inequality} introduced in Section \ref{section:FouriertoDFT} for the Fourier basis. But the $L^2$ convergence is already good enough in many situations: In fact, $L^2$ is the only $L^p$ space that is also an inner product space.} We put the justification of (b) in the footnote below\footnote{For $\smash{\vec{v} \in \bigoplus_{j \in \{J\}} \mathcal{E}_{j}}$, it will be in the form of $\smash{\vec{v} = \sum_{j \in \{J\}} c_j\varphi^{(j)}}$ which is clearly orthogonal to $\vec{w} = \smash{\sum_{j' \notin \{J\}} c_{j'}\varphi^{(j')} \in \bigoplus_{j' \notin \{J\}} \mathcal{E}_{j'}}$, seen by expanding and computing $\langle \vec{v}, \vec{w} \rangle = 0$. Each $\smash{\langle \varphi^{(j)}, \varphi^{(j')} \rangle}$ term is zero when $j \neq j'$, as $\varphi^{(j)}$ and $\varphi^{(j')}$ are orthogonal to each other.} and (c) follows its counterpart in Theorem \ref{thm:spectral} but the group indices there are now replaced by individual indices. The resolution of the identity (d) is then derived similarly as in Theorem \ref{thm:spectral}: express any $\vec{v}$ as $\vec{v} = c_1\varphi^{(1)} + c_2\varphi^{(2)} + \cdots = \lim_{n \to \infty} \bigoplus_{j=1}^{n} c_j\varphi^{(j)}$ and by Definition \ref{defn:orthoprojinner}, we have
\begin{align*}
T_j(\vec{v}) &= \langle \vec{v}, \varphi^{(j)} \rangle \varphi^{(j)} \\
&= \langle \cdots + c_{j-1}\varphi^{(j-1)} + c_j\varphi^{(j)} + c_{j+1}\varphi^{(j+1)} + \cdots, \varphi^{(j)} \rangle \varphi^{(j)} \\
&= (\cdots(0) + c_{j}(1) + \cdots(0))\varphi^{(j)} = c_j\varphi^{(j)}
\end{align*}
and thus
\begin{align*}
I(\vec{v}) = \vec{v} &= c_1\varphi^{(1)} + c_2\varphi^{(2)} + \cdots = \lim_{n \to \infty} \sum_{j=1}^{n} c_j\varphi^{(j)} \\
&= \langle \vec{v}, \varphi^{(1)} \rangle \varphi^{(1)} + \langle \vec{v}, \varphi^{(2)} \rangle \varphi^{(2)} + \cdots = \lim_{n \to \infty} \sum_{j=1}^{n} \langle \vec{v}, \varphi^{(j)} \rangle \varphi^{(j)} \\ 
&= T_1(\vec{v}) + T_2(\vec{v}) + \cdots = (T_1 + T_2 + \cdots)(\vec{v}) = \left(\lim_{n \to \infty} \sum_{j=1}^{n} T_j\right)(\vec{v})
\end{align*}
So $I = \lim_{n \to \infty} \sum_{j=1}^{n} T_j$. (e) is also shown in a similar fashion:
\begin{align*}
T(\vec{v}) &= T(c_1\varphi^{(1)} + c_2\varphi^{(2)} + \cdots) = T\left(\lim_{n \to \infty} \sum_{j=1}^{n} c_j\varphi^{(j)}\right) \\
&= c_1T(\varphi^{(1)}) + c_2T(\varphi^{(2)}) + \cdots = \lim_{n \to \infty} \sum_{j=1}^{n} c_jT(\varphi^{(j)}) & \text{($T$ is linear\footnotemark)}\\
&= \lambda_1 c_1\varphi^{(1)} + \lambda_2 c_2\varphi^{(2)} + \cdots =  \lim_{n \to \infty} \sum_{j=1}^{n} \lambda_jc_j\varphi^{(j)} \\
&= \lambda_1 T_1(\vec{v}) + \lambda_2 T_2(\vec{v}) + \cdots & \text{(as in deriving (d))} \\
&= (\lambda_1T_1 + \lambda_2T_2 + \cdots)\vec{v} = \left(\lim_{n \to \infty} \sum_{j=1}^{n} \lambda_jT_j\right)(\vec{v}) 
\end{align*}
\footnotetext{Notice that in such an infinite series, linearity is actually not sufficient, see the caution just below the derivation.}Be wary that in (e), the linear operator $T$ applied on $\vec{v}$ is required to be Hermitian (truly self-adjoint), not only formally self-adjoint.\footnote{Curious readers may like to search about what a compact operator is.} This means that there will be problems if boundary terms appear when an integral-like inner product is used. This issue is going to be raised in the next example and short exercise.\par

Recall that in Example \ref{exmp:Fourierbasis} we have derived the Fourier basis $\{\sin(x), \sin(2x), \\ \sin(3x), \ldots, 1, \cos(x), \cos(2x), \cos(3x), \ldots\}$ for the $L^2[-\pi, \pi]$ Hilbert space with the inner product of Equation (\ref{eqn:integralinner}). To normalize it, note that each eigenfunction has a norm of $\frac{1}{\sqrt{\pi}}$\footnotemark ($\frac{1}{\sqrt{2\pi}}$ for $1$) and hence the orthonormal Fourier basis is 
\begin{align}
\begin{aligned}
&\left\{\frac{1}{\sqrt{\pi}}\sin(x), \frac{1}{\sqrt{\pi}}\sin(2x), \frac{1}{\sqrt{\pi}}\sin(3x), \ldots,\right. \\
&\left.\frac{1}{\sqrt{2\pi}}, \frac{1}{\sqrt{\pi}}\cos(x), \frac{1}{\sqrt{\pi}}\cos(2x), \frac{1}{\sqrt{\pi}}\cos(3x), \ldots\right\} 
\end{aligned}
\end{align} According to part (a) of the Spectral Theorem \ref{thm:spectralinner}, any function $f \in L^2[-\pi, \pi]$ can thus be expanded in a so-called \index{Fourier Series}\keywordhl{Fourier series}, which is often expressed in the form of
\begin{subequations}
 \label{eqn:fourierseries}
\begin{align}
f &= \frac{a_0}{2} + a_1\cos(x) + a_2\cos(2x) + a_3\cos(3x) + \cdots \nonumber \\
& \quad + b_1\sin(x) + b_2\sin(2x) + b_3\sin(3x) + \cdots \\
&= \frac{a_0}{2} + \sum_{m=1}^{\infty} a_m \cos(mx) + \sum_{n=1}^{\infty} b_n \sin(nx) 
\end{align}
\end{subequations}
an infinite sum of these sinusoidal eigenfunctions (omitted the $\frac{1}{\sqrt{\pi}}$ normalization factor). To compute the \index{Fourier Coefficients}\keywordhl{Fourier coefficients} $a_m$ and $b_n$, we use (d) of the Spectral Theorem where the $T_j$ are in the form of Equation (\ref{eqn:orthoprojinner}), leading to\footnotetext{We will only show this for the cosines but the calculation is the same for the sines. For any integer $m$,
\begin{align*}
\norm{\cos(mx)}^2 &= \int_{-\pi}^{\pi} \cos(mx)\overline{\cos(mx)} dx \\
&= \int_{-\pi}^{\pi} \cos^2(mx) dx \\
&= \int_{-\pi}^{\pi} \frac{1}{2}(1 + \cos(2mx)) dx \\
&= [\frac{1}{2}x + \frac{1}{4m}\sin(2mx)]_{-\pi}^{\pi} = \frac{1}{2}(2\pi) + (0) = \frac{1}{\pi} \end{align*}}
\begin{align*}
f &= \sum_{j} \langle f, \varphi^{(j)} \rangle \varphi^{(j)} \\ 
&= \left\langle f, \frac{1}{\sqrt{2\pi}} \right\rangle \left(\frac{1}{\sqrt{2\pi}}\right)\\
&\quad + \left\langle f, \frac{1}{\sqrt{\pi}}\cos(x) \right\rangle \left(\frac{1}{\sqrt{\pi}}\cos(x)\right) + \left\langle f, \frac{1}{\sqrt{\pi}}\cos(2x) \right\rangle \left(\frac{1}{\sqrt{\pi}}\cos(2x)\right) + \cdots \\
&\quad + \left\langle f, \frac{1}{\sqrt{\pi}}\sin(x) \right\rangle \left(\frac{1}{\sqrt{\pi}}\sin(x)\right) + \left\langle f, \frac{1}{\sqrt{\pi}}\sin(2x) \right\rangle \left(\frac{1}{\sqrt{\pi}}\sin(2x)\right) + \cdots \\
&= \frac{1}{2\pi} \langle f, 1 \rangle + \frac{1}{\pi} \langle f, \cos(x) \rangle \cos(x) + \frac{1}{\pi} \langle f, \cos(2x) \rangle \cos(2x) + \cdots \\
&\quad + \frac{1}{\pi}\langle f, \sin(x) \rangle \sin(x) + \frac{1}{\pi}\langle f, \sin(2x) \rangle \sin(2x) + \cdots
\end{align*}
Comparing this expression with the form of the Fourier series in Equation (\ref{eqn:fourierseries}), we yield the following formulae for the Fourier coefficients.
\begin{proper}[Fourier Series]
\label{proper:fourierseries}
For a function $f(x)$ in the $L^2[-\pi, \pi]$ space, it can be written as a Fourier series of
\begin{align}
f(x) = \frac{a_0}{2} + \sum_{m=1}^{\infty} a_m \cos(mx) + \sum_{n=1}^{\infty} b_n \sin(nx) \label{eqn:fourierseri}
\end{align}
where the Fourier coefficients are given by
\begin{subequations}
\begin{align}
a_m &= \frac{1}{\pi}\langle f, \cos(mx) \rangle = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x)\cos(mx) dx \label{eqn:fouriera} \\
b_n &= \frac{1}{\pi}\langle f, \sin(nx) \rangle = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x)\sin(nx) dx \label{eqn:fourierb}
\end{align}    
\end{subequations}
\end{proper}
\begin{exmp}
\label{exmp:fourierx}
Expand the function $x$ as a Fourier series in the interval $[-\pi, \pi]$.
\end{exmp}
\begin{solution}
It is simply to compute $a_m$ and $b_n$ mechanically according to Equations (\ref{eqn:fouriera}) and (\ref{eqn:fourierb}). But note that since $x$ is an odd function and cosines are even, the integrals for $a_m$ where the interval is symmetric about the origin and the resulting integrands are odd, are thus all zero. Now we just have to calculate the general form of $b_n$, as below.
\begin{align*}
b_n &= \frac{1}{\pi} \int_{-\pi}^{\pi} x\sin(nx) dx \\
&= \frac{1}{\pi}[-\frac{1}{n}x\cos(nx)]_{-\pi}^{\pi} - \frac{1}{\pi}\int_{-\pi}^{\pi} (-\frac{1}{n}\cos(nx)) dx \\
&= \frac{1}{\pi}[-\frac{1}{n}\pi\cos(n\pi) + \frac{1}{n}(-\pi)\cos(n(-\pi))] + \frac{1}{n\pi}\int_{-\pi}^{\pi} \cos(nx) dx \\
&= -\frac{1}{n}\cos(n\pi) - \frac{1}{n}\cos(-n\pi) + \frac{1}{n^2\pi}[\sin(nx)]_{-\pi}^{\pi} \\
&= -\frac{2(-1)^n}{n} + (0) = -\frac{2(-1)^n}{n}
\end{align*}
as $\cos(n\pi) = \cos(-n\pi) = (-1)^n$. Therefore, the Fourier series of $x$ is
\begin{align*}
x = -2 \sum_{n=1}^{\infty} \frac{(-1)^n}{n} \sin(nx)
\end{align*}
$\blacktriangleright$ Short Exercise: It seems that if we apply part (e) of the Spectral Theorem \ref{thm:spectralinner}, where the self-adjoint operator $T = \frac{d^2}{dx^2}$ induces the Fourier basis as in Example \ref{exmp:Fourierbasis}, on the Fourier series of $x$ above, the L.H.S. will readily become zero but the R.H.S. will still contain non-trivial sine terms under the twice differentiation so they are obviously unequal. Why does this paradox occur?\footnotemark
\end{solution}

\section{Special Polynomials}

\subsection{Sturm-Liouville Equations}

The theory of Hermitian operators is most widely applied in the context of \index{Sturm-Liouville Equation/Operator}\keywordhl{Sturm-Liouville Equations} that are frequently encountered when solving \index{Partial Differential Equation (PDE)}\keywordhl{Partial Differential Equations (PDEs)} with the technique of \index{Separation of Variables}\textit{separation of variables}. A Sturm-Liouville equation takes the general form of
\begin{align}
\frac{d}{dx} \left(p(x) \frac{dy}{dx}\right) + q(x) y + \lambda w(x)y = 0
\end{align}
where $p(x)$, $q(x)$ are real functions of $x$ and $w(x)$ is a real positive-definite weighting function in $x$. It can be written in terms of the \keywordhl{Sturm-Liouville operator}
\begin{align}
\mathcal{L}[f] = -\left[\frac{d}{dx} \left(p(x) \frac{d}{dx}\right) + q(x)\right][f]    
\end{align}
as
\begin{align}
\mathcal{L}[y] = \lambda w(x)y
\end{align}
which apparently poses an eigenvalue-eigenfunction problem with a weighting $w(x)$ where $\lambda$ is the eigenvalue. The Sturm-Liouville operator is self-adjoint with respect to the inner product (\ref{eqn:integralinner}) and can become Hermitian under suitable boundary conditions as shown below:\footnotetext{\label{foot:fourier}It is because the Hermiticity of $T$, required by (e) of the Spectral Theorem, is not satisfied for the function $x$. The boundary terms derived in Example \ref{exmp:Fourierbasis}
\begin{align*}
[f(x) \frac{d}{dx}\overline{([g(x)])}]_{-\pi}^\pi - [\frac{d}{dx}(f(x)) \overline{g(x)}]_{-\pi}^\pi    
\end{align*}
will vanish only if $f(x)$ and $g(x)$ and their derivatives $f'(x)$ and $g'(x)$ are periodic, i.e.\ equal at the two end-points $-\pi, \pi$. Clearly, $x$ is not a periodic function so it does not satisfy the boundary conditions and this part of the Spectral Theorem is not applicable. For reference, in general, a Fourier series can be differentiated terms by terms if the function $f(x)$ is continuous, takes the same value at the two end-points $f(-\pi) = f(\pi)$ and its derivative $f'(x)$ is piecewise continuous. A Fourier series essentially extends the given function by repeating it with a period of $2\pi$, so actually such a periodic extension of the function $x$ will be discontinuous at the boundaries between the repeated graphs.}
\begin{align*}
\langle \mathcal{L}[f], g \rangle &= \int_a^b \mathcal{L}[f(x)] \overline{g(x)} dx \\
&= \int_a^b -\left(\left(\frac{d}{dx} (p(x) \frac{df(x)}{dx}\right) + q(x)f(x)\right)  \overline{g(x)} dx \\
&= -\int_a^b \frac{d}{dx} \left(p(x) \frac{d f(x)}{dx}\right) \overline{g(x)} - \int_a^b q(x)f(x)\overline{g(x)} dx \\
&= -\left[p(x) \frac{df(x)}{dx} \overline{g(x)}\right]_a^b + \int_a^b \frac{df(x)}{dx} p(x) \frac{d\overline{g(x)}}{dx} dx \\
&\quad - \int_a^b q(x)f(x)\overline{g(x)} dx\\
&= -\left[p(x) \frac{df(x)}{dx} \overline{g(x)}\right]_a^b + \left[f(x) p(x) \frac{d\overline{g(x)}}{dx}\right]_a^b  \\
&\quad -\int f(x)\frac{d}{dx}\left(p(x) \frac{d\overline{g(x)}}{dx}\right) dx - \int_a^b q(x)f(x)\overline{g(x)} dx \\
&= -\left[p(x) \frac{df(x)}{dx} \overline{g(x)}\right]_a^b + \left[f(x) p(x) \frac{d\overline{g(x)}}{dx}\right]_a^b  \\
&\quad + \int f(x) \left(-\left[\frac{d}{dx}\left(p(x) \frac{d\overline{g(x)}}{dx}\right) + q(x)\overline{g(x)}\right]\right) dx \\
&= -\left[\frac{df(x)}{dx} p(x) \overline{g(x)}\right]_a^b + \left[f(x) p(x) \frac{d\overline{g(x)}}{dx}\right]_a^b \\
&\quad + \int f(x) \overline{\left(-\left[\frac{d}{dx}\left(p(x) \frac{d g(x)}{dx}\right) + q(x)g(x)\right]\right)} dx \\
&\quad \text{($p(x), q(x)$ are real)} \\
&= -\left[\frac{df(x)}{dx} p(x) \overline{g(x)}\right]_a^b + \left[f(x) p(x) \frac{d\overline{g(x)}}{dx}\right]_a^b \\
&\quad + \int_a^b f(x) \overline{\mathcal{L}[g(x)]} dx
\end{align*}
So $\mathcal{L}^\dag = \mathcal{L}$ is self-adjoint, and the boundary condition for the boundary terms to vanish is $\smash{[f(x) p(x) \frac{d\overline{g(x)}}{dx}]_a^b} = 0$ for any pair of $f(x)$ and $g(x)$\footnote{This implies that $\smash{[\frac{d\overline{f(x)}}{dx} p(x) g(x)]_a^b} = 0$ as well when the roles of $f(x)$ and $g(x)$ are exchanged. Conjugating this relation gives $\smash{[\frac{df(x)}{dx} p(x) \overline{g(x)}]_a^b} = 0$ so the first boundary term also vanishes.} so that $\mathcal{L}$ can be Hermitian. This boundary condition will also have to be obeyed by the eigenfunctions $\varphi_j$ where $\mathcal{L}[\varphi_j] = \lambda w(x)\varphi_j$. However, sometimes $p(x)$ may take a form such that at the end-points its value may be zero, so that $\mathcal{L}$ is automatically Hermitian over this \index{Natural Interval}\textit{natural interval}. The reality of eigenvalues (left as an exercise, see Properties \ref{proper:hermrealeiginner}) and the orthogonality of eigenfunctions are enabled by the Hermiticity of the Sturm-Liouville operator as in Properties \ref{proper:orthogonalherm} but with an extra factor from the weighting function $w(x)$:
\begin{align*}
\int_a^b \mathcal{L}[\varphi_1]\overline{\varphi_2} &= \int_a^b \varphi_1\overline{\mathcal{L}[\varphi_2]} \\
\int_a^b \lambda_1 w(x)\varphi_1\overline{\varphi_2} dx &= \int_a^b \varphi_1\overline{\lambda_2 w(x)\varphi_2} dx \\
\int_a^b \lambda_1 w(x)\varphi_1\overline{\varphi_2} dx &= \int_a^b \lambda_2 w(x)\varphi_1 \overline{\varphi_2} dx\\
(\lambda_1 - \lambda_2) \int_a^b w(x)\varphi_1 \overline{\varphi_2} dx &= 0
\end{align*}
so that $\int_a^b w(x)\varphi_1 \overline{\varphi_2} dx = 0$ and the two different eigenfunctions are orthogonal with respect to the inner product (\ref{eqn:integralinner2}) assuming that $\lambda_1 \neq \lambda_2$. To transform a general second-order ODE appearing as $\smash{P(x)\frac{d^2y}{dx^2} + R(x)\frac{dy}{dx}} + Q(x)y + \lambda w(x)y \allowbreak = 0$ into the Sturm-Liouville form, we can multiply it by the integrating factor 
\begin{align}
F(x) = \exp(\int \frac{R(x) - P'(x)}{P(x)} dx) \label{eqn:intfactorsl}
\end{align}
so that it becomes
\begin{align}
&\quad [F(x)P(x)y']' + F(x)Q(x)y + \lambda F(x)w(x)y \nonumber \\
&= [p(x)y']' + q(x)y + \lambda F(x)w(x)y = 0
\end{align}
\footnotemark{} where $p(x) = F(x)P(x)$, $q(x) = F(x)Q(x)$, and the new weighting is $F(x)w(x)$ which is still positive-definite as $F(x)$ is an exponential function.

\begin{exmp}
\label{exmp:hermiteeqn}
Convert the \index{Hermite's Equation}\keywordhl{Hermite's Equation}
\begin{align}
y'' - 2xy' + 2\nu y = 0
\end{align}
to the Sturm-Liouville form and find its eigenfunctions.
\end{exmp}
\begin{solution}
The integrating factor, by (\ref{eqn:intfactorsl}), is
\begin{align*}
F(x) &= \exp(\int \frac{(-2x) - (0)}{(1)} dx) \\
&= \exp(\int -2x dx) = e^{-x^2}
\end{align*}
and hence by multiplying it to the Hermite's Equation
\begin{align}
e^{-x^2}y'' - 2xe^{-x^2}y' + 2\nu e^{-x^2} y &= 0 \nonumber \\
(e^{-x^2}y')' + 2\nu e^{-x^2} y &= 0
\end{align}
yields the Sturm-Liouville form with $p(x) = e^{-x^2}$, $q(x) = 0$, $\lambda = 2\nu$, $w(x) = e^{-x^2}$. We can find the eigenfunctions by the method of \index{Series Solution}\textit{series solution}. Assume a series solution of\footnotetext{$F(x)$ is derived such that $F(x)P(x)y'' + F(x)R(x)y' = [F(x)P(x)y']'$:
\begin{align*}
[F(x)P(x)y']' = F(x)P(x)y'' + F(x)R(x)y' &= F(x)P(x)y'' + F(x)P'(x)y' + F'(x)P(x)y' \\
F(x)R(x)y' &=  F(x)P'(x)y' + F'(x)P(x)y' \\
F(x)(R(x) - P'(x)) &= F'(x)P(x) \\
\frac{dF(x)}{F(x)} &= \frac{R(x) - P'(x)}{P(x)} \\
\ln F(x) &= \int \frac{R(x) - P'(x)}{P(x)} dx \\
\Rightarrow F(x) &= \exp(\int \frac{R(x) - P'(x)}{P(x)} dx)
\end{align*}} 
\begin{align}
y = a_0 + a_1x + a_2x^2 + \cdots = \sum_{n=0}^{\infty} a_n x^n    
\end{align}
then
\begin{subequations}
\begin{align}
y' &= \sum_{n=1}^{\infty} n a_nx^{n-1} \\
y'' &= \sum_{n=2}^{\infty} n(n-1) a_nx^{n-2}
\end{align}    
\end{subequations}
Substituting them into the original form of Hermite's equation, we have
\begin{align}
\sum_{n=2}^{\infty} n(n-1) a_nx^{n-2} - 2x\sum_{n=1}^{\infty} n a_nx^{n-1} + 2\nu \sum_{n=0}^{\infty} a_n x^n &= 0 \nonumber \\
\sum_{n=0}^{\infty} (n+2)(n+1) a_{n+2}x^{n} - 2\sum_{n=1}^{\infty} n a_nx^{n} + 2\nu \sum_{n=0}^{\infty} a_n x^n &= 0 
\end{align}
Comparing the coefficients of $x^n$ gives a recurrence relation of
\begin{align}
(n+2)(n+1)a_{n+2} - 2n a_n + 2\nu a_n &= 0
\end{align}
for $n \geq 1$. For $n=0$ it is simply $2a_2 + 2\nu a_0 = 0$. Rearranging we have
\begin{align}
a_{n+2} = \frac{2(n-\nu)}{(n+2)(n+1)}a_n
\end{align}
Such a form indicates that there will be two series, one for the odd indices and another for the even. Note that for the corresponding Sturm-Liouville operator to become Hermitian, the boundary term
\begin{align*}
[\varphi_1 p(x) \frac{d\overline{\varphi_2}}{dx}]_a^b = [e^{-x^2} \varphi_1 \frac{d\overline{\varphi_2}}{dx}]_a^b = 0
\end{align*}
has to vanish, and the $e^{-x^2}$ factor means that the natural interval would be $(-\infty, \infty)$ along the entire real axis. The eigenfunctions must therefore grow slower than $e^{x^2}$, which happens when one of the series solutions is truncated to a polynomial when $\nu = n$. These polynomials are subsequently known as the \index{Hermite polynomials}\textit{Hermite polynomials}. Here we compute the first four of them ($n=0,1,2,3$). For $n=0$, we simply have $y = a_0$ as $2a_2 + 2(0)a_0 = 0$ means $a_2 = 0$ and the series is immediately terminated. For $n=1$, we similarly have $y = a_1x$ as $a_3 = \frac{2(1-1)}{(3)(2)}a_1 = (0)a_1 = 0$. Going up to $n=2$, we have
\begin{align*}
a_2 &= \frac{2(0-2)}{(2)(1)}a_0 = -2a_0 \\
a_4 &= \frac{2(2-2)}{(4)(3)}a_2 = 0
\end{align*}
So $y = a_0 - 2a_0 x^2 = a_0(1-2x^2)$. In the same fashion, when $n=3$, we have
\begin{align*}
a_3 &= \frac{2(1-3)}{(3)(2)}a_1 = -\frac{2}{3}a_1 \\
a_5 &= \frac{2(3-3)}{(5)(4)}a_3 = 0
\end{align*}
hence $y = a_1x - \frac{2}{3}a_1x^3 = a_1(x-\frac{2}{3}x^3)$. By convention, the Hermite polynomials are scaled in a way such that the leading highest degree term has a coefficient of $2^n$. So the first four Hermite polynomials are
\begin{align}
\begin{aligned}
H_0(x) &= 1 \\
H_1(x) &= 2x \\
H_2(x) &= 4x^2 - 2 \\
H_3(x) &= 8x^3 - 12x
\end{aligned}
\end{align}    
$\blacktriangleright$ Short Exercise: Find the Hermite polynomial of degree $4$, $H_4(x)$, corresponding to $\nu = n = 4$.\footnotemark
\end{solution}

\subsection{Generating Special Polynomials by Gram-Schmidt Orthogonalization}

Since a Sturm-Liouville operator is Hermitian given appropriate boundary conditions and Hermiticity leads to a complete orthonormal basis that is countable if the underlying Hilbert space is separable, which we assume to be that so, we can derive the corresponding orthogonal \index{Special Polynomials}\keywordhl{special polynomials} to the Sturm-Liouville equation, e.g.\ Hermite polynomials in the previous part, by applying the Gram-Schmidt process on the standard polynomial basis $\{1, x, x^2, x^3, \ldots\}$ to any degree with respect to the inner product in Equation (\ref{eqn:integralinner2}). The details of the calculation are shown below.\footnotetext{Using the recurrence relation, we have
\begin{align*}
a_2 &= \frac{2(0-4)}{(2)(1)}a_0 = -4a_0 \\
a_4 &= \frac{2(2-4)}{(4)(3)}a_2 = -\frac{1}{3}a_2 = \frac{4}{3}a_0 \\
a_6 &= \frac{2(4-4)}{(6)(5)}a_4 = 0
\end{align*}
hence $y = a_0 - 4a_0x^2 + \frac{4}{3}a_0x^4$ and after scaling it becomes $H_4(x) = 16x^4 - 48x^2 + 12$.}
\begin{exmp}
Compute the first four Hermite polynomials as introduced in Example \ref{exmp:hermiteeqn} by Gram-Schmidt Orthogonalization.
\end{exmp}
\begin{solution}
The Hermite polynomials have a weighting of $w(x) = e^{-x^2}$ and are integrated over the entire real axis $(-\infty, \infty)$. It is instructive to first note that for any non-negative integer $m$
\begin{align*}
\int_{-\infty}^{\infty} x^{2m+1}e^{-x^2} dx = 0
\end{align*}
since $e^{-x^2}$ is even and $x^{2m+1}$ is odd, and
\begin{align*}
\int_{-\infty}^{\infty} x^{2m}e^{-x^2} dx = \frac{(1)(3)(5)\cdots(2m-1)\sqrt{\pi}}{2^m}
\end{align*}
(see the footnote next page)\footnotemark{} Now we apply Gram-Schmidt Orthogonalization on the standard polynomials $\{\vec{u}^{(1)}, \vec{u}^{(2)}, \vec{u}^{(3)}, \vec{u}^{(4)}, \ldots\} = \{1, x, x^2, x^3, \ldots\}$ according to Definition \ref{defn:GSorthinner}. The zeroth degree Hermite polynomial is trivially $\vec{v}^{(1)} = H_0(x) = 1$ and $\norm{\vec{v}^{(1)}}^2 = \sqrt{\pi}$. Next, for $n=1$, we have
\begin{align*}
\vec{v}^{(2)} &= \vec{u}^{(2)} - \frac{\langle \vec{u}^{(2)}, \vec{v}^{(1)}\rangle}{\norm{\vec{v}^{(1)}}^2} \vec{v}^{(1)} \\
&= x - \frac{\int_{-\infty}^{\infty} e^{-x^2}(x)(1) dx}{\norm{\vec{v}^{(1)}}^2} \vec{v}^{(1)} \\
&= x - \frac{\int_{-\infty}^{\infty} xe^{-x^2} dx}{\sqrt{\pi}} (1) = x - (0)(1) = x
\end{align*}
So the Hermite polynomial of degree $1$ is just $\vec{v}^{(2)} \leftarrow H_1(x) = 2x$ (scaled by convention, similar for the following). Now\footnotetext{It is a well-known result from multivariable calculus that
\begin{align*}
\int_{-\infty}^{\infty} e^{-x^2} dx = \sqrt{\pi}
\end{align*}
for the Gaussian integral. By mathematical induction and integration by parts, we have
\begin{align*}
\int_{-\infty}^{\infty} x^{2(m+1)}e^{-x^2} dx &= \int_{-\infty}^{\infty} x^{2m+2}e^{-x^2} dx \\
&= \int_{-\infty}^{\infty} -\frac{1}{2}x^{2m+1} (-2xe^{-x^2}) dx \\
&= \int_{-\infty}^{\infty} -\frac{1}{2}x^{2m+1} d(e^{-x^2}) \\
&= [-\frac{1}{2}x^{2m+1}e^{-x^2}]_{-\infty}^{\infty} - \int_{-\infty}^{\infty} (-\frac{2m+1}{2}x^{2m})e^{-x^2} dx \\
&= (0) + \frac{2m+1}{2} \int_{-\infty}^{\infty} x^{2m}e^{-x^2} dx \\
&= \frac{(1)(3)(5)\cdots(2m-1)(2m+1)\sqrt{\pi}}{2^{(m+1)}} \\
&\quad \text{(via the induction hypothesis)}
\end{align*}
so the formula is established.} 
\begin{align*}
\norm{\vec{v}^{(2)}}^2 &= \int_{-\infty}^{\infty} (2x)^2 e^{-x^2} dx \\
&= 4\int_{-\infty}^{\infty} x^2 e^{-x^2} dx \\
&= 4\left(\frac{1}{2}\sqrt{\pi}\right) = 2\sqrt{\pi}
\end{align*}
For $n=2$, we have
\begin{align*}
\vec{v}^{(3)} &= \vec{u}^{(3)} - \frac{\langle\vec{u}^{(3)}, \vec{v}^{(1)}\rangle}{\norm{\vec{v}^{(1)}}^2} \vec{v}^{(1)} - \frac{\langle\vec{u}^{(3)}, \vec{v}^{(2)}\rangle}{\norm{\vec{v}^{(2)}}^2} \vec{v}^{(2)} \\
&= x^2 - \frac{\int_{-\infty}^{\infty} (x^2)(1) e^{-x^2} dx}{\sqrt{\pi}} (1) - \frac{\int_{-\infty}^{\infty} (x^2)(2x) e^{-x^2} dx}{2\sqrt{\pi}} (2x) \\
&= x^2 - \frac{\int_{-\infty}^{\infty} x^2 e^{-x^2} dx}{\sqrt{\pi}} (1) - \frac{2 \int_{-\infty}^{\infty} x^3 e^{-x^2} dx}{2\sqrt{\pi}} (2x) \\
&= x^2 - \frac{(\frac{1}{2}\sqrt{\pi})}{\sqrt{\pi}} (1) - (0) (2x) \\
&= x^2 - \frac{1}{2}
\end{align*}
Hence $\vec{v}^{(3)} \leftarrow H_2(x) = 4x^2 - 2$, and $\norm{\vec{v}^{(3)}}^2 = \int_{-\infty}^{\infty} (4x^2 - 2)^2 e^{-x^2} dx = \int_{-\infty}^{\infty} (16 x^4 - 16 x^2 + 4)e^{-x^2} dx = 16(\frac{3}{4}\sqrt{\pi}) - 16(\frac{1}{2}\sqrt{\pi}) + 4(\sqrt{\pi}) = 8\sqrt{\pi}$. Finally, for $n=3$
\begin{align*}
\vec{v}^{(4)} &= \vec{u}^{(4)} - \frac{\langle\vec{u}^{(4)}, \vec{v}^{(1)}\rangle}{\norm{\vec{v}^{(1)}}^2} \vec{v}^{(1)} - \frac{\langle\vec{u}^{(4)}, \vec{v}^{(2)}\rangle}{\norm{\vec{v}^{(2)}}^2} \vec{v}^{(2)} - \frac{\langle\vec{u}^{(4)}, \vec{v}^{(3)}\rangle}{\norm{\vec{v}^{(3)}}^2} \vec{v}^{(3)}\\  
&= x^3 - \frac{\int_{-\infty}^{\infty} (x^3)(1) e^{-x^2} dx}{\sqrt{\pi}} (1) - \frac{\int_{-\infty}^{\infty} (x^3)(2x) e^{-x^2} dx}{2\sqrt{\pi}} (2x) \\
&\quad - \frac{\int_{-\infty}^{\infty} (x^3)(4x^2 - 2) e^{-x^2} dx}{8\sqrt{\pi}} (4x^2 - 2) \\
&= x^3 - (0)(1) - \frac{2\int_{-\infty}^{\infty} x^4 e^{-x^2} dx}{2\sqrt{\pi}} (2x) - (0)(4x^2 - 2) \\
&= x^3 - \left(\frac{3\sqrt{\pi}}{4\sqrt{\pi}}\right)(2x) = x^3 - \frac{3}{2}x
\end{align*}
thus $\vec{v}^{(4)} \leftarrow H_3(x) = 8x^3 - 12x$.
\end{solution}

\section{Earth Science Applications}

\begin{exmp}
Consider the \index{Linearized Shallow Water System}\textit{linearized shallow water system} along the Equator:
\begin{subequations}
\begin{empheq}[left={\empheqlbrace}]{alignat=1}
\frac{\partial u}{\partial t} - \beta y v &= -\frac{\partial \Phi}{\partial x} \\
\frac{\partial v}{\partial t} + \beta y u &= -\frac{\partial \Phi}{\partial y} \\
\frac{\partial \Phi}{\partial t} &= - gH\left(\frac{\partial u}{\partial x} + \frac{\partial v}{\partial y}\right)
\end{empheq}    
\end{subequations}
The first two equations are the momentum equations, and the last one is the continuity equation. $u, v, \Phi$ are the zonal, meridional winds, and the geopotential. The constants include $\beta$ which denotes the beta effect (the change of $f$ over latitudes), and $g, H$ as the gravitational acceleration and equivalent depth. By assuming a traveling wave solution (written in the complex form, $i$ is the imaginary number)
\begin{subequations}
\begin{align}
u &= \hat{u}(y)\exp(i(kx-\omega t)) \\
v &= \hat{v}(y)\exp(i(kx-\omega t)) \\
\Phi &= \hat{\Phi}(y)\exp(i(kx-\omega t))
\end{align}    
\end{subequations}
so that $\hat{u}(y), \hat{v}(y), \hat{\Phi}(y)$ are now functions of $y$ only. Substitute them into the three shallow water equations above, simplify them, and apply some suitable change of variables to arrive at the Hermite's equation, and hence derive the dynamical modes of the linearized shallow water system.
\end{exmp}
\begin{solution}
Plugging the ansatz in, the system of equations becomes
\begin{align}
-i\omega \hat{u}(y) e^{i(kx-\omega t)} - \beta y \hat{v}(y)e^{i(kx-\omega t)} &= -i k \hat{\Phi}(y)e^{i(kx-\omega t)} \nonumber\\
-i\omega \hat{u}(y) - \beta y \hat{v}(y) &= -i k \hat{\Phi}(y) \label{eqn:linshallow1}
\end{align}
\begin{align}
-i\omega \hat{v}(y)e^{i(kx-\omega t)} + \beta y \hat{u}(y)e^{i(kx-\omega t)} &= -\frac{d\hat{\Phi}(y)}{dy}e^{i(kx-\omega t)} \nonumber\\
-i\omega \hat{v}(y) + \beta y \hat{u}(y) &= -\frac{d\hat{\Phi}(y)}{dy} \label{eqn:linshallow2}
\end{align}
and
\begin{align}
-i\omega \hat{\Phi}(y)e^{i(kx-\omega t)} &= - gH\left(i k\hat{u}(y)e^{i(kx-\omega t)} + \frac{d\hat{v}(y)}{dy}e^{i(kx-\omega t)}\right) \nonumber\\
-i\omega \hat{\Phi}(y) &= - gH \left(ik\hat{u}(y) + \frac{d\hat{v}(y)}{dy}\right) \label{eqn:linshallow3} 
\end{align}
Differentiating (\ref{eqn:linshallow3}) gives
\begin{equation}
-i\omega \frac{d\hat{\Phi}(y)}{dy} = - gH \left(i k\frac{d\hat{u}(y)}{dy} + \frac{d^2\hat{v}(y)}{dy^2}\right) \label{eqn:linshallow4}      
\end{equation}
Substituting (\ref{eqn:linshallow2}) into (\ref{eqn:linshallow4}) leads to
\begin{align}
i\omega (-i\omega \hat{v}(y) + \beta y \hat{u}(y)) &= - gH\left(i k\frac{d\hat{u}(y)}{dy} + \frac{d^2\hat{v}(y)}{dy^2}\right) \nonumber \\
\omega^2 \hat{v}(y) + i \omega\beta y \hat{u}(y) &= - gH\left(i k\frac{d\hat{u}(y)}{dy} + \frac{d^2\hat{v}(y)}{dy^2}\right) \label{eqn:linshallow5}   
\end{align}
Using (\ref{eqn:linshallow1}) in (\ref{eqn:linshallow5}) gives
\begin{align}
\omega^2 \hat{v}(y) + i \omega\beta y \hat{u}(y) &= -gH\left(-\frac{k}{\omega} \frac{d}{dy} (-i k \hat{\Phi}(y) + \beta y\hat{v}(y)) + \frac{d^2\hat{v}(y)}{dy^2}\right) \nonumber \\
\omega^2 \hat{v}(y) + i \omega\beta y \hat{u}(y) &= 
\frac{-i gHk^2}{\omega}\frac{d\hat{\Phi}(y)}{dy} + \frac{gHk}{\omega}\left(\beta\hat{v}(y) + \beta y\frac{d\hat{v}(y)}{dy}\right) \nonumber \\ 
&\quad- gH \frac{d^2\hat{v}(y)}{dy^2}   \label{eqn:linshallow6}   
\end{align}
Applying both (\ref{eqn:linshallow2}) and (\ref{eqn:linshallow3}) in (\ref{eqn:linshallow6}) yields
\begin{align}
&\quad \omega^2 \hat{v}(y) + i \omega\beta y \hat{u}(y) \nonumber \\
&= \frac{i gHk^2}{\omega}(-i\omega \hat{v}(y) + \beta y \hat{u}(y)) + \frac{gHk}{\omega}\left(\beta\hat{v}(y) + \beta y \left(\frac{i \omega}{gH} \hat{\Phi}(y) - i k\hat{u}(y)\right)\right)  \nonumber \\
&\quad - gH \frac{d^2\hat{v}(y)}{dy^2}    
\end{align}
Rearranging then gives
\begin{align}
&\quad \omega^2 \hat{v}(y) + i \omega\beta y \hat{u}(y) \nonumber \\
&= gHk^2 \hat{v}(y) \mathcolor{red}{+ \frac{i gHk^2}{\omega} \beta y \hat{u}(y)} + \frac{gHk}{\omega}\beta\hat{v}(y) + i k \beta y \hat{\Phi}(y) \nonumber \\
&\quad \mathcolor{red}{- \frac{i gHk^2}{\omega} \beta y \hat{u}(y)} - gH \frac{d^2\hat{v}(y)}{dy^2} \nonumber \\
&= gHk^2 \hat{v}(y)  + \frac{gHk}{\omega}\beta\hat{v}(y) + i k \beta y \hat{\Phi}(y) - gH \frac{d^2\hat{v}(y)}{dy^2}
\end{align}
Finally, using (\ref{eqn:linshallow1}) again, we have
\begin{align}
\omega^2 \hat{v}(y) - \beta^2 y^2 \hat{v}(y) = gHk^2 \hat{v}(y)  + \frac{gHk}{\omega}\beta\hat{v}(y) - gH \frac{d^2\hat{v}(y)}{dy^2}    
\end{align}
Cleaning this up, we obtain
\begin{align}
\frac{d^2\hat{v}(y)}{dy^2} + \left[\left(\frac{\omega^2}{gH} - k^2 - \beta\frac{k}{\omega}\right) - \frac{\beta^2}{gH}y^2\right]\hat{v}(y) = 0
\end{align}
Finally, with a change of variable $\tilde{y} = y/y_0$, $y_0 = (\sqrt{gH}/\beta)^{1/2}$, it becomes
\begin{align}
\frac{\beta}{\sqrt{gH}} \frac{d^2\hat{v}(\tilde{y})}{d\tilde{y}^2} + \left[\left(\frac{\omega^2}{gH} - k^2 - \beta\frac{k}{\omega}\right) - \frac{\beta^2}{gH}\left(\frac{\sqrt{gH}}{\beta}\tilde{y}^2\right)\right]\hat{v}(\tilde{y}) &= 0 \nonumber \\
\frac{d^2\hat{v}(\tilde{y})}{d\tilde{y}^2} + \left[\frac{\sqrt{gH}}{\beta}\left(\frac{\omega^2}{gH} - k^2 - \beta\frac{k}{\omega}\right) - \tilde{y}^2\right]\hat{v}(\tilde{y}) &= 0   
\end{align}
Letting 
\begin{align}
\mu = \frac{\sqrt{gH}}{\beta}\left(\frac{\omega^2}{gH} - k^2 - \beta\frac{k}{\omega}\right)
\end{align}
simplifies the equation to 
\begin{align}
\hat{v}'' + (\mu - \tilde{y}^2)\hat{v} = 0    
\end{align}
Further, if
\begin{align}
\hat{v} = \hat{w} \exp(-\frac{\tilde{y}^2}{2})
\end{align}
then 
\begin{align}
\hat{v}' = \hat{w}' \exp(-\frac{\tilde{y}^2}{2}) - \tilde{y}\hat{w} \exp(-\frac{\tilde{y}^2}{2})
\end{align}
and
\begin{align}
\hat{v}'' &= \hat{w}'' \exp(-\frac{\tilde{y}^2}{2}) - \tilde{y}\hat{w}' \exp(-\frac{\tilde{y}^2}{2}) - \hat{w} \exp(-\frac{\tilde{y}^2}{2}) \nonumber \\
&\quad - \tilde{y}\hat{w}' \exp(-\frac{\tilde{y}^2}{2}) + \tilde{y}^2\hat{w} \exp(-\frac{\tilde{y}^2}{2}) \nonumber \\
&= (\hat{w}'' - 2\tilde{y}\hat{w}' + (\tilde{y}^2-1)\hat{w})\exp(-\frac{\tilde{y}^2}{2})
\end{align}
So
\begin{align}
\hat{v}'' + (\mu - \tilde{y}^2)\hat{v} = 0
\end{align}
becomes
\begin{align}
(\hat{w}'' - 2\tilde{y}\hat{w}' + (\tilde{y}^2-1)\hat{w})\exp(-\frac{\tilde{y}^2}{2}) + (\mu - \tilde{y}^2)\hat{w} \exp(-\frac{\tilde{y}^2}{2}) &= 0 \nonumber \\
\hat{w}'' - 2\tilde{y}\hat{w}' + (\mu-1)\hat{w} &= 0
\end{align}
This is Hermite's equation in disguise, where we have $\mu = 2\nu + 1$, $\nu = 0, 1, 2, \ldots$ if the solutions have to be bounded. Hence the equatorial wave modes are given in terms of $\hat{w} = H_n(\tilde{y})$ and
\begin{align}
\hat{v}(\tilde{y}) &= \hat{v}_0H_n(\tilde{y})\exp(-\frac{\tilde{y}^2}{2})    
\end{align}
From this, $\hat{u}(\tilde{y})$ and $\hat{\Phi}(\tilde{y})$ can be found via any two of (\ref{eqn:linshallow1}), (\ref{eqn:linshallow2}) and (\ref{eqn:linshallow3}).
\end{solution}
\begin{figure}[ht!]
    \centering
    \includegraphics[height=0.75\textheight]{graphics/rog1687-fig-0003.png}
    \caption{\textit{Horizontal structures of selected zonally propagating wave solutions to the shallow water equations on an equatorial $\beta$ plane. (Kiladis et al., 2009 \cite{https://doi.org/10.1029/2008RG000266})}}
\end{figure}

\section{Python Programming}
Here we demonstrate how to compute a real inner product supplied by a symmetric bilinear form $B$ and carry out Gram-Schmidt Orthogonalization with respect to it. We use Example \ref{exmp:R3innerGS} as a test case. We first define a function to calculate the inner product given two input vectors and the matrix $B$:
\begin{lstlisting}
import numpy as np

def real_inner_prod(u, v, B):
    if np.all(B == B.T): # Check if symmetric
        return(u @ B @ v)
    else:
        print("Not symmetric bilinear form!")
        return(None)
\end{lstlisting}
Let's check it with that $B$ in Example \ref{exmp:R3innerGS} and $\vec{u} = (1,0,1)^T$, $\vec{v} = (0,2,-1)^T$. Then
\begin{lstlisting}
u = np.array([1., 0., 1.])
v = np.array([0., 2., -1.])
B = np.array([[2., 1., 0.],  
              [1., 2., 1.],
              [0., 1., 2.]])

print(real_inner_prod(u,v,B))    
\end{lstlisting}
gives \verb|2.0| which turns out to be correct. For convenience, we also define a function to compute norm, which is simply a wrapped version of \verb|real_inner_prod|:
\begin{lstlisting}
def norm(v, B):
    return(np.sqrt(real_inner_prod(v, v, B)))
\end{lstlisting}
Now comes the main part of executing the Gram-Schmidt procedure. The inner loop subtracts the parallel components of each previous vector from the current vector and the outer loop iterates the calculation for every input vector.
\begin{lstlisting}
def GS_inner_prod(vecs, B):
    """
    Gram-Schmidt Orthogonalization with respect to an inner product (finite-dimensional)
    vecs: A list containing the vectors
    B: The symmetric matrix for the inner product
    """
    n_vecs = len(vecs)
    for jj in np.arange(n_vecs):
        for ii in np.arange(jj):
            vecs[jj] -= real_inner_prod(vecs[jj], vecs[ii], B) / norm(vecs[ii], B)**2 * vecs[ii]
    return(vecs)
\end{lstlisting}
Trying this with Example \ref{exmp:R3innerGS}
\begin{lstlisting}
vecs = [np.array([1.,0.,0.]), 
        np.array([0.,1.,0.]), 
        np.array([0.,0.,1.])]
print(GS_inner_prod(vecs, B))
\end{lstlisting}
produces
\begin{lstlisting}
[array([1., 0., 0.]), array([-0.5,  1. ,  0. ]), array([ 0.33333333, -0.66666667,  1.        ])]    
\end{lstlisting}
which matches our answer in the example.

\section{Exercises}

\begin{Exercise}
Show that the set of all $n \times n$ (complex) matrices $\mathcal{V} = \mathcal{M}_{n \times n}(\mathbb{C})$ is a vector space and the definition $\langle A, B\rangle = \text{tr}(AB^*)$ satisfies the requirements of an inner product for this vector space. This form of inner product is better known as the \index{Frobenius Inner Product}\textit{Frobenius inner product}.
\end{Exercise}
\begin{Answer}
We will only argue for (1), (4), (5), and (6) in Definition \ref{defn:realvecspaceaxiom}. Given $A, B \in \mathcal{M}_{n \times n}(\mathbb{C})$ are square complex matrices of extent $n$, for (1), we know that the usual matrix addition $A + B$ just gives another matrix (of $\mathcal{M}_{n \times n}(\mathbb{C})$), so it is closed under vector addition. In (4) and (5), the zero vector is simply the zero matrix $[\textbf{0}]$ and the additive inverse of any $A \in \mathcal{M}_{n \times n}(\mathbb{C})$ is just $-A$. Finally for (6), given $c \in \mathbb{C}$, it is obvious that $cA \in \mathcal{M}_{n \times n}(\mathbb{C})$ as well, so it is closed under scalar multiplication. Meanwhile for the Frobenius inner product, we will justify all axioms in Definition \ref{defn:innerprod}. Note that
\begin{align*}
\langle A, B\rangle = \text{tr}(AB^*) = \sum_{i=1}^{n}\left(\sum_{k=1}^{n} A_{ik}\overline{B^T_{ki}}\right) = \sum_{i=1}^{n}\left(\sum_{k=1}^{n} A_{ik}\overline{B_{ik}}\right)
\end{align*}
then, for (1), $\overline{\langle B, A\rangle} = \smash{\overline{\sum_{i=1}^{n}\left(\sum_{k=1}^{n} B_{ik}\overline{A_{ik}}\right)}} = \smash{\sum_{i=1}^{n}\left(\sum_{k=1}^{n} A_{ik}\overline{B_{ik}}\right)} = \langle A, B\rangle$. For (2), $\langle A+C, B\rangle = \smash{\sum_{i=1}^{n}\left(\sum_{k=1}^{n} (A_{ik} + C_{ik})\overline{B_{ik}}\right)} = \smash{\sum_{i=1}^{n}\left(\sum_{k=1}^{n} A_{ik}\overline{B_{ik}}\right)} + \smash{\sum_{i=1}^{n}\left(\sum_{k=1}^{n} C_{ik}\overline{B_{ik}}\right)} = \langle A, B\rangle + \langle C, B\rangle$. For (3), it is simply $\langle aA, B\rangle = \smash{\sum_{i=1}^{n}\left(\sum_{k=1}^{n} aA_{ik}\overline{B_{ik}}\right)} = a\smash{\sum_{i=1}^{n}\left(\sum_{k=1}^{n} A_{ik}\overline{B_{ik}}\right)} = a\langle A,B\rangle$. The argument for (4) is the most important one: $\langle A, A\rangle = \text{tr}(AA^*) = \smash{\sum_{i=1}^{n}\left(\sum_{k=1}^{n} A_{ik}\overline{A_{ik}}\right)} = \sum_{i=1}^{n}\sum_{k=1}^{n} \abs{A_{ik}}^2$ which will be the sum of modulus of all entries in $A$. This will always be positive unless all entries are zero, i.e.\ $A$ is the zero matrix.
\end{Answer}

\begin{Exercise}
Show that $\vec{u} = (-1,0,2)^T$ and $\vec{v} = (1,-1,1)^T$ in $\mathbb{R}^3$ are orthogonal to each other if an inner product of
\begin{align*}
\langle\vec{u}, \vec{v}\rangle = \textbf{u}^TB\textbf{v}
\end{align*}
where 
\begin{align*}
B = 
\begin{bmatrix}
2 & 1 & 1 \\
1 & 2 & 1 \\
1 & 1 & 1
\end{bmatrix}
\end{align*}
is used. Also, find the norm $\norm{\vec{u}}$ and $\norm{\vec{v}}$ of both $\vec{u}$ and $\vec{v}$ with respect to this inner product.
\end{Exercise}
\begin{Answer}
The inner product of $\vec{u}$ and $\vec{v}$ is now
\begin{align*}
\textbf{u}^TB\textbf{v} = 
\begin{bmatrix}
-1 & 0 & 2 
\end{bmatrix}
\begin{bmatrix}
2 & 1 & 1 \\
1 & 2 & 1 \\
1 & 1 & 1
\end{bmatrix}
\begin{bmatrix}
1 \\
-1 \\
1
\end{bmatrix}
= 0
\end{align*}
so they are orthogonal with respect to this inner product. The corresponding norm $\norm{\vec{u}}$ is found from
\begin{align*}
\norm{\vec{u}}^2 = \textbf{u}^TB\textbf{u} = 
\begin{bmatrix}
-1 & 0 & 2 
\end{bmatrix}
\begin{bmatrix}
2 & 1 & 1 \\
1 & 2 & 1 \\
1 & 1 & 1
\end{bmatrix}
\begin{bmatrix}
-1 \\
0 \\
2
\end{bmatrix}
= 2
\end{align*}
so it is $\norm{\vec{u}} = \sqrt{2}$. Similarly, $\norm{\vec{v}} = \sqrt{3}$.
\end{Answer}

\begin{Exercise}
Let $\mathcal{V} = \mathbb{R}^3$. Show that
\begin{align*}
\langle\vec{u}, \vec{v}\rangle = \textbf{u}^TB\textbf{v}
\end{align*}
where 
\begin{align*}
B = 
\begin{bmatrix}
3 & 1 & -1 \\ 
1 & 3 & 0 \\ 
-1 & 0 & 1
\end{bmatrix}
\end{align*}
is a valid inner product for all $\vec{u}, \vec{v} \in \mathcal{V}$ and turns $\mathcal{V}$ into an inner product space. Hence derive an orthonormal basis for $\mathbb{R}^3$ with respect to this inner product using Gram-Schmidt Orthogonalization. 
\end{Exercise}
\begin{Answer}
For the first part, we check the eigenvalues of $B$ which are $\lambda \approx 0.5188, 2.3111, 4.1701$ all positive. We can apply the Gram-Schmidt process on $\{\vec{u}^{(1)}, \vec{u}^{(2)}, \vec{u}^{(3)}\} = \{(1,0,0)^T, (0,1,0)^T, (0,0,1)^T\}$, which will yield
\begin{align*}
\hat{v}^{(1)} &= \frac{\vec{u}^{(1)}}{\norm{\vec{u}^{(1)}}} = \left(\frac{1}{\sqrt{3}},0,0\right)^T \\
\vec{v}^{(2)} &= \vec{u}^{(2)} - \langle \vec{u}^{(2)}, \hat{v}^{(1)}\rangle\hat{v}^{(1)} \\
&= \begin{bmatrix}
0 \\
1 \\
0
\end{bmatrix}
-
\left(\begin{bmatrix}
0 & 1 & 0
\end{bmatrix}
\begin{bmatrix}
3 & 1 & -1 \\ 
1 & 3 & 0 \\ 
-1 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
\frac{1}{\sqrt{3}} \\
0 \\
0
\end{bmatrix}\right)
\begin{bmatrix}
\frac{1}{\sqrt{3}} \\
0 \\
0
\end{bmatrix}
=
\begin{bmatrix}
-\frac{1}{3} \\
1 \\
0
\end{bmatrix} \\
\hat{v}^{(2)} &= \frac{\vec{v}^{(2)}}{\norm{\vec{v}^{(2)}}} = \frac{\vec{v}^{(2)}}{\textbf{v}^{{(2)}T}B\textbf{v}^{(2)}}  \\
&=
\frac{\sqrt{3}}{2\sqrt{2}}
\begin{bmatrix}
-\frac{1}{3} \\
1 \\
0
\end{bmatrix}
=
\begin{bmatrix}
-\frac{1}{2\sqrt{6}}\\ 
\frac{\sqrt{3}}{2\sqrt{2}}\\ 
0
\end{bmatrix} \\
\vec{v}^{(3)} &= \vec{u}^{(3)} - \langle \vec{u}^{(3)}, \hat{v}^{(1)}\rangle\hat{v}^{(1)} - \langle \vec{u}^{(3)}, \hat{v}^{(2)}\rangle\hat{v}^{(2)} \\
&= \begin{bmatrix}
0 \\
0 \\
1
\end{bmatrix}
-
\left(\begin{bmatrix}
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
3 & 1 & -1 \\ 
1 & 3 & 0 \\ 
-1 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
\frac{1}{\sqrt{3}} \\
0 \\
0
\end{bmatrix}\right)
\begin{bmatrix}
\frac{1}{\sqrt{3}} \\
0 \\
0
\end{bmatrix} \\
& \quad -
\left(\begin{bmatrix}
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
3 & 1 & -1 \\ 
1 & 3 & 0 \\ 
-1 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
-\frac{1}{2\sqrt{6}}\\ 
\frac{\sqrt{3}}{2\sqrt{2}}\\ 
0
\end{bmatrix}\right)
\begin{bmatrix}
-\frac{1}{2\sqrt{6}}\\ 
\frac{\sqrt{3}}{2\sqrt{2}}\\ 
0
\end{bmatrix}
=
\begin{bmatrix}
\frac{3}{8}\\
-\frac{1}{8}\\ 
1
\end{bmatrix} \\
\hat{v}^{(3)} &= \frac{\vec{v}^{(3)}}{\norm{\vec{v}^{(3)}}} = 
\frac{2\sqrt{2}}{\sqrt{5}} \begin{bmatrix}
\frac{3}{8}\\
-\frac{1}{8}\\ 
1
\end{bmatrix}
=
\begin{bmatrix}
\frac{3}{2\sqrt{10}}\\ 
-\frac{1}{2\sqrt{10}}\\ 
\frac{2\sqrt{2}}{\sqrt{5}}
\end{bmatrix}
\end{align*}
(Other answers are acceptable as long as the derived vectors are orthonormal with respect to $B$)
\end{Answer}

\begin{Exercise}
\label{ex:triangular2}
Prove the \index{Triangle Inequality (for a General Inner Product)}\textit{Triangle Inequality}
\begin{align}
\norm{\vec{u} + \vec{v}} \leq \norm{\vec{u}} + \norm{\vec{v}}
\end{align}
as in Exercise \ref{ex:triangular} but now for any inner product.
\end{Exercise}
\begin{Answer}
\begin{align*}
\norm{\vec{u} + \vec{v}}^2 &= \langle (\vec{u} + \vec{v}), (\vec{u} + \vec{v}) \rangle \\
&= \norm{\vec{u}}^2 + 2\langle\vec{u}\cdot\vec{v}\rangle + \norm{\vec{v}}^2 \\
&\leq \norm{\vec{u}}^2 + 2\norm{\vec{u}}\norm{\vec{v}} + \norm{\vec{v}}^2 &\text{(Theorem \ref{thm:cauchyschinner})}\\
&= (\norm{\vec{u}} + \norm{\vec{v}})^2
\end{align*}
\end{Answer}

\begin{Exercise}
Show that $x^3$ and $\cos(2x)$ are orthogonal to each other in the $L^2[-\pi, \pi]$ space with respect to the inner product of Equation (\ref{eqn:integralinner}).
\end{Exercise}
\begin{Answer}
It is simply to note that $x^3$ is odd but $\cos(2x)$ is even, so the inner product integral
\begin{align*}
\int_{-\pi}^{\pi} x^3 \cos(2x) dx = 0
\end{align*}
is zero as the integrand is overall odd and the integration limits are symmetric.
\end{Answer}

\begin{Exercise}
Find the adjoint of $\mathcal{L}[f] = \frac{d}{dx}(x\frac{d}{dx}[f])$ with respect to the inner product of Equation (\ref{eqn:integralinner2})
\begin{align*}
\langle f,g \rangle = \int_a^b w(x) f(x) \overline{g(x)} dx    
\end{align*}
where $w(x) =$ (a) $1$, and (b) $e^{-x}$, with $a > 0$.
\end{Exercise}
\begin{Answer}
\begin{enumerate}[label=(\alph*)]
\item 
\begin{align*}
&\int_a^b \mathcal{L}[f(x)] \overline{g(x)} dx \\
=& \int_a^b \frac{d}{dx}\left(x\frac{d}{dx}[f(x)]\right) \overline{g(x)} dx \\
=& \left[x\frac{df(x)}{dx}\overline{g(x)}\right]_a^b - \int_a^b x\frac{df(x)}{dx}\frac{d\overline{g(x)}}{dx} dx \\
=& \left[x\frac{df(x)}{dx}\overline{g(x)}\right]_a^b - [xf(x)\frac{d\overline{g(x)}}{dx}]_a^b + \int_a^b f(x) \frac{d}{dx}\left(x\frac{d\overline{g(x)}}{dx}\right) dx \\
=& \, \text{Boundary Terms} + \int_a^b f(x) \overline{\frac{d}{dx}\left(x\frac{d}{dx}[g(x)]\right)} dx
\end{align*}
so $\mathcal{L}^\dag[g] = \frac{d}{dx}\left(x\frac{d}{dx}[g]\right) = \mathcal{L}[g]$ is self-adjoint.
\item In a similar vein, we note that the new adjoint is \\
$\mathcal{L}^\dag[f] = e^x\frac{d}{dx}\left(x\frac{d}{dx}[e^{-x}f]\right)$.
\end{enumerate}   
\end{Answer}

\begin{Exercise}
Show that given a real inner product induced by $\langle \vec{u},\vec{v} \rangle = \textbf{u}^TB\textbf{v}$ where
\begin{align*}
B = 
\begin{bmatrix}
1 & 1 \\
1 & 2
\end{bmatrix}
\end{align*}
The linear operator that has a matrix representation of
\begin{align*}
T = 
\begin{bmatrix}
2 & 1 \\
0 & 1
\end{bmatrix}
\end{align*}
is self-adjoint.
\end{Exercise}
\begin{Answer}
According to (\ref{eqn:adjointformatrix}) (adapted for real matrices), we simply check
\begin{align*}
T^\dag &\equiv B^{-1} [T]_\beta^T B \\
&= \begin{bmatrix}
1 & 1 \\
1 & 2
\end{bmatrix}^{-1}
\begin{bmatrix}
2 & 0 \\
1 & 1
\end{bmatrix}
\begin{bmatrix}
1 & 1 \\
1 & 2
\end{bmatrix} \\
&= \begin{bmatrix}
2 & 1 \\
0 & 1
\end{bmatrix} \equiv T
\end{align*}
\end{Answer}

\begin{Exercise}
Find the Fourier series of (a) $x^2$, (b) $x^3$, and (c) $4+\sin(3x)+2\cos(8x)$ over $[-\pi, \pi]$.
\end{Exercise}
\begin{Answer}
\begin{enumerate}[label=(\alph*)]
\item Note that $x^2$ is even, so only the cosine components remain. Then we simply compute $a_m$ as in (\ref{eqn:fouriera}) with integration by parts:
\begin{align*}
a_m &= \frac{1}{\pi} \int_{-\pi}^{\pi} x^2\cos(mx) dx \\
&= \left[\frac{1}{m\pi}x^2\sin(mx)\right]_{-\pi}^{\pi} - \frac{2}{m\pi} \int_{-\pi}^{\pi} x \sin(mx) dx \\
&= (0) - \frac{2}{m\pi} \left[-\frac{1}{m} x \cos(mx)\right]_{-\pi}^{\pi} + \frac{2}{m^2\pi} \int_{-\pi}^{\pi} (-\cos(mx)) dx \\
&= \frac{2}{m^2\pi} [\pi (-1)^m - (-\pi) (-1)^m] - (0) = \frac{4(-1)^m}{m^2}
\end{align*}
Meanwhile, the constant term is just
\begin{align*}
\frac{1}{2\pi} \int_{-\pi}^{\pi} x^2 dx &= \frac{1}{2\pi}\left[\frac{x^3}{3}\right]_{-\pi}^{\pi} \\\
&= \frac{1}{2\pi}\left[\frac{\pi^3}{3} - \frac{(-\pi)^3}{3}\right] = \frac{\pi^2}{3}
\end{align*}
Therefore, the required Fourier series is
\begin{align*}
x^2 = \frac{\pi^2}{3}+\sum_{m=1}^{\infty} \frac{4(-1)^m}{m^2} \cos(mx)
\end{align*}
\item Similarly, $x^3$ is odd and we only need to calculate $b_n$ as in (\ref{eqn:fourierb}):
\begin{align*}
b_n &= \frac{1}{\pi} \int_{-\pi}^{\pi} x^3\sin(nx) dx \\
&= \left[-\frac{1}{n\pi}x^3\cos(nx)\right]_{-\pi}^{\pi} + \frac{3}{n\pi}\int_{-\pi}^{\pi} x^2\cos(nx) dx \\
&= -\frac{1}{n\pi}\left[\pi^3 (-1)^n - (-\pi)^3 (-1)^n\right] + \frac{3}{n^2\pi}[x^2 \sin(nx)]_{-\pi}^{\pi} \\
&\quad - \frac{6}{n^2\pi}\int_{-\pi}^{\pi} x \sin (nx) dx \\
&= -\frac{2\pi^2(-1)^n}{n} + (0) + \frac{6}{n^3\pi} [x \cos (nx)]_{-\pi}^{\pi} - \frac{6}{n^3\pi} \int_{-\pi}^{\pi} \cos(nx) dx \\
&= -\frac{2\pi^2(-1)^n}{n} + \frac{6}{n^3\pi}[\pi (-1)^n - (-\pi) (-1)^n] - (0) \\
&= -\frac{2\pi^2(-1)^n}{n} + \frac{12(-1)^n}{n^3}
\end{align*}
So the required Fourier series is
\begin{align*}
x^3 = \sum_{m=1}^{\infty} \left(-\frac{2\pi^2(-1)^n}{n} + \frac{12(-1)^n}{n^3}\right) \sin(nx)
\end{align*}
\item This is a trick question where the given form is already a Fourier series so the answer is just $4+\sin(3x)+2\cos(8x)$ itself.
\end{enumerate}
\end{Answer}

\begin{Exercise}
Convert the \index{Legendre's Equation}\textit{Legendre's Equation}
\begin{align}
(1-x^2) y'' - 2xy' + \lambda y = 0
\end{align}
into the Sturm-Liouville form, what should be the natural interval such that the corresponding Sturm-Liouville operator is Hermitian? Then, show that when $\lambda = n(n+1)$, $n$ is a non-negative integer, the series solution truncates and produces the \index{Legendre Polynomials}\textit{Legendre polynomials} as its eigenfunctions. Find the first five of them. Alternatively, apply the Gram-Schmidt procedure over the standard polynomial basis over the natural interval to come up with the same set of Legendre polynomials.
\end{Exercise}
\begin{Answer}
The integrating factor needed is
\begin{align*}
F(x) &= \exp(\int \frac{R(x) - P'(x)}{P(x)} dx) \\
&= \exp(\int \frac{-2x-(1-x^2)'}{(1-x^2)} dx) \\
&= \exp(\int \frac{-2x-(-2x)}{1-x^2} dx) = \exp(\int 0 dx) = 1
\end{align*}
So the original equation can already be transformed into the Sturm-Liouville form as
\begin{align*}
((1-x^2) y')' + \lambda y = 0    
\end{align*}
The $(1-x^2)$ factor inside the derivative term indicates that the natural interval is $[-1, 1]$ so that the operator $\mathcal{L} = \frac{d}{dx}((1-x^2)\frac{d}{dx})$ is Hermitian. By the same method of series solution as introduced in the chapter, we let
\begin{align*}
y &= a_0 + a_1x + a_2x^2 + \cdots = \sum_{n=0}^{\infty} a_n x^n \\
y' &= \sum_{n=1}^{\infty} n a_nx^{n-1} \\
y'' &= \sum_{n=2}^{\infty} n(n-1) a_nx^{n-2}
\end{align*}
and substitution into the Legendre's Equation gives
\begin{align*}
(1-x^2) \sum_{n=2}^{\infty} n(n-1) a_nx^{n-2} - 2x\sum_{n=1}^{\infty} n a_nx^{n-1} + \lambda \sum_{n=0}^{\infty} a_n x^n &= 0 \\
\sum_{n=2}^{\infty} n(n-1) a_nx^{n-2} - \sum_{n=2}^{\infty} n(n-1) a_nx^{n} - 2\sum_{n=1}^{\infty} n a_nx^{n} + \lambda \sum_{n=0}^{\infty} a_n x^n &= 0 \\
\sum_{n=0}^{\infty} (n+2)(n+1) a_{n+2}x^{n} - \sum_{n=2}^{\infty} n(n-1) a_nx^{n} - 2\sum_{n=1}^{\infty} n a_nx^{n} + \lambda \sum_{n=0}^{\infty} a_n x^n &= 0
\end{align*}
So the recurrence relation is
\begin{align*}
(n+2)(n+1) a_{n+2} - n(n-1) a_n - 2n a_n + \lambda a_n &= 0 \\
(n+2)(n+1) a_{n+2} - (n(n+1)-\lambda) a_n &= 0 \\
a_{n+2} &= \frac{n(n+1)-\lambda}{(n+2)(n+1)}
\end{align*}
which implies that there are two polynomials solutions for the odd/even indices respectively when $\lambda = n(n+1)$. We only explicitly show the reasoning to construct the fifth Legendre polynomial: let $\lambda = 4 \cdot 5$ that corresponds to $n=4$, then
\begin{align*}
a_{2} &= \frac{0(0+1)-(4)(5)}{(0+2)(0+1)}a_0 = -10a_0 \\
a_{4} &= \frac{2(2+1)-(4)(5)}{(2+2)(2+1)}a_0 = -\frac{7}{6}a_2 = \frac{70}{6}a_0
\end{align*}
So we have
\begin{align*}
a_0\left(\frac{70}{6}x^4 - 10x^2 + 1\right)
\end{align*}
which after rescaling (the sum of coefficients has to be $1$), gives the fifth Legendre polynomial at $n=4$ as
\begin{align*}
P_4(x) &= \frac{1}{8}(35x^4 - 30x^2 + 3)
\end{align*}
The first four Legendre polynomials are given below for reference.
\begin{align*}
P_0(x) &= 1 \\
P_1(x) &= x \\
P_2(x) &= \frac{1}{2}(3x^2 - 1) \\
P_3(x) &= \frac{1}{2}(5x^3 - 3x)
\end{align*}
To use the Gram-Schmidt method, consider the inner product below, applied on the standard polynomials
\begin{align*}
\langle \varphi_1, \varphi_2 \rangle = \int_{-1}^{1} \varphi_1\varphi_2 dx
\end{align*}
where the integration range is the natural interval $[-1,1]$ and the weighting is just $w = 1$.
\end{Answer}