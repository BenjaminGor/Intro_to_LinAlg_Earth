\chapter{Complex Vectors/Matrices and Block Matrices}
\label{chap:complex}

In this chapter, we will take a detour to talk about two auxiliary topics. The first one is the generalization of vectors and matrices to having complex numbers as entries. Eventually, we will mention about \textit{complex vector spaces}, and compare them to real vector spaces that we just learnt in the previous chapters. The second one is about the \textit{block form} of a matrix (or simply referred to as a \textit{block matrix}) that is composed of smaller \textit{submatrices} as the building blocks. Writing a matrix in block form enables efficient manipulation for many situations that we will encounter in the remaining parts of this book.

\section{Definition and Operations of Complex Numbers}
\label{section:complexno}

\subsection{Basic Structure of Complex Numbers} 

The idea of complex numbers initially came from some algebra problems that led to the square root of a negative quantity, which was undefined back in the day. Later, mathematicians addressed this issue by introducing the \index{Imaginary Number}\keywordhl{imaginary number} $i = \sqrt{-1}$, and $i^2 = -1$. For any positive number $b$, we have $\sqrt{-b^2} = \sqrt{b^2}\sqrt{-1} = bi$. \index{Complex Number}\keywordhl{Complex numbers} are then quantities in the form of $a + bi$, where $a$ and $b$ themselves are real. Here $a$ and $b$ are called the \index{Real Part}\keywordhl{real} and \index{Imaginary Part}\keywordhl{imaginary part} respectively. As a small example of how complex numbers arise, note that the solutions to the quadratic equation $(3x+2)^2 = -1$, are $-\frac{2}{3} \pm \frac{1}{3}i$.
\begin{defn}[Complex Number]
Complex numbers are scalars in the form of $z = a + bi$, where $a$ and $b$ are some real numbers. Their real and imaginary parts are denoted by $\Re{z} = a$ and $\Im{z} = b$.
\end{defn}
We also need to consider when two complex numbers are equal. This happens when their real parts, as well as imaginary parts, are equal to each other respectively.
\begin{proper}
Two complex numbers $z_1 = a + bi$, and $z_2 = c + di$, where $a, b, c, d$ are real numbers, are equal if and only if $\Re{z_1} = a = c = \Re{z_2}$ and $\Im{z_1} = b = d = \Im{z_2}$.
\end{proper}
For every complex number, there exists another corresponding complex number known as the \index{Complex Conjugate}\keywordhl{(complex) conjugate} associated with it, formed by flipping the sign of its imaginary part.
\begin{defn}[Complex Conjugate]
For a complex number $z = a + bi$, its complex conjugate is defined as $\overline{z} = a - bi$.
\end{defn}
For example, the conjugate of $2-5i$ is $2+5i$.

\subsection{Complex Number Operations}
Below are some rules about usual operations on two complex numbers.
\subsubsection{Addition and Subtraction}
Addition and subtraction between two complex numbers are carried out over the real parts and the imaginary parts separately.
\begin{defn}
For two complex numbers $z_1 = a + bi$, and $z_2 = c + di$, we have
\begin{align}
z_1 \pm z_2 &= (a + bi) \pm (c + di) \nonumber \\
&= (a \pm c) + (b \pm d)i \nonumber \\
&= (\Re{z_1} \pm \Re{z_2}) + (\Im{z_1} \pm \Im{z_2})i    
\end{align}
\end{defn}
For instance, adding $1 + 3i$ to $2 - 4i$ results in $(1+2) + (3-4)i = 3 - i$.

\subsubsection{Multiplication and Division}
Multiplication of two complex numbers simply works like the usual distributive law where we pretend that $i$ is a variable.
\begin{defn}
Given two complex numbers $a + bi$, and $c + di$, their product is
\begin{align}
(a + bi)(c + di) &= a(c + di) + bi(c + di) \nonumber\\
&= ac + adi + bci + bdi^2 \nonumber \\
&= (ac - bd) + (ad + bc)i & (i^2 = -1)
\end{align}
\end{defn}

\begin{exmp}
Evaluate $(1+2i)(3-4i)$.
\end{exmp}
\begin{solution}
\begin{align*}
(1+2i)(3-4i) &= ((1)(3) - (2)(-4)) + ((1)(-4) + (2)(3))i \\
&= 11 + 2i \qedhere
\end{align*}
\end{solution}

Dividing something by a complex number $a+bi$ can be viewed as multiplication by its complex conjugate $a-bi$, as
\begin{align}
\frac{1}{a+bi} &= \frac{1}{a+bi}\frac{a-bi}{a-bi} \nonumber \\
&= \frac{a-bi}{a^2 - (-b^2) - abi + bai} \nonumber \\
&= \frac{a-bi}{a^2 + b^2}
\end{align}
with an additional factor of $\frac{1}{a^2+b^2}$. It is interesting that this $a^2+b^2$ term coming from multiplying the complex number by its conjugate over the denominator looks like the square of hypotenuse as in the \textit{Pythagoras' Theorem}. Later on, we will see more when we discuss the geometric meaning of complex numbers.

\begin{exmp}
Compute $\frac{1+4i}{2+3i}$.
\end{exmp}
\begin{solution}
Following the idea outlined above, we have
\begin{align*}
\frac{1+4i}{2+3i} &= \frac{1+4i}{2+3i}\frac{2-3i}{2-3i} \\
&= \frac{(1+4i)(2-3i)}{2^2+3^2} \\
&= \frac{((1)(2) - (4)(-3)) + ((1)(-3) + (4)(2))i}{13} = \frac{14}{13}+\frac{5}{13}i
\end{align*}
\end{solution}

\subsection{Geometric Meaning of Complex Numbers}
\label{section:complexnogeo}

\begin{figure}[ht!]
\centering
\begin{tikzpicture}[scale=0.5]
\draw[->] (-5,0)--(5,0) node[right](x){Real Axis};
\draw[->] (0,-5)--(0,5) node[above]{Imaginary Axis};
\draw[blue,-stealth] (0,0)--(3,4) node[anchor=south](z){$z = 3+4i$};
\draw[gray,dashed] (3,4)--(3,0) node[below]{$\Re{z} = 3$};
\draw[gray,dashed] (3,4)--(0,4) node[left]{$\Im{z} = 4$};
\pic[draw, ->, "$\theta$",angle eccentricity=1.5] {angle = x--0--z};
\node[below left]{$O$}; 
\end{tikzpicture}
\caption{\textit{A complex number $z = 3+4i$ represented in the complex plane.}}
\label{fig:argand}
\end{figure}
A complex number can be visualized as a two-dimensional vector, in the so-called \index{Complex Plane}\keywordhl{complex plane} (or sometimes referred to as the \index{Argand Plane}\keywordhl{Argand plane}), where the $x$-axis represents the real part and the $y$-axis represents the imaginary part. These two axes are referred to as the \index{Real Axis}\keywordhl{real axis} and \index{Imaginary Axis}\keywordhl{imaginary axis} respectively.\par

It is obvious that the length of such a vector is 
\begin{align}
\abs{z} = \sqrt{\Re{z}^2 + \Im{z}^2} \label{eqn:modulus}
\end{align}
which is called the \index{Modulus}\keywordhl{modulus} of the corresponding complex number. In the diagram (Figure \ref{fig:argand}) above, the modulus of $z$ is easily seen to be $\abs{z} = 5$. \par
The angle between the real axis and the complex number is called the \index{Argument}\keywordhl{argument}, shown as
\begin{align}
\theta = \arctan(\Im{z}/\Re{z}) \label{eqn:argument} 
\end{align}
in the same figure. Since its complex conjugate $\overline{z}$ has the sign of the imaginary part flipped while the real part remains the same, the argument of the complex conjugate is simply the negative of that of the original complex number $z$. Also, the modulus will be unchanged. \par
Moreover, from elementary trigonometry, we know that $\Re{z} = \abs{z} \cos{\theta}$ and $\Im{z} = \abs{z} \sin{\theta}$. Hence $z$ can be represented as $z = \Re{z} + i\Im{z} = \abs{z} (\cos \theta + i \sin \theta)$. We also have the famous \index{Euler's Formula}\keywordhl{Euler's Formula}, relating the geometry of any complex number with an exponential raised to an imaginary power.
\begin{defn}[Euler's Formula]
\label{defn:Euler}
An exponential raised to an imaginary power is a complex number such that
\begin{align}
e^{i \theta} = \cos \theta + i \sin \theta \label{eqn:Euler}
\end{align}
where $\theta$ is taken to be real.
\end{defn}
Hence $z$ can be further written as $z = \abs{z} e^{i \theta}$, and $\overline{z} = \Re{z} - i\Im{z} = \abs{z} (\cos \theta - i \sin \theta) = \abs{z} (\cos (-\theta) + i \sin (-\theta)) = \abs{z} e^{-i \theta}$. Conversely, the quantity $e^{i \theta}$ can be regarded as a complex number that has a modulus of $1$ and an argument of $\theta$. Additionally, this provides formulae to express sines and cosines with complex exponentials.
\begin{proper}
\label{proper:sincoscomplex}
For any $\theta$ which is confined to be real,
\begin{subequations}
\label{eqn:sincoscomplex}
\begin{align}
\cos \theta &= \frac{e^{i\theta} + e^{-i\theta}}{2} \label{eqn:sincoscomplexa}\\
\sin \theta &= \frac{e^{i\theta} - e^{-i\theta}}{2i}
\end{align}
\end{subequations}
\end{proper}
\begin{proof}
By Definition \ref{defn:Euler},
\begin{align*}
\frac{e^{i\theta} + e^{-i\theta}}{2} &= \frac{1}{2}((\cos \theta + i \sin \theta) + (\cos (-\theta) + i \sin (-\theta))) \\
&= \frac{1}{2}((\cos \theta + i \sin \theta) + (\cos \theta - i \sin \theta)) \\
&= \cos \theta
\end{align*}  
The derivation for $\sin \theta$ is left as an exercise.
\end{proof}
Now we can go back to investigate complex multiplication and division. Multiplication of a complex number $z_1$ by another complex number $z_2$, can be viewed as $z_1z_2 = (\abs{z_1}e^{i \theta_1}) (\abs{z_2} e^{i \theta_2}) = \abs{z_1}\abs{z_2}e^{i (\theta_1+\theta_2)}$.\footnote{We take it for granted that $e^{i \theta_1}e^{i \theta_2} = e^{i (\theta_1+\theta_2)}$.} This can be interpreted as, starting with the complex number $z_1 = \abs{z_1}e^{i \theta_1}$ on the complex plane, rotating it anti-clockwise (i.e.\ in the positive direction) by an angle of $\theta_2$, and scaling its modulus by a factor of $\abs{z_2}$. \par
Similarly, division of $z_1$ by $z_2$, is $z_1/z_2 = (\abs{z_1}/\abs{z_2})e^{i (\theta_1-\theta_2)}$. Notice that for a fraction like $1/z = 1/(a+bi)$, it can be rewritten as
\begin{align*}
\frac{1}{z} = \frac{1}{\abs{z}e^{i \theta}} = \frac{1}{\abs{z}}\frac{e^{-i \theta}}{e^{i \theta}e^{-i \theta}} = \frac{1}{\abs{z}}\frac{e^{-i \theta}}{e^{i (\theta-\theta)}} = \frac{1}{\abs{z}}\frac{e^{-i \theta}}{e^0} &= \frac{1}{\abs{z}} e^{-i \theta} \\
&= \frac{1}{\abs{z}^2} (\abs{z} e^{-i \theta}) \\
&= \frac{1}{\abs{z}^2} \overline{z}
\end{align*}
which is consistent with the discussion about complex division in the last subsection, where $\abs{z}^2 = a^2 + b^2$ arises in the denominator. In addition, we can observe that $\abs{z}^2 = z\overline{z}$. This is not a coincidence, as
\begin{align}
z\overline{z} &= \abs{z} e^{i \theta} \abs{z} e^{-i \theta} \nonumber \\
&= \abs{z}^2 e^{i(\theta-\theta)} = \abs{z}^2e^{0} = \abs{z}^2 \label{eqn:zzbar}
\end{align}
Geometrically, we can think of it as starting with $1$ along the real axis in the complex plane, then we scale it by $\abs{z}$ and rotate it by $\theta$, and finally scale it again by $\abs{z}$ but rotate it by $-\theta$, the same angle but in opposite direction. The results will be a real number $\abs{z}^2$ (the length of $z$ squared), since the two opposite rotations cancel out each other. \par
Below are some properties of modulus and complex conjugate to be remembered.
\begin{proper}
\label{proper:complexnum}
For two complex numbers $z_1$ and $z_2$, we have
\begin{enumerate}[label=(\alph*)]
\item $\overline{z_1 \pm z_2} = \overline{z_1} \pm \overline{z_2}$;
\item $\overline{z_1z_2} = \overline{z_1}\,\overline{z_2}$;
\item $\overline{z_1/z_2} = \overline{z_1}/\overline{z_2}$;
\item $\overline{\overline{z}} = z$;
\item $\overline{\overline{z_1}z_2} = z_1\overline{z_2}$;
\item $\abs{\overline{z}} = \abs{z}$;
\item $\abs{z_1z_2} = \abs{z_1}\abs{z_2}$;
\item $\abs{z_1/z_2} = \abs{z_1}/\abs{z_2}$.
\end{enumerate}
\end{proper}
Another very useful result is the \index{De Moivre's Formula}\keywordhl{De Moivre's Formula} that builds up on the Euler's formula, expressing $e^{i \theta}$ raised to an integer power $n$.
\begin{thm}[De Moivre's Formula]
Given $n$ as an integer, then
\begin{subequations}
\begin{align}
(e^{i \theta})^n &= e^{i (n\theta)} \\
(\cos\theta + i \sin\theta)^n &= \cos(n\theta) + i \sin(n\theta)
\end{align}
\end{subequations}
\end{thm}

\section{Complex Vectors and Complex Matrices}

Our discussion about vectors and matrices in previous chapters is limited to those with real entries. However, we can extend the ideas to include complex entries. A complex vector is simply a vector that have complex numbers as components. An $n$-dimensional complex vector can be somehow viewed as a real vector that is $2n$-dimensional, as each complex entry can be expressed in two parts, real and imaginary. This equivalence will be further clarified at the end of this section. A complex matrix is similarly a matrix with complex entries, or from another perspective, formed by complex column vectors.

\subsection{Operations and Properties of Complex Vectors}
Addition and subtraction for complex vectors are the same as the real counterpart, carried out component-wise. Multiplication by a scalar is also similar, applied to all components. However, the form of complex dot product is slightly different from the real dot product, as defined below.
\begin{defn}[Complex Dot Product]
\label{defn:complexdotproduct}
The dot product of two complex vectors $\vec{u}$ and $\vec{v}$ is computed as the sum of products between each pair of components, but additionally with the conjugate operation applied on the second complex vector beforehand.
\begin{align}
\vec{u} \cdot \vec{v} &= \textbf{u}^T \overline{\textbf{v}} \nonumber \\
&= u_1\overline{v_1} + u_2\overline{v_2} + \cdots + u_n\overline{v_n} = \sum_{k=1}^{n} u_k\overline{v_k}
\end{align}
The bar on $\overline{\textbf{v}}$ means carrying out conjugate on every entry of $\textbf{v}$. If $\textbf{v} = \Re{\textbf{v}} + i\Im{\textbf{v}}$, where $\Re{\textbf{v}}$ and $\Im{\textbf{v}}$ are the vectors consisting of the real/imaginary parts of every entry in $\textbf{v}$, then $\overline{\textbf{v}} = \Re{\textbf{v}} - i\Im{\textbf{v}}$.
\end{defn}
The Euclidean norm, or length of a complex vector, is defined in a similar fashion.
\begin{defn}
The length $\norm{\vec{v}}$ of a complex vector $\vec{v}$ is calculated by
\begin{align}
\norm{\vec{v}} &= \sqrt{\vec{v} \cdot \vec{v}} = \sqrt{\textbf{v}^T \overline{\textbf{v}}} \nonumber \\
&= \sqrt{v_1\overline{v_1} + v_2\overline{v_2} + \cdots + v_n\overline{v_n}} \nonumber\\
&= \sqrt{\abs{v_1}^2 + \abs{v_2}^2 + \cdots + \abs{v_n}^2} & \text{(by (\ref{eqn:zzbar}))} \nonumber \\
&= \sqrt{\sum_{k=1}^{n} \abs{v_k}^2}
\end{align}
\end{defn}
Properties of complex dot product hence also vary slightly from its real counterpart, Properties \ref{proper:dotproper}.
\begin{proper}
\label{proper:complexdot}
For two complex vectors $\vec{u}$ and $\vec{v}$, we have
\begin{align*}
\vec{u} \cdot \vec{v} &= \textcolor{red}{\overline{\vec{v} \cdot \vec{u}}} &\text{Conjugate-symmetric Property} \\
\vec{u} \cdot (\vec{v} \pm \vec{w}) &= \vec{u} \cdot \vec{v} \pm \vec{u} \cdot \vec{w} &\text{Distributive Property} \\
(\vec{u} \pm \vec{v}) \cdot \vec{w} &= \vec{u} \cdot \vec{w} \pm \vec{v} \cdot \vec{w} &\text{Distributive Property} \\
(a\vec{u}) \cdot (b\vec{v}) &= a\textcolor{red}{\bar{b}}(\vec{u} \cdot \vec{v}) &\text{where $a$, $b$ are some complex constants}    
\end{align*}
\end{proper}
There is no complex analog for cross product.
\begin{exmp}
Show that the conjugate-symmetric property in Properties \ref{proper:complexdot} holds for $\vec{u} = (1+2i, 3+i)^T$, $\vec{v} = (2-5i, 1+4i)^T$.
\end{exmp}
\begin{solution}
\begin{align*}
\vec{u} \cdot \vec{v} &= (1+2i)(\overline{2-5i}) + (3+i)(\overline{1+4i}) \\
&= (1+2i)(2+5i) + (3+i)(1-4i) \\
&= (-8+9i) + (7-11i) \\
&= -1-2i 
\end{align*}
\begin{align*}
\vec{v} \cdot \vec{u} &= (2-5i)(\overline{1+2i}) + (1+4i)(\overline{3+i}) \\
&= (2-5i)(1-2i) + (1+4i)(3-i) \\
&= (-8-9i) + (7+11i) \\
&= -1+2i 
\end{align*}
Hence $\vec{u} \cdot \vec{v} = \overline{\vec{v} \cdot \vec{u}}$.
\end{solution}

$\blacktriangleright$ Short Exercise: Find the norm $\norm{\vec{u}}$ and $\norm{\vec{v}}$ respectively.\footnote{$\norm{\vec{u}} = \sqrt{(1+2i)(1-2i) + (3+i)(3-i)} = \sqrt{(1^2 + 2^2) + (3^2 + 1^2)} = \sqrt{15}$. Similarly, $\norm{\vec{v}} = \sqrt{46}$.}

\subsection{Operations and Properties of Complex Matrices}
Matrix multiplication between two complex matrices is carried out in the same way as we have been always doing, according to Definition \ref{defn:matprod}. However, due to the difference in the definition of dot product for real and complex vectors, we can no longer claim like in the discussion below Definition \ref{defn:dotreal} that the entries resulting from a complex matrix product are complex vector dot products between the corresponding rows and columns. To make the statement work again, a minor modification is needed, as we will see soon. 

\subsubsection{Conjugate Transpose}
Transpose can be similarly defined for complex matrices. However, there exists a more useful operation that combines transpose and conjugate.
\begin{defn}[Conjugate Transpose]
\label{defn:conjutrans}
The \index{Conjugate Transpose}\keywordhl{conjugate transpose} of a matrix $A$, denoted as $A^* = \overline{A^T}$, has its entries as $A^*_{pq} = \overline{A}_{qp}$, where $\overline{A}$ is the conjugate of the matrix $A$ produced by changing every entry in $A$ to its complex conjugate. Sometimes $A^*$ is called the \textit{adjoint} or \index{Hermitian Transpose}\keywordhl{Hermitian transpose} of $A$, and alternatively denoted as $A^H$. 
\end{defn}
It means that conjugate transpose is done by conjugating all entries of the matrix and flipping them about its main diagonal. A \index{Hermitian Matrix}\keywordhl{Hermitian matrix} is a complex matrix whose conjugate transpose equals to itself.
\begin{defn}
\label{defn:Hermitian}
A complex square matrix $A$ is called Hermitian if $A^* = A$.
\end{defn}
Note that the diagonal entries of a Hermitian matrix are always real. Properties of conjugate transpose are alike to those for real transpose stated in Properties \ref{proper:transp}. Furthermore, for complex dot product, there is also something parallel to the second half of Properties \ref{proper:dotproper}.
\begin{proper}
\label{proper:complexmat}
For two complex matrices $A$ and $B$, we have
\begin{enumerate}
\item $(cA)^* = \overline{c}A^*$, where $c$ is any complex scalar;
\item $(A^*)^* = A$;
\item $(A \pm B)^* = A^* \pm B^*$, if $A$ and $B$ have the same shape;
\item $(AB)^* = B^*A^*$, if $A$ and $B$ have compatible shapes.
\end{enumerate}
\end{proper}
\begin{proper}
\label{proper:complexdotherm}
For two complex vectors $\vec{u}$ and $\vec{v}$ and a complex matrix $A$, we have 
\begin{align*}
\vec{u} \cdot (A\vec{v}) &= \textbf{u}^T\overline{(A\textbf{v})} = (\overline{A^T}\textbf{u})^T\overline{\textbf{v}} = (A^*\vec{u}) \cdot \vec{v} \\
(A\vec{u}) \cdot \vec{v} &= (A\textbf{u})^T\overline{\textbf{v}} = \textbf{u}^T(A^T\overline{\textbf{v}}) = \vec{u} \cdot (A^*\vec{v})
\end{align*}
where Properties \ref{proper:transp} and Definitions \ref{defn:complexdotproduct}, \ref{defn:conjutrans} are used.
\end{proper}

With the complex conjugate of a matrix defined in passing, we can now say that the complex vector dot products between each of the row and column vectors in a matrix $A$ and another matrix $B$ respectively, are encoded in the entries of the complex matrix product $A\overline{B}$ where a conjugate is applied on the second matrix.
\begin{exmp}
For two complex matrices
\begin{align*}
& A =
\begin{bmatrix}
1 & i \\
-i & 0
\end{bmatrix} 
& B =
\begin{bmatrix}
1 & 2+3i \\
1+i & 1-i
\end{bmatrix}
\end{align*}
Verify that $(AB)^* = B^*A^*$.
\end{exmp}
\begin{solution}
\begin{align*}
A^* &=
\begin{bmatrix}
1 & i \\
-i & 0
\end{bmatrix} \\
B^* &=
\begin{bmatrix}
1 & 1-i \\
2-3i & 1+i
\end{bmatrix} \\
B^*A^* &= 
\begin{bmatrix}
1 & 1-i \\
2-3i & 1+i
\end{bmatrix} 
\begin{bmatrix}
1 & i \\
-i & 0
\end{bmatrix} \\
&=
\begin{bmatrix}
(1)(1) + (1-i)(-i) & (1)(i) + (1-i)(0)  \\
(2-3i)(1) + (1+i)(-i) & (2-3i)(i) + (1+i)(0)
\end{bmatrix} \\
&=
\begin{bmatrix}
-i & i \\
3-4i & 3+2i
\end{bmatrix} 
\end{align*}
\begin{align*}
AB &= 
\begin{bmatrix}
1 & i \\
-i & 0
\end{bmatrix} 
\begin{bmatrix}
1 & 2+3i \\
1+i & 1-i
\end{bmatrix} \\
&= 
\begin{bmatrix}
(1)(1)+(i)(1+i) & (1)(2+3i) + (i)(1-i) \\
(-i)(1)+(0)(1+i) & (-i)(2+3i) + (0)(1-i)
\end{bmatrix} \\
&= 
\begin{bmatrix}
i & 3+4i \\
-i & 3-2i
\end{bmatrix} \\
(AB)^* &= 
\begin{bmatrix}
-i & i \\
3-4i & 3+2i
\end{bmatrix} 
\end{align*}
\end{solution}

\subsubsection{Determinants and Inverses for complex matrices}
Complex matrices also have determinants and inverses, and are calculated in the exact same ways outlined in Sections \ref{section:det} and \ref{section:inv}. We provide a few examples here.

\begin{exmp}
Calculate the determinant for
\begin{align*}
A = 
\begin{bmatrix}
1-i & 3 & 2 \\
1+i & 0 & i \\
2 & -2i & 1
\end{bmatrix}
\end{align*}
\end{exmp}
\begin{solution}
We apply cofactor expansion along the middle row in the way outlined in Properties \ref{proper:cofactorex}, and the result is
\begin{align*}
\det(A) &= -(1+i)
\begin{vmatrix}
3 & 2 \\
-2i & 1
\end{vmatrix}
+ (0)
\begin{vmatrix}
1-i & 2 \\
2 & 1
\end{vmatrix}
- (i)
\begin{vmatrix}
1-i & 3 \\
2 & -2i
\end{vmatrix} \\
&= -(1+i)(3+4i) - (i)(-8-2i) \\
&= -1 + i
\end{align*}  
\end{solution}

\begin{exmp}
Find the inverse of the matrix $A$ in the last example.
\end{exmp}

\begin{solution}
The computation of the inverse follows Properties \ref{proper:invadj}. First, we note that
\begin{align*}
\frac{1}{\det(A)} &= \frac{1}{-1+i} \\
&= \frac{1}{-1+i} \frac{-1-i}{-1-i} \\
&= \frac{-1-i}{1+1} = -\frac{1+i}{2}
\end{align*}
Then, we proceed to compute the cofactor matrix for $A$, which is
\begin{align*}
C &=
\begin{bmatrix*}[r]
\begin{vmatrix}
0 & i \\
-2i & 1
\end{vmatrix} &
-\begin{vmatrix}
1+i & i \\
2 & 1
\end{vmatrix} &
\begin{vmatrix}
1+i & 0 \\
2 & -2i
\end{vmatrix} \\[10pt]
-\begin{vmatrix}
3 & 2 \\
-2i & 1
\end{vmatrix} &
\begin{vmatrix}
1-i & 2 \\
2 & 1
\end{vmatrix} &
-\begin{vmatrix}
1-i & 3 \\
2 & -2i
\end{vmatrix} \\[10pt]
\begin{vmatrix}
3 & 2 \\
0 & i
\end{vmatrix} &
-\begin{vmatrix}
1-i & 2 \\
1+i & i
\end{vmatrix} &
\begin{vmatrix}
1-i & 3 \\
1+i & 0
\end{vmatrix}
\end{bmatrix*} \\
&= 
\begin{bmatrix}
-2 & -1+i & 2-2i \\
-3-4i & -3-i & 8+2i \\
3i & 1+i & -3-3i
\end{bmatrix}
\end{align*}
Thus, by Properties \ref{proper:invadj}, the inverse of $A$ is
\begin{align*}
A^{-1} &= \frac{1}{\det(A)} \text{adj}(A) = \frac{1}{\det(A)} C^T \\
&= -\frac{1+i}{2} 
\begin{bmatrix}
-2 & -3-4i & 3i \\
-1+i & -3-i & 1+i \\
2-2i & 8+2i & -3-3i
\end{bmatrix} \\
&= 
\begin{bmatrix}
1+i & -\frac{1}{2}+\frac{7}{2}i & \frac{3}{2}-\frac{3}{2}i \\
1 & 1+2i & -i \\
-2 & -3-5i & 3i
\end{bmatrix} 
\end{align*}
\end{solution}

$\blacktriangleright$ Short Exercise: Find $A^{-1}$ via Gaussian Elimination.\footnote{Notice that we will now need to multiply rows with complex constants instead when doing elementary row operations. You should be able to get the same answer. Possible first few steps are multiplying the first row by $\frac{1+i}{2}$ to create a leading $1$ and subtracting $1+i$ and $2$ times the new first row from the second and third row respectively.}

Below are some useful properties of determinants and inverses for complex matrices that can be compared to Properties \ref{proper:properdet} and \ref{proper:inverse}.
\begin{proper}
If $A$ is a complex matrix, then
\begin{enumerate}
\item $\text{det}(A^T) = \text{det}(A)$;
\item $\text{det}(A^*) = \overline{\text{det}(A)}$;
\item $\text{det}(kA) = k^n \text{det}(A)$, for any complex constant $k$;
\item $\text{det}(AB) = \text{det}(A)\text{det}(B)$; and
\item $\text{det}(A^{-1}) = \frac{1}{\text{det}(A)}$, given $A$ is invertible ($\det(A) \neq 0$ as the same as before).
\end{enumerate}
Additionally, if $A$ is non-singular, then
\begin{enumerate}
\item $(cA)^{-1} = \frac{1}{c}A^{-1}$, for any complex scalar $c \neq 0$;
\item $(A^{-1})^{-1} = A$;
\item $(A^n)^{-1} = (A^{-1})^n$, for any positive integer $n$;
\item $(AB)^{-1} = B^{-1}A^{-1}$, provided that $B$ is invertible too, plus $A$ and $B$ are conformable;
\item $(A^T)^{-1} = (A^{-1})^T$;
\item $(A^*)^{-1} = (A^{-1})^*$.
\end{enumerate}
\end{proper}

\subsection{The Complex $n$-space $\mathbb{C}^n$}

Similar to the real $n$-space $\mathbb{R}^n$ brought up in Definition \ref{defn:real_nspace}, the set of all complex vectors, now with $n$ complex components, forms the \index{Complex $n$-space}\keywordhl{complex $n$-space} $\mathbb{C}^n$ as follows.
\begin{defn}[The Complex $n$-space $\mathbb{C}^n$]
\label{defn:complex_nspace}
The complex $n$-space $\mathbb{C}^n$ is defined as the set of all possible $n$-tuples in the form of $\vec{v} = (v_1, v_2, v_3, \ldots, v_n)^T$, where $v_i$ can be any \textit{complex} numbers, for $i = 1,2,3,\ldots,n$. They are known as $n$-dimensional complex vectors.
\end{defn}
A very interesting (and perhaps quite confusing) fact about the complex $n$-space $\mathbb{C}^n$, or an $n$-dimensional complex vector, is that it can be considered as $2n$-dimensional when put in the frame of a real vector space. The key lies in Definition \ref{defn:realvecspaceaxiom}, where if the underlying scalar is set to $\mathbb{R}$ or $\mathbb{C}$ so that it becomes a real/complex vector space. Notice the subtle difference between a real/complex vector (that is indicative of its components being real/complex) and real/complex vector space (concerning \textit{the underlying scalar used in scalar multiplication)}. We take $\mathbb{C}$ as a vector space here for illustration. If $\mathbb{C}$ is treated as a complex vector space, i.e.\ \textit{over} $\mathbb{C}$ itself, then $\{1\}$ is a basis for $\mathbb{C}$ since the scalar multiplication of $1$ by any arbitrary \textit{complex scalar} can generate all complex numbers as the scalar itself. Hence, the dimension of $\mathbb{C}$ is $1$ over $\mathbb{C}$ (Properties \ref{proper:samenvecsbases} still holds for complex vector spaces). Otherwise, if $\mathbb{C}$ is taken as a real vector space (\textit{over} $\mathbb{R}$), then $\{1\}$ is not sufficient to be a basis for $\mathbb{C}$ since multiplication of $1$ by only any \textit{real scalar} $a$ can never produce complex numbers with a non-zero imaginary part. On the other hand, $\{1, i\}$ can instead be a basis for $\mathbb{C}$ over $\mathbb{R}$ as linear combinations of $1$ and $i$ with purely real coefficients can produce all complex numbers. So by Properties \ref{proper:samenvecsbases}, the dimension of $\mathbb{C}$ over $\mathbb{R}$ is $2$, and with Theorem \ref{thm:isomorphism}, it is isomorphic to $\mathbb{R}^2$ in this situation. An explicit isomorphism between $\mathbb{C}$ and $\mathbb{R}^2$ over $\mathbb{R}$ is simply
\begin{align*}
T(a+bi) = (a,b)^T
\end{align*}
Extending this observation, $\mathbb{C}^n$ can either be treated as $n$-dimensional over $\mathbb{C}$ or $2n$-dimensional over $\mathbb{R}$ (and is isomorphic to $\mathbb{R}^{2n}$). However, unless mentioned otherwise, we consider any $\mathbb{C}^n$ vector (or complex matrix) to be taken over $\mathbb{C}$ (the former case) onwards. All major results from the last two chapters are still valid if we replace $\mathbb{R}$ and $\mathbb{R}^n$ by $\mathbb{C}$ and $\mathbb{C}^n$ appropriately.\footnote{For example, a linear combination of complex vectors (Definition \ref{defn:linearcomb}) $\vec{v}^{(j)} \in \mathbb{C}^n$ is still in the form of $c_1\vec{v}^{(1)} + c_2\vec{v}^{(2)} + c_3\vec{v}^{(3)} + \cdots + c_q\vec{v}^{(q)}$ but the coefficients $c_j \in \mathbb{C}$ are complex numbers now.} In particular, we note that
\begin{proper}
The angle between two complex vectors $\vec{u}$ and $\vec{v}$ is found according to
\begin{align}
\cos \theta = \frac{\Re{\vec{u} \cdot \vec{v}}}{\norm{u}\norm{v}}
\end{align}
\end{proper}
which should be compared to (\ref{eqn:dotgeo}).

\section{Manipulating Block Matrices}

Moving to our second topic, a \keywordhl{block matrix} is a matrix written in smaller \textit{submatrices} as if they are ordinary entries. For example, a $2 \times 2$ block matrix has the form of
\begin{align*}
M =
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix}
\end{align*}
where $A$, $B$, $C$, $D$ are themselves matrices having the shapes of $m \times p$, $m \times q$, $n \times p$, $n \times q$, and $m, n, p, q$ can be any positive integer. As a more concrete example, we have
\begin{align*}
M = 
\left[\begin{array}{@{\,}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}wc{10pt}}
1 & 0 & 3 & 0 & 4 \\
0 & 1 & 1 & 2 & -1 \\
\hline
2 & -1 & 0 & 1 & -2 \\
\end{array}\right]
\end{align*}
being a $3 \times 5$ matrix at the same time a $2 \times 2$ block matrix where
\begin{align*}
A &= \begin{bmatrix}
1 & 0 & 3 \\
0 & 1 & 1
\end{bmatrix}
& 
B &= \begin{bmatrix}
0 & 4 \\
2 & -1
\end{bmatrix} \\
C &= \begin{bmatrix}
2 & -1 & 0 \\
\end{bmatrix}
& 
D &= \begin{bmatrix}
1 & -2
\end{bmatrix}
\end{align*}
are of the shapes $2 \times 3$, $2 \times 2$, $1 \times 3$ and $1 \times 2$. We can extend this for block matrices of any partition. For instance, a $3 \times 4$ block matrix will be in the form of
\begin{align*}
M =
\begin{bmatrix}
M_{11} & M_{12} & M_{13} & M_{14} \\
M_{21} & M_{22} & M_{23} & M_{24} \\
M_{31} & M_{32} & M_{33} & M_{34} \\
\end{bmatrix}
\end{align*}
where the $M_{ij}$ are submatrices, and for a fixed $i$ [$j$], $M_{ij}$ has the same number of rows [columns].

\subsection{Block Matrix Multiplication}
\label{subsection:blockmul}

With the structure of a block matrix explained, we can now examine how matrix multiplication between two block matrices is done. Let's take a look at the easiest case of two $2 \times 2$ block matrices:
\begin{align*}
M &=
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix} 
& &
N =
\begin{bmatrix}
X & Y \\
Z & W
\end{bmatrix} 
\end{align*}
Of course, from the very beginning (Section \ref{section:matrixdefn}), we know that $M$ and $N$ themselves have to be of the shapes $m \times r$ and $r \times n$ as ordinary matrices, but how about the submatrices? In fact, we just carry out the multiplication as if each of them is a single entry, such that
\begin{align}
MN = 
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix} 
\begin{bmatrix}
X & Y \\
Z & W
\end{bmatrix} 
=
\begin{bmatrix}
AX + BZ & AY + BW \\
CX + DZ & CY + DW
\end{bmatrix}
\label{eqn:22blockmul}
\end{align}
Then, for each resulting block to be valid, the number of columns in $A$ and $C$ [$B$ and $D$] must be the same as that of rows in $X$ and $Y$ [$Z$ and $W$]. So that $A$ and $C$ will have the shapes of $m_1 \times r_1$, $m_2 \times r_1$, $X$ and $Y$ will have the shapes of $r_1 \times n_1$, $r_1 \times n_2$, where $m_1 + m_2 = m$, $n_1 + n_2 = n$. Similarly, $B$ and $D$ need to have the shapes of $m_1 \times r_2$, $m_2 \times r_2$, $Z$ and $W$ need to have the shapes of $r_2 \times n_1$, $r_2 \times n_2$, and $r = r_1 + r_2$. In short, the position of vertical cuts along the column direction of $M$ must coincide with that of horizontal cuts along the row direction of $N$. Below is a walk-through example.

\begin{exmp}
Given 
\begin{align*}
M &=
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix} 
& &
N =
\begin{bmatrix}
X & Y \\
Z & W
\end{bmatrix} 
\end{align*}
as a $3 \times 3$ and $3 \times 2$ matrix respectively, with
\begin{align*}
A &= 
\begin{bmatrix}
1 & 2 \\
0 & 1
\end{bmatrix}
& 
B &=
\begin{bmatrix}
1 \\
-2
\end{bmatrix} \\
C &= 
\begin{bmatrix}
0 & -1 
\end{bmatrix}
&
D &=
\begin{bmatrix}
1
\end{bmatrix} \\
X &= 
\begin{bmatrix}
0 \\
2 
\end{bmatrix}
&
Y &=
\begin{bmatrix}
1 \\
-1
\end{bmatrix} \\
Z &= 
\begin{bmatrix}
-1
\end{bmatrix}
&
W &=
\begin{bmatrix}
1
\end{bmatrix} 
\end{align*}
such that the partitions look like
\begin{align*}
M &=
\left[\begin{array}{@{\,}wc{10pt}wc{10pt}|wc{10pt}}
1 & 2 & 1 \\
0 & 1 & -2 \\
\hline
0 & -1 & 1
\end{array}\right]
& &
N =
\left[\begin{array}{@{\,}wc{10pt}|wc{10pt}}
0 & 1 \\
2 & -1 \\
\hline
-1 & 1
\end{array}\right]   
\end{align*}
Use block matrix multiplication to compute $MN$.
\end{exmp}
\begin{solution}
Note that the cuts along the column/row direction in $M$ and $N$ are both located in-between the $2$nd-$3$rd index. Consequentially, we can use Formula (\ref{eqn:22blockmul}) above:
\begin{align*}
MN = 
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix} 
\begin{bmatrix}
X & Y \\
Z & W
\end{bmatrix} 
=
\begin{bmatrix}
AX + BZ & AY + BW \\
CX + DZ & CY + DW
\end{bmatrix}
\end{align*}
which requires us to compute
\begin{align*}
AX &=
\begin{bmatrix}
1 & 2 \\
0 & 1
\end{bmatrix}
\begin{bmatrix}
0 \\
2 
\end{bmatrix}
=
\begin{bmatrix}
4 \\
2
\end{bmatrix}
&
BZ &=
\begin{bmatrix}
1 \\
-2
\end{bmatrix}
\begin{bmatrix}
-1
\end{bmatrix}
=
\begin{bmatrix}
-1 \\
2
\end{bmatrix} \\
AY &=
\begin{bmatrix}
1 & 2 \\
0 & 1
\end{bmatrix}
\begin{bmatrix}
1 \\
-1
\end{bmatrix}
=
\begin{bmatrix}
-1 \\
-1
\end{bmatrix}
&
BW &=
\begin{bmatrix}
1 \\
-2
\end{bmatrix}
\begin{bmatrix}
1
\end{bmatrix} 
=
\begin{bmatrix}
1 \\
-2
\end{bmatrix} \\
CX &=
\begin{bmatrix}
0 & -1 
\end{bmatrix}
\begin{bmatrix}
0 \\
2 
\end{bmatrix}
=
\begin{bmatrix}
-2
\end{bmatrix}
&
DZ &=
\begin{bmatrix}
1
\end{bmatrix}
\begin{bmatrix}
-1
\end{bmatrix}
=
\begin{bmatrix}
-1
\end{bmatrix} \\
CY &=
\begin{bmatrix}
0 & -1 
\end{bmatrix}
\begin{bmatrix}
1 \\
-1
\end{bmatrix}
=
\begin{bmatrix}
1
\end{bmatrix}
&
DW &=
\begin{bmatrix}
1
\end{bmatrix}
\begin{bmatrix}
1
\end{bmatrix} 
=
\begin{bmatrix}
1
\end{bmatrix}
\end{align*}
Hence
\begin{align*}
MN &= \begin{bmatrix}
AX + BZ & AY + BW \\
CX + DZ & CY + DW
\end{bmatrix} \\
&=
\left[\begin{array}{@{\,}wc{55pt}|wc{55pt}}
\begin{bmatrix}
4 \\
2
\end{bmatrix}+
\begin{bmatrix}
-1 \\
2
\end{bmatrix}
&
\begin{bmatrix}
-1 \\
-1
\end{bmatrix}+
\begin{bmatrix}
1 \\
-2
\end{bmatrix}\\[9pt]
\hline
\begin{bmatrix}
-2
\end{bmatrix}+
\begin{bmatrix}
-1 
\end{bmatrix}
&
\begin{bmatrix}
1
\end{bmatrix}+
\begin{bmatrix}
1 
\end{bmatrix} \Tstrut
\end{array}\right]
=
\left[\begin{array}{@{\,}wc{10pt}|wc{10pt}}
3 & 0 \\
4 & -3 \\
\hline
-3 & 2
\end{array}\right]
\end{align*}
The readers can check the answer by computing the matrix product in the usual way.
\end{solution}

For multiplication involving block matrices with more blocks, the two block matrices $M$ and $N$ must have a partition of $m \times r$ and $r \times n$ blocks, and the block multiplication is carried out as if they are individual entries in usual matrix multiplication as well. Particularly, the positions where the $r$ column [row] partition of $M$ [$N$] occurs must align exactly. Given
\begin{align*}
M &=
\small\begin{bmatrix}
M_{11} & M_{12} & M_{13} & \cdots & M_{1r} \\
M_{21} & M_{22} & M_{23} &  & M_{2r} \\
M_{31} & M_{32} & M_{33} &  & M_{3r} \\
\vdots & & & \ddots & \vdots \\
M_{m1} & M_{m2} & M_{m3} &  & M_{mr} 
\end{bmatrix} & \text{ and } & &
N &=
\small\begin{bmatrix}
N_{11} & N_{12} & N_{13} & \cdots & N_{1n} \\
N_{21} & N_{22} & N_{23} &  & N_{2n} \\
N_{31} & N_{32} & N_{33} &  & N_{3n} \\
\vdots & & & \ddots & \vdots \\
N_{r1} & N_{r2} & N_{r3} &  & N_{rn} 
\end{bmatrix}
\end{align*}
this means that the numbers of columns and rows in $M_{ik}$ and $N_{kj}$ for any fixed $k$ should be equal, such that $M_{ik}$ and $N_{kj}$ are of the shapes $m_i \times r_k$ and $r_k \times n_j$. 

\subsection{Inverse and Determinant of a Block Matrix}
To properly utilize block matrices, we also need to know how to compute some basic quantities related to them, like inverse and determinant. Since most of the situations involve $2 \times 2$ block matrices only, we will handle them exclusively. Specifically, we consider $2 \times 2$ block matrices in the form of
\begin{align*}
M = \begin{bmatrix}
A & B \\
C & D
\end{bmatrix}
\end{align*}
where $A$, $B$, $C$, $D$ are submatrices of the shapes $p \times p$, $p \times q$, $q \times p$ and $q \times q$, such that $A$, $D$ and thus $M$ are square. To proceed, we need the following observations.
\begin{proper}
\label{proper:blockmatinv}
Denote the $p \times p$ and $q \times q$ identity matrices by $I_p$ and $I_q$. Then the matrix
\begin{align*}
\begin{bmatrix}
I_p & [\textbf{0}]_{p \times q} \\
-C & I_q
\end{bmatrix}
\end{align*}
is invertible, particularly having a determinant of $1$. If furthermore, $A$ is invertible, then
\begin{align*}
\begin{bmatrix}
A^{-1} & [\textbf{0}]_{p\times q} \\
[\textbf{0}]_{q\times p} & I_q
\end{bmatrix}
\end{align*}
is also invertible with a determinant of $\det(A^{-1}) = (\det(A))^{-1}$.
\end{proper}
\begin{proof}
For the first matrix, simply note that it is a lower-triangular matrix with all diagonal entries being $1$, and hence it has a determinant of $1$. Therefore, by Properties \ref{proper:invnonzerodet}, it is invertible. Similarly, by repeated cofactor expansions along the bottommost row for $q$ times, the determinant of the second matrix can be seen to be $\det(A^{-1}) = (\det(A))^{-1}$ (Properties \ref{proper:properdet}). If $A$ is invertible, then $\det(A^{-1}) = (\det(A))^{-1}$ is nonzero by Properties \ref{proper:invnonzerodet} again, and 
\begin{align*}
\begin{bmatrix}
A^{-1} & [\textbf{0}]_{p\times q} \\
[\textbf{0}]_{q\times p} & I_q
\end{bmatrix}    
\end{align*}
will also be invertible.
\end{proof}
The above properties imply that these two matrices are the results from elementary row operations (Properties \ref{proper:invseqelement}), and therefore, their product
\begin{align*}
&\quad \begin{bmatrix}
I_p & [\textbf{0}]_{p \times q} \\
-C & I_q
\end{bmatrix}
\begin{bmatrix}
A^{-1} & [\textbf{0}]_{p\times q} \\
[\textbf{0}]_{q\times p} & I_q
\end{bmatrix} \\
&=
\begin{bmatrix}
I_pA^{-1} + [\textbf{0}]_{p\times q}[\textbf{0}]_{q\times p} & I_p[\textbf{0}]_{p\times q} + [\textbf{0}]_{p\times q}I_q \\
(-C)A^{-1} + I_q[\textbf{0}]_{q \times p} & -C[\textbf{0}]_{p \times q} + I_qI_q
\end{bmatrix} \\
&=
\begin{bmatrix}
A^{-1} & [\textbf{0}]_{p\times q} \\
-CA^{-1} & I_q
\end{bmatrix}
\end{align*}
can also be arrived via elementary row operations and is invertible as well (Properties \ref{proper:ABinv}). By multiplying this matrix to $M$, we have
\begin{align*}
\begin{bmatrix}
A^{-1} & [\textbf{0}]_{p\times q} \\
-CA^{-1} & I_q
\end{bmatrix}
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix} &= 
\begin{bmatrix}
A^{-1}A + [\textbf{0}]_{p\times q}C & A^{-1}B + [\textbf{0}]_{p \times q}D \\
-CA^{-1}A + I_qC & - CA^{-1}B + I_qD  
\end{bmatrix} \\
&= 
\begin{bmatrix}
I_p & A^{-1}B \\
-C + C & - CA^{-1}B + D  
\end{bmatrix} \\
&=
\begin{bmatrix}
I_p & A^{-1}B \\
[\textbf{0}]_{q\times p} & D - CA^{-1}B 
\end{bmatrix}
\end{align*}
The bottom right block, $D - CA^{-1}B$, is known as the \index{Schur complement}\keywordhl{Schur complement} of block $A$ in $M$, denoted as $M/A$ and has the same shape $q \times q$ as $D$. The above block multiplication then constitutes a \textit{block Gaussian Elimination} over the matrix $M$ to make it \textit{block upper-triangular}. It is not hard to see that
\begin{align*}
\begin{bmatrix}
I_p & A^{-1}B \\
[\textbf{0}]_{q\times p} & D - CA^{-1}B 
\end{bmatrix}
\begin{bmatrix}
I_p & -A^{-1}B \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix}
=
\begin{bmatrix}
I_p & [\textbf{0}]_{p \times q} \\
[\textbf{0}]_{q\times p} & D - CA^{-1}B 
\end{bmatrix}
\end{align*}
Therefore,
\begin{align*}
&\quad \begin{bmatrix}
A^{-1} & [\textbf{0}]_{p\times q} \\
-CA^{-1} & I_q
\end{bmatrix}
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix}
\begin{bmatrix}
I_p & -A^{-1}B \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix} \\
&= \begin{bmatrix}
I_p & A^{-1}B \\
[\textbf{0}]_{q\times p} & D - CA^{-1}B 
\end{bmatrix}
\begin{bmatrix}
I_p & -A^{-1}B \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix}
=
\begin{bmatrix}
I_p & [\textbf{0}]_{p \times q} \\
[\textbf{0}]_{q\times p} & D - CA^{-1}B 
\end{bmatrix}
\end{align*}
According to the above equation, if the Schur complement $M/A = D-CA^{-1}B$ is also invertible, then the inverse of $M$ will exist, because
\begin{align*}
\small
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix}
&= 
\small
\left(\begin{bmatrix}
A^{-1} & [\textbf{0}]_{p\times q} \\
-CA^{-1} & I_q
\end{bmatrix}\right)^{-1}
\begin{bmatrix}
I_p & [\textbf{0}]_{p \times q} \\
[\textbf{0}]_{q\times p} & D - CA^{-1}B 
\end{bmatrix}
\left(\begin{bmatrix}
I_p & -A^{-1}B \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix}\right)^{-1}
\end{align*}
where the three matrices on R.H.S. are all invertible.\footnote{The invertibility of the first and last matrix follows the same arguments in Properties \ref{proper:blockmatinv}, while for the matrix in the middle, we have required that $D-CA^{-1}B$ has to be invertible, and its inverse can be readily seen to be
\begin{align*}
\begin{bmatrix}
I_p & [\textbf{0}]_{p \times q} \\
[\textbf{0}]_{q\times p} & D - CA^{-1}B 
\end{bmatrix}^{-1}
=
\begin{bmatrix}
I_p & [\textbf{0}]_{p \times q} \\
[\textbf{0}]_{q\times p} & (D - CA^{-1}B)^{-1}
\end{bmatrix}
\end{align*}} By Properties \ref{proper:inverse}, we arrive at
\begin{align*}
M^{-1} &=
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix}^{-1} \\
&= \left(\begin{bmatrix}
A^{-1} & [\textbf{0}]_{p\times q} \\
-CA^{-1} & I_q
\end{bmatrix}^{-1}
\begin{bmatrix}
I_p & [\textbf{0}]_{p \times q} \\
[\textbf{0}]_{q\times p} & D - CA^{-1}B 
\end{bmatrix}
\begin{bmatrix}
I_p & -A^{-1}B \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix}^{-1} \right)^{-1} \\
&=
\begin{bmatrix}
I_p & -A^{-1}B \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix} 
\begin{bmatrix}
I_p & [\textbf{0}]_{p \times q} \\
[\textbf{0}]_{q\times p} & (D - CA^{-1}B)^{-1}
\end{bmatrix}
\begin{bmatrix}
A^{-1} & [\textbf{0}]_{p\times q} \\
-CA^{-1} & I_q
\end{bmatrix} \\
&=
\begin{bmatrix}
I_p & -A^{-1}B (D - CA^{-1}B)^{-1} \\
[\textbf{0}]_{q \times p} & (D - CA^{-1}B)^{-1}
\end{bmatrix}
\begin{bmatrix}
A^{-1} & [\textbf{0}]_{p\times q} \\
-CA^{-1} & I_q
\end{bmatrix} \\
&= \begin{bmatrix}
A^{-1} + A^{-1}B (D - CA^{-1}B)^{-1} CA^{-1} & -A^{-1}B (D - CA^{-1}B)^{-1} \\
-(D - CA^{-1}B)^{-1}CA^{-1} & (D - CA^{-1}B)^{-1}
\end{bmatrix} \\
&= \begin{bmatrix}
A^{-1} + A^{-1}B (M/A)^{-1} CA^{-1} & -A^{-1}B (M/A)^{-1} \\
-(M/A)^{-1}CA^{-1} & (M/A)^{-1}
\end{bmatrix}
\end{align*}
To summarize, we have the following statements.
\begin{proper}
\label{proper:schurinv}
For the $2 \times 2$ block matrix
\begin{align*}
M = 
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix}
\end{align*}
where $A$ and $D$ are square submatrices, if $A$ and its Schur complement $M/A = D - CA^{-1}B$ of block $A$ are both invertible, then $M$ is invertible with
\begin{align}
M^{-1} = \begin{bmatrix}
A^{-1} + A^{-1}B (M/A)^{-1} CA^{-1} & -A^{-1}B (M/A)^{-1} \\
-(M/A)^{-1}CA^{-1} & (M/A)^{-1}
\end{bmatrix}
\label{eqn:schurinv1}
\end{align}
\end{proper}
\begin{proper}
\label{proper:schurdet}
The determinant of the $2 \times 2$ block matrix in Properties \ref{proper:schurinv} is
\begin{align}
\det(M) = \det(A)\det(D-CA^{-1}B) = \det(A)\det(M/A)
\end{align}
if $A^{-1}$ is well-defined.
\end{proper}
\begin{proof}
From the derivation above, we have
\begin{align*}
\begin{bmatrix}
A^{-1} & [\textbf{0}]_{p\times q} \\
-CA^{-1} & I_q
\end{bmatrix}
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix}
\begin{bmatrix}
I_p & -A^{-1}B \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix} 
=
\begin{bmatrix}
I_p & [\textbf{0}]_{p \times q} \\
[\textbf{0}]_{q\times p} & D - CA^{-1}B 
\end{bmatrix}
\end{align*}
Evaluating the determinants of both sides (Properties \ref{proper:properdet}) leads to
\begin{align*}
&\quad \det(\begin{bmatrix}
A^{-1} & [\textbf{0}]_{p\times q} \\
-CA^{-1} & I_q
\end{bmatrix})
\det(M)
\det(\begin{bmatrix}
I_p & -A^{-1}B \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix}) \\
&=
\det(\begin{bmatrix}
I_p & [\textbf{0}]_{p \times q} \\
[\textbf{0}]_{q\times p} & D - CA^{-1}B 
\end{bmatrix})
\end{align*}
In the same vein of Properties \ref{proper:blockmatinv}, we then have
\begin{align*}
(\det(A))^{-1}\det(M)(1) &= \det(D-CA^{-1}B) \\
\det(M) &= \det(A)\det(D-CA^{-1}B) = \det(A)\det(M/A)
\end{align*}
\end{proof}

\begin{exmp}
Use Properties \ref{proper:schurinv} and \ref{proper:schurdet} to compute the inverse and determinant of the following matrix
\begin{align*}
M = 
\left[\begin{array}{@{\,}wc{10pt}wc{10pt}|wc{10pt}}
1 & 2 & 0 \\
0 & 1 & 1 \\
\hline
-2 & -1 & 2
\end{array}\right]
\end{align*}
via the partition above.
\end{exmp}
\begin{solution}
To use Properties \ref{proper:schurinv}, we need to first compute $A^{-1}$ and $M/A = D - CA^{-1}B$. We leave it to the readers to verify that
\begin{align*}
A^{-1} &= 
\begin{bmatrix}
1 & -2 \\
0 & 1
\end{bmatrix} \\
M/A = D - CA^{-1}B &=
\begin{bmatrix}
2
\end{bmatrix}
-
\begin{bmatrix}
-2 & -1
\end{bmatrix}
\begin{bmatrix}
1 & -2 \\
0 & 1    
\end{bmatrix}
\begin{bmatrix}
0 \\
1
\end{bmatrix}
= \begin{bmatrix}
-1
\end{bmatrix}
\end{align*}
Then by Properties \ref{proper:schurinv}, we have
\begin{align*}
M^{-1} &= \begin{bmatrix}
A^{-1} + A^{-1}B (M/A)^{-1} CA^{-1} & -A^{-1}B (M/A)^{-1} \\
-(M/A)^{-1}CA^{-1} & (M/A)^{-1}
\end{bmatrix} \\
&= \small\left[\begin{array}{@{\,}wc{210pt}|wc{84pt}}
\begin{bmatrix}
1 & -2 \\
0 & 1
\end{bmatrix}
+
\begin{bmatrix}
1 & -2 \\
0 & 1
\end{bmatrix}
\begin{bmatrix}
0 \\
1
\end{bmatrix}
\begin{bmatrix}
-1
\end{bmatrix}
\begin{bmatrix}
-2 & -1
\end{bmatrix} 
\begin{bmatrix}
1 & -2 \\
0 & 1
\end{bmatrix}
&
-\begin{bmatrix}
1 & -2 \\
0 & 1
\end{bmatrix}
\begin{bmatrix}
0 \\
1
\end{bmatrix}
\begin{bmatrix}
-1
\end{bmatrix}
\\[9pt]
\hline
-
\begin{bmatrix}
-1
\end{bmatrix}
\begin{bmatrix}
-2 & -1
\end{bmatrix}
\begin{bmatrix}
1 & -2 \\
0 & 1
\end{bmatrix}
&
\begin{bmatrix}
-1
\end{bmatrix}
\Tstrut
\end{array}\right] \\
&=
\left[\begin{array}{@{\,}wc{10pt}wc{10pt}|wc{10pt}}
-3 & 4 & -2 \\
2 & -2 & 1 \\
\hline
-2 & 3 & -1
\end{array}\right]
\end{align*}
Meanwhile, by Properties \ref{proper:schurdet},
\begin{align*}
\det(M) &= \det(A)\det(M/A) \\
&= \det(\begin{bmatrix}
1 & -2 \\
0 & 1
\end{bmatrix})
\det([-1]) = (1)(-1) = -1
\end{align*}
\end{solution}

Similar results are also available in terms of the Schur complement using block $D$ instead of $A$.
\begin{proper}
\label{proper:schurinvD}
For the $2 \times 2$ block matrix
\begin{align*}
M = 
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix}
\label{eqn:schurinv2}
\end{align*}
where $A$ and $D$ are square submatrices, if $D$ and its Schur complement $M/D = A - BD^{-1}C$ of block $D$ are both invertible, then $M$ is invertible with
\begin{align}
M^{-1} = \begin{bmatrix}
(M/D)^{-1} & -(M/D)^{-1}BD^{-1}  \\
-D^{-1}C (M/D)^{-1} & D^{-1} + D^{-1}C (M/D)^{-1} BD^{-1}
\end{bmatrix}
\end{align}
and its determinant can be computed by
\begin{align}
\det(M) = \det(D)\det(A - BD^{-1}C) = \det(D)\det(M/D)
\end{align}
\end{proper}
\begin{proof}
See Exercise \ref{ex:schurinvD}.
\end{proof}

\subsection{Restriction of a Linear Transformation, Direct Sum of a Matrix}
In the last chapter, we have discussed linear transformations between two vector spaces, let's say, from $\mathcal{U}$ to $\mathcal{V}$. Sometimes we only care about how the linear transformation works on some specific subspace $\mathcal{W}$ of $\mathcal{U}$. This leads to the idea of \index{Restriction of a Linear Transformation}\keywordhl{restriction} of a linear transformation as follows.
\begin{defn}[Restriction of a Linear Transformation]
\label{defn:restrictionTW}
Given a linear transformation $T: \mathcal{U} \to \mathcal{V}$ and a proper subspace $\mathcal{W} \subset \mathcal{U}$, the restriction of $T$ to $\mathcal{W}$ is defined as
\begin{align*}
T|_W: \mathcal{W} \to \mathcal{V}, T|_W(\vec{w}) = T(\vec{w}) \text{ for any $\vec{w} \in \mathcal{W}$.}
\end{align*}
\end{defn}
In simpler terms, $T|_W$ works exactly as $T$ but is only defined on $\mathcal{W}$. Assume the vector spaces involved are all finite-dimensional, and $\dim(\mathcal{W}) = r < n = \dim(\mathcal{U})$. $\mathcal{W}$ then has a basis $\mathcal{\beta}_W$ with $r$ generating vectors (Properties \ref{proper:samenvecsbases}), which by part (c) of Properties \ref{proper:linindspanbasisnewver} can be extended to a new basis $\mathcal{\beta}' = \mathcal{\beta}_W \cup \mathcal{\gamma}$ for $\mathcal{U}$, where $\mathcal{\gamma}$ contains $n - r$ vectors and $\mathcal{\beta}' = \mathcal{\beta}_W \cup \mathcal{\gamma}$ has exactly $n$ linearly independent vectors by construction. Some may wonder why we suddenly talk about the restriction of a linear transformation here, and the reason is that its related principles can be viewed from the standpoint of a block matrix.\par
To see this, let $\mathcal{\beta}_W = \{\vec{u}^{(1)}, \vec{u}^{(2)}, \ldots, \vec{u}^{(r)}\}$ and $\mathcal{\gamma} = \{\vec{u}^{(r+1)}, \ldots, \vec{u}^{(n)}\}$, and thus $\mathcal{\beta}' = \{\vec{u}^{(1)}, \vec{u}^{(2)}, \ldots, \vec{u}^{(r)}, \vec{u}^{(r+1)}, \ldots, \vec{u}^{(n)}\}$. By Definition \ref{defn:directsum}, since the vectors in $\mathcal{\beta}' = \mathcal{\beta}_W \cup \mathcal{\gamma}$ are designed to be linearly independent, the subspace $\mathcal{W}^C$ generated by $\mathcal{\gamma}$ will be the complement of $\mathcal{W}$ as a result of $\mathcal{W} \oplus \mathcal{W}^C$ being a direct sum that produces the $n$-dimensional $\mathcal{U}$ (Properties \ref{proper:complement}). Any $\vec{w} \in \mathcal{W} \subset \mathcal{U}$ will then have a coordinate representation of
\begin{align*}
(w_1, w_2, \ldots, w_r, 0, \ldots, 0)^T
\end{align*}
in the $\mathcal{\beta}'$ basis where components beyond the $r$-th index are all zeros. From the perspective of direct sum, it is the same as $\vec{w} \oplus \textbf{0}_{n-r} = (w_1, w_2, \ldots, w_r)_{\beta_W}^T \oplus (0, \ldots, 0)_{\gamma}^T$, i.e. the components of $\vec{w}$ for $\mathcal{W}^C$ are all zeros. By Definition \ref{defn:matrixrepoflintrans}, writing out the matrix representation of $[T]_{\beta'}^\eta$ where $\eta$ is an arbitrary basis for the $m$-dimensional $\mathcal{V}$ results in
\begin{align*}
[T]_{\beta'}^\eta = \begin{bmatrix}
a_1^{(1)} & a_1^{(2)} & \cdots & a_1^{(r)} & a_1^{(r+1)} & \cdots & a_1^{(n)} \\
a_2^{(1)} & a_2^{(2)} & & a_2^{(r)} & a_2^{(r+1)} & & a_2^{(n)} \\
\vdots & & & \vdots & \vdots & & \vdots \\
a_m^{(1)} & a_m^{(2)} & \cdots & a_m^{(r)} & a_m^{(r+1)} & \cdots & a_m^{(n)}
\end{bmatrix}
\end{align*}
Since we are only concerned about $\vec{w} \in \mathcal{W} \subset \mathcal{U}$ (or $\vec{w} \oplus \textbf{0} \in \mathcal{W} \oplus \mathcal{W}^C$) when dealing with $T|_W$, when we apply $T$ on $\vec{w}$, which is
\begin{align}
[T]_{\beta'}^\eta[\vec{w}]_{\beta'}
&=
\left[\begin{array}{@{\,}wc{15pt}wc{15pt}wc{15pt}wc{15pt}|@{\quad}wc{15pt}wc{15pt}wc{15pt}}
a_1^{(1)} & a_1^{(2)} & \cdots & a_1^{(r)} & a_1^{(r+1)} & \cdots & a_1^{(n)} \\
a_2^{(1)} & a_2^{(2)} & & a_2^{(r)} & a_2^{(r+1)} & & a_2^{(n)} \\
\vdots & & & \vdots & \vdots & & \vdots \\
a_m^{(1)} & a_m^{(2)} & \cdots & a_m^{(r)} & a_m^{(r+1)} & \cdots & a_m^{(n)}
\end{array}\right]_{\beta_W+\gamma}^\eta
\left[\begin{array}{c}
w_1 \\
w_2 \\
\vdots \\
w_r \\
\hline
0 \\
\vdots \\
0
\end{array}\right]_{\beta_W+\gamma} \nonumber \\
&= [(T|_W)\mid(T|_{W^C})]_{\beta_W+\gamma}^\eta [\vec{w} \oplus \textbf{0}]^T_{\beta_W+\gamma} 
\end{align}
We can simply ignore $[T|_{W^C}]_\gamma^\eta$, the block at the right of the $[T]_{\beta'}^\eta$ partition as well as discard the all-zero components of $[\vec{w}]_{\beta'}$ starting from the $(r+1)$-th index, and keep only the other block $[T|_{W}]_{\beta_W}^\eta$ at the left and the $[\vec{w}]_{\beta_W}$ part. The output of the truncated multiplication
\begin{align}
[T|_{W}]_{\beta_W}^\eta [\vec{w}]_{\beta_W} = 
\left[\begin{array}{@{\,}wc{15pt}wc{15pt}wc{15pt}wc{15pt}}
a_1^{(1)} & a_1^{(2)} & \cdots & a_1^{(r)} \\
a_2^{(1)} & a_2^{(2)} & & a_2^{(r)} \\
\vdots & & & \vdots \\
a_m^{(1)} & a_m^{(2)} & \cdots & a_m^{(r)} 
\end{array}\right]
\begin{bmatrix}
w_1 \\
w_2 \\
\vdots \\
w_r \\
\end{bmatrix}
\end{align}
will represent the same vector in $\mathcal{W} \subset \mathcal{U}$ as that mapped from the full form $[T]_{\beta'}^\eta[\vec{w}]_{\beta'}$.
\begin{proper}
\label{proper:restrictmat}
For a linear transformation $T: \mathcal{U} \to \mathcal{V}$ between two finite-dimensional spaces, if a proper subspace $\mathcal{W}$ of $\mathcal{U}$ is generated by a basis $\mathcal{\beta}_W = \{\vec{w}^{(1)}, \vec{w}^{(2)}, \ldots, \vec{w}^{(r)}\}$, then the matrix representation of the restriction of $T$ to $\mathcal{W}$ with respect to $\mathcal{\beta}_W$ and $\mathcal{\eta}$ will be given by
\begin{align}
[T|_{W}]_{\beta_W}^\eta = \begin{bmatrix}
[T(\vec{w}^{(1)})]_\eta | [T(\vec{w}^{(2)})]_\eta | \cdots | [T(\vec{w}^{(r)})]_\eta
\end{bmatrix}    
\end{align}
where $\mathcal{\eta}$ is any basis for $\mathcal{V}$. (This can be compared to Definition \ref{defn:matrixrepoflintrans}.)
\end{proper}
In general, the effect of a linear transformation $T: \mathcal{U} \to \mathcal{V}$ applied to $\vec{u} \in \mathcal{U}$ is equivalent to the sum of responses from the restrictions of $T$ to a set of subspaces $\mathcal{W}_1, \mathcal{W}_2, \cdots, \mathcal{W}_s$ where they constitute a direct sum $\mathcal{W}_1 \oplus \mathcal{W}_2 \oplus \cdots \oplus \mathcal{W}_s = \mathcal{U}$, applied on the corresponding components $\vec{w}_1 \in \mathcal{W}_1, \vec{w}_2 \in \mathcal{W}_2, \cdots, \vec{w}_s \in \mathcal{W}_s$ of $\vec{u} = \vec{w}_1 + \vec{w}_2 + \cdots + \vec{w}_s$ in these smaller subspaces: $T(\vec{u}) = T(\vec{w}_1) + T(\vec{w}_2) + \cdots + T(\vec{w}_s)$.
\begin{exmp}
Given a linear transformation $T: \mathcal{U} \to \mathcal{V}$ that has a matrix representation of
\begin{align*}
[T]_\beta^\eta = 
\begin{bmatrix}
1 & 1 & 2 \\
2 & 0 & 1 \\
1 & -1 & 1
\end{bmatrix}
\end{align*}
with respect to some bases $\mathcal{\beta} = \{\vec{u}^{(1)}, \vec{u}^{(2)}, \vec{u}^{(3)}\}$ and $\mathcal{\eta} = \{\vec{v}^{(1)}, \vec{v}^{(2)}, \vec{v}^{(3)}\}$ for $\mathcal{U}$ and $\mathcal{V}$, find the restriction of $T$ to $\mathcal{W}$, where $\mathcal{W} \subset \mathcal{U}$ has a basis of $\beta_W = \{\vec{w}^{(1)}, \vec{w}^{(2)}\}$, with $\vec{w}^{(1)} = \vec{u}^{(1)} + \vec{u}^{(2)}$ and $\vec{w}^{(2)} = \vec{u}^{(1)} + \vec{u}^{(2)} + \vec{u}^{(3)}$.
\end{exmp}
\begin{solution}
We will take an indirect approach of reconstructing the basis first by finding a third vector generating $\mathcal{W}^C$ and producing the direct sum $\mathcal{W} \oplus \mathcal{W}^C = \mathcal{U}$. The change of coordinates matrix $\smash{P_{\beta_W}^{\beta}}$ from $\beta_W$ to $\beta$ as devised in Theorem \ref{thm:bijectivechincoord} appropriate in this situation is a $3 \times 2$ matrix instead since there are only two basis vectors in $\mathcal{\beta}_W$, and it can be easily seen to be
\begin{align*}
P_{\beta_W}^{\beta} &= \begin{bmatrix}
[\vec{w}^{(1)}]_\beta | [\vec{w}^{(2)}]_\beta
\end{bmatrix} \\
&= \begin{bmatrix}
[\vec{u}^{(1)} + \vec{u}^{(2)}]_\beta | [\vec{u}^{(1)} + \vec{u}^{(2)} + \vec{u}^{(3)}]_\beta
\end{bmatrix} \\
&= \begin{bmatrix}
1 & 1 \\
1 & 1 \\
0 & 1
\end{bmatrix}
\end{align*}
and we are to find $[\vec{w}^{(3)}]_\beta$ to complete a basis $\mathcal{\beta}' = \mathcal{\beta}_W \cup \{\vec{w}^{(3)}\}$ and hence $\smash{P_{\beta'}^{\beta}}$. An algorithm to do so, motivated by Footnote \ref{foot:inconsth} in Chapter \ref{chap:vec_space}, is to apply Gaussian Elimination to $\smash{P_{\beta_W}^{\beta}}$ first and then append 
$(0,0,1)^T$ to the right of the RREF to make an identity matrix, and reverse the entire reduction procedure as follows.
\begin{align*}
\left[\begin{array}{@{\,}wc{10pt}wc{10pt}@{\,}}
1 & 1 \\
1 & 1 \\
0 & 1
\end{array}\right] &\to
\left[\begin{array}{@{\,}wc{10pt}wc{10pt}@{\,}}
1 & 1 \\
0 & 0 \\
0 & 1
\end{array}\right] 
& R_2 - R_1 \to R_2 \\
&\to \left[\begin{array}{@{\,}wc{10pt}wc{10pt}@{\,}}
1 & 1 \\
0 & 1 \\
0 & 0
\end{array}\right]
& R_2 \leftrightarrow R_3 \\
&\to \left[\begin{array}{@{\,}wc{10pt}wc{10pt}@{\,}}
1 & 0 \\
0 & 1 \\
0 & 0
\end{array}\right]
& R_1 - R_2 \to R_1
\end{align*}
\begin{align*}
\left[\begin{array}{@{\,}wc{10pt}wc{10pt}|wc{10pt}@{\,}}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right] &\to
\left[\begin{array}{@{\,}wc{10pt}wc{10pt}|wc{10pt}@{\,}}
1 & 1 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right]
& R_1 + R_2 \to R_1 \\
&\to \left[\begin{array}{@{\,}wc{10pt}wc{10pt}|wc{10pt}@{\,}}
1 & 1 & 0 \\
0 & 0 & 1 \\
0 & 1 & 0
\end{array}\right]
& R_2 \leftrightarrow R_3 \\
&\to \left[\begin{array}{@{\,}wc{10pt}wc{10pt}|wc{10pt}@{\,}}
1 & 1 & 0 \\
1 & 1 & 1 \\
0 & 1 & 0
\end{array}\right]
& R_2 + R_1 \to R_2
\end{align*}
So $[\vec{w}^{(3)}]_\beta = (0,1,0)_\beta^T$ is a possible choice. This simple algorithm remains straightforward when the number of dimensions and vectors to be appended becomes much larger.\footnote{In fact, we only need to keep track of the row-swapping operations with full-zero rows.} Now
\begin{align*}
P_{\beta'}^\beta = 
\begin{bmatrix}
1 & 1 & 0 \\
1 & 1 & 1 \\
0 & 1 & 0
\end{bmatrix}
\end{align*}
and by Properties \ref{proper:chcoordsmat}
\begin{align*}
[T]_{\beta'}^\eta &= [T]_\beta^\eta P_{\beta'}^\beta \\
&= \begin{bmatrix}
1 & 1 & 2 \\
2 & 0 & 1 \\
1 & -1 & 1
\end{bmatrix}
\begin{bmatrix}
1 & 1 & 0 \\
1 & 1 & 1 \\
0 & 1 & 0
\end{bmatrix} \\
&=
\begin{bmatrix}
2 & 4 & 1 \\
2 & 3 & 0 \\
0 & 1 & -1
\end{bmatrix}
\end{align*}
The matrix representation of the restriction of $T$ to $\mathcal{W}$ with respect to $\mathcal{\beta}_W$ then agrees with the first two columns of $\smash{[T]_{\beta'}^\eta}$. The third column of $[T]_{\beta'}^\eta$ that characterizes the action of $T|_{W_C}$, is removed. These lead to
\begin{align*}
[T|_W]_{\beta_W}^\eta =
\begin{bmatrix}
2 & 4 \\
2 & 3 \\
0 & 1 
\end{bmatrix}
\end{align*}
\end{solution}
$\blacktriangleright$ Short Exercise: Directly apply Properties \ref{proper:restrictmat} to redo the example above.\footnote{$[T(\vec{w}_1)]_\eta = [T(\vec{u}_1+\vec{u}_2)]_\eta = [T]_\beta^\eta(1,1,0)_\beta^T =
\begin{bmatrix}
1 & 1 & 2 \\
2 & 0 & 1 \\
1 & -1 & 1
\end{bmatrix}_\beta^\eta
\begin{bmatrix}
1 \\
1 \\
0
\end{bmatrix}_\beta
= \begin{bmatrix}
2 \\
2 \\
0
\end{bmatrix}_\eta
$ and this will be the first column of $\smash{[T|_W]_{\beta_W}^\eta}$. The second column is derived similarly by evaluating $[T(\vec{w}_2)]_\eta$.}\par
With the concept of restriction, we can now introduce the matrix analog of a direct sum. For a linear transformation $T: \mathcal{U} \to \mathcal{V}$, if the vector spaces (finite-dimensional) involved are direct sums such that $\mathcal{U} = \mathcal{W} \oplus \mathcal{W}_C$ and $\mathcal{V} = \mathcal{Y} \oplus \mathcal{Y}^C$, and the ranges
\begin{align*}
\mathcal{R}(T|_W) \subseteq \mathcal{Y} & & \mathcal{R}(T|_{W^C}) \subseteq \mathcal{Y}^C
\end{align*}
of the two restrictions are such that vectors in $\mathcal{W}$ and $\mathcal{W}^C$ are mapped by $T$ to vectors in $\mathcal{Y}$ and $\mathcal{Y}^C$ separately, then $T = T|_W \oplus T|_{W^C}$ is a \index{Matrix Direct Sum}\keywordhl{matrix direct sum} in the sense that the linear transformation $T$ maps each of the complement subspaces in a direct sum of $\mathcal{U}$ into the corresponding complement subspaces in a direct sum of $\mathcal{V}$. If we write the input vector $\vec{u} = \vec{w} + \vec{w}^C$ as a direct sum where $\vec{w} \in \mathcal{W}$ and $\vec{w}^C \in \mathcal{W}^C$, then the output vector will also be a direct sum $\vec{v} = \vec{y} + \vec{y}^C$ where $\vec{y} = T|_W(\vec{w}) = T(\vec{w})$ and $\vec{y}^C = T|_{W^C}(\vec{w}^C) = T(\vec{w}^C)$, which are obtained by first computing $T(\vec{w})$ and $T(\vec{w}^C)$ individually, and then directly concatenating them together.
\begin{defn}[Matrix Direct Sum]
\label{defn:matdirectsum}
The direct sum of two matrices acting as linear transformations $T_1: \mathcal{U}_1 \to \mathcal{V}_1$ and $T_2: \mathcal{U}_2 \to \mathcal{V}_2$ is $T = T_1 \oplus T_2$ such that for any vector direct sum $\vec{u} = \vec{u}_1 + \vec{u}_2$ in $\mathcal{U} = \mathcal{U}_1 \oplus \mathcal{U}_2$, applying $T$ on $\vec{u}$ will yield an output of a vector direct sum $T(\vec{u}) = \vec{v} = \vec{v}_1 + \vec{v}_2$ in $\mathcal{V}_1 \oplus \mathcal{V}_2$ as well, where $\vec{v}_1 = T_1(\vec{u}_1) = T|_{U_1}(\vec{u}_1) \in \mathcal{V}_1$ and $\vec{v}_2 = T_2(\vec{u}_2) = T|_{U_2}(\vec{u}_2) \in \mathcal{V}_2$. The matrix direct sum is then the matrix representation of $T = T_1 \oplus T_2$ with respect to the direct sum basis for $\mathcal{U}_1 \oplus \mathcal{U}_2$ and $\mathcal{V}_1 \oplus \mathcal{V}_2$.
\end{defn}
Using the above definition, if $\mathcal{U}_1$ and $\mathcal{U}_2$ has a basis $\mathcal{\beta}_1 = \{\vec{w}^{(1)}, \vec{w}^{(2)}, \ldots, \vec{w}^{(r)}\}$ and $\mathcal{\beta}_2 = \{\vec{w}^{(r+1)}, \vec{w}^{(r+2)}, \ldots, \vec{w}^{(n)}\}$, and $\mathcal{V}_1$ and $\mathcal{V}_2$ has a basis $\mathcal{\eta}_1 = \{\vec{y}^{(1)}, \vec{y}^{(2)}, \allowbreak \ldots, \vec{y}^{(s)}\}$ and $\mathcal{\eta}_2 = \{\vec{y}^{(s+1)}, \vec{y}^{(s+2)}, \ldots, \vec{y}^{(m)}\}$, where $r, n, s, m$ are some integers, then $T = T_1 \oplus T_2$ will have a \textit{block diagonal} matrix representation of
\begin{align}
[T]_{\beta_1 + \beta_2}^{\eta_1 + \eta_2}
\equiv
\begin{bmatrix}
([T_1]_{\beta_1}^{\eta_1})_{s \times r} & [\textbf{0}]_{s \times (n-r)} \\
[\textbf{0}]_{(m-s) \times r} & ([T_2]_{\beta_2}^{\eta_2})_{(m-s) \times (n-r)}
\end{bmatrix}_{\beta_1 + \beta_2}^{\eta_1 + \eta_2}
\end{align}
with respect to the $\mathcal{\beta}_1 \cup \mathcal{\beta}_2$ and $\mathcal{\eta}_1 \cup \mathcal{\eta}_2$ bases. To see this, let $\vec{u} = \vec{u}^{(1)} \oplus \vec{u}^{(2)}$, $\vec{u}^{(1)} \in \mathcal{U}_1$ and $\vec{u}^{(2)} \in \mathcal{U}_2$, then
\begin{align*}
T(\vec{u}) &= [T]_{\beta_1 + \beta_2}^{\eta_1 + \eta_2}[\vec{u}]_{\beta_1 + \beta_2} \\ 
&= \begin{bmatrix}
([T_1]_{\beta_1}^{\eta_1})_{s \times r} & [\textbf{0}]_{s \times (n-r)} \\
[\textbf{0}]_{(m-s) \times r} & ([T_2]_{\beta_2}^{\eta_2})_{(m-s) \times (n-r)}
\end{bmatrix}_{\beta_1 + \beta_2}^{\eta_1 + \eta_2}
\begin{bmatrix}
([\vec{u}_1]_{\beta_1})_r \\
([\vec{u}_2]_{\beta_2})_{n-r}
\end{bmatrix}_{\beta_1 + \beta_2} \\
&= 
\begin{bmatrix}
([T_1]_{\beta_1}^{\eta_1})_{s \times r}([\vec{u}_1]_{\beta_1})_r + [\textbf{0}]_{s \times (n-r)}([\vec{u}_2]_{\beta_2})_{n-r} \\
[\textbf{0}]_{(m-s) \times r}([\vec{u}_1]_{\beta_1})_r + ([T_2]_{\beta_2}^{\eta_2})_{(m-s) \times (n-r)}([\vec{u}_2]_{\beta_2})_{n-r}
\end{bmatrix}_{\eta_1 + \eta_2} \\
&= 
\begin{bmatrix}
([T_1]_{\beta_1}^{\eta_1}[\vec{u}_1]_{\beta_1})_s \\
([T_2]_{\beta_2}^{\eta_2}[\vec{u}_2]_{\beta_2})_{m-s}
\end{bmatrix}_{\eta_1 + \eta_2} \\
&\equiv
T_1(\vec{u}_1) \oplus T_2(\vec{u}_2)
\end{align*}
where the image is a direct sum composed of $T_1(\vec{u}_1) \equiv \smash{[T_1]_{\beta_1}^{\eta_1}}[\vec{u}_1]_{\beta_1} \in \mathcal{V}_1$ and $T_2(\vec{u}_2) \equiv \smash{[T_2]_{\beta_2}^{\eta_2}}[\vec{u}_2]_{\beta_2} \in \mathcal{V}_2$ from applying $T_1$ and $T_2$ separately to the preimages $\vec{u}_1 \in \mathcal{U}_1$ and $\vec{u}_2 \in \mathcal{U}_2$ in the two subspaces.
% we will consider linear operators such that $T: \mathcal{V} \to \mathcal{V}$ is a linear transformation within the vector space $\mathcal{V}$ itself. A special case for the restriction of a linear operator, originally $T|_W: \mathcal{W} \to \mathcal{V}$ with $\mathcal{W} \subset \mathcal{V}$, is that the range of $T|_W$ is a subspace of $\mathcal{W}$ so that it can be reduced to $T|_W: \mathcal{W} \to \mathcal{W}$. $\mathcal{W}$ is then referred to as an \index{Invariant Subspace}\keywordhl{invariant subspace} under $T$, since the mapping $T|_W$ and hence $T$ does not alter the subspace in the sense that any vector in $\mathcal{W}$ is never mapped to other vectors outside $\mathcal{W}$ via $T$, i.e. $T(\vec{w}) \notin \mathcal{W}^C$ except the zero vector. In this case, we can replace $T|_W$ by $T$.
%\begin{defn}[Invariant Subspaces]
%A subspace $\mathcal{W}$ of $\mathcal{V}$ is known as an invariant subspace under the linear transformation $T$ if for any $\vec{w} \in \mathcal{W}$, $T(\vec{w}) \in \mathcal{W}$.
%\end{defn}
%If $\dim(\mathcal{V}) = n$, $\dim(\mathcal{W}) = r$ are both finite-dimensional, then the matrix representation of $T: \mathcal{V} \to \mathcal{V}$, with respect to the basis $\mathcal{B}' = \mathcal{B}_W \cup \mathcal{G}$ as put during the derivation for Properties \ref{proper:restrictmat}, can be expressed as a block matrix:
%\begin{align*}
%[T]_{B'} = \begin{bmatrix}
%A_{W,{r \times r}} & *_{r\times(n-r)} \\
%0_{(n-r) \times r} & *_{r\times(n-r)}
%\end{bmatrix}
%=
%\begin{bmatrix}
%[T|_W]_{B_W} \, |\,*_{n\times(n-r)}
%\end{bmatrix}
%\end{align*}
%where the $r$ bottommost rows of $[T|_W]_{B_W} = \begin{bmatrix}
%A_{W,{r \times r}} \\
%0_{(n-r) \times r}
%\end{bmatrix}$ are all zeros and $A_W$ is an $r \times r$ matrix. Note that for any $\vec{w} \in \mathcal{W}$, its mapped image $T(\vec{w})$
%\begin{align*}
%[T]_{B'}[\vec{w}]_{B'} &= \begin{bmatrix}
%A_{W,{r \times r}} & *_{r\times(n-r)} \\
%0_{(n-r) \times r} & *_{r\times(n-r)}
%\end{bmatrix}_{B'}
%\begin{bmatrix}
%w_1 \\
%w_2 \\
%\vdots \\
%w_r \\
%0 \\
%\vdots \\
%0
%\end{bmatrix}_{B'}
%=
%\begin{bmatrix}
%\sum_{j=1}^r (A_W)_{1j} w_j \\
%\sum_{j=1}^r (A_W)_{2j} w_j \\
%\vdots \\
%\sum_{j=1}^r (A_W)_{rj} w_j \\
%0 \\
%\vdots \\
%0
%\end{bmatrix}_{B'}
%\in \mathcal{W}
%\end{align*}
%is still in $\mathcal{W}$ so that the block matrix representation above is consistent. If the complement $\mathcal{W}^C$ generated by $\mathcal{G}$ itself is also a invariant subspace under $T$, then by a similar logic, we have
%\begin{align*}
%[T]_{B'} = \begin{bmatrix}
%A_{W,{r \times r}} & 0_{r\times(n-r)} \\
%0_{(n-r) \times r} & A_{W^C,{r \times r}}
%\end{bmatrix}
%\end{align*}
For example, the matrix direct sum of $A \oplus B$ given
\begin{align*}
A &= 
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix}
& 
B &=
\begin{bmatrix}
1 & 8 \\
1 & 1 \\
4 & 0
\end{bmatrix}
\end{align*}
is
\begin{align*}
A \oplus B =
\left[\begin{array}{@{\,}wc{10pt}wc{10pt}wc{10pt}|wc{10pt}wc{10pt}}
1 & 2 & 3 & 0 & 0 \\
4 & 5 & 6 & 0 & 0 \\
\hline
0 & 0 & 0 & 1 & 8 \\
0 & 0 & 0 & 1 & 1 \\
0 & 0 & 0 & 4 & 0
\end{array}\right]
\end{align*}
in which $A$ and $B$ are matrices representing linear transformations of $T_1: \mathcal{U}_1 \to \mathcal{V}_1$ and $T_2: \mathcal{U}_2 \to \mathcal{V}_2$, where $\mathcal{U}_1$, $\mathcal{U}_2$, $\mathcal{V}_1$, $\mathcal{V}_2$ have dimensions of $3,2,2,3$. Subsequently, $A \oplus B$ is a matrix corresponding to a mapping $T_1 \oplus T_2$ from $\mathcal{U} = \mathcal{U}_1 \oplus \mathcal{U}_2$ to $\mathcal{V} = \mathcal{V}_1 \oplus \mathcal{V}_2$. Finally, the matrix direct sum of more than two matrices $A_1, A_2, A_3, \ldots, A_{n-1}, A_n$ are defined recursively just like a vector direct sum as
\begin{align*}
&\quad A_1 \oplus A_2 \oplus A_3 \oplus \cdots \oplus A_{n-1} \oplus A_n \\
&= (\cdots ((A_1 \oplus A_2) \oplus A_3) \oplus \cdots \oplus A_{n-1}) \oplus A_n
\end{align*}
As another example, sometimes we may regard a matrix that does not look like a direct sum to be effectively one with respect to appropriate coordinate systems in a broader sense.
\begin{exmp}
For a linear transformation $T: \mathcal{U} \to \mathcal{V}$ that has a matrix representation of
\begin{align*}
[T]_\beta^\eta =
\begin{bmatrix}
1 & 0 & 2 & -2 \\
0 & 0 & 1 & 0 \\
1 & -2 & 1 & 0
\end{bmatrix} 
\end{align*}
with respect to some bases $\mathcal{\beta} = \{\vec{u}^{(1)}, \vec{u}^{(2)}, \vec{u}^{(3)}, \vec{u}^{(4)}\}$, $\mathcal{\eta} = \{\vec{v}^{(1)}, \vec{v}^{(2)}, \vec{v}^{(3)}\}$, show that it can turn into an apparent matrix direct sum if the coordinate systems are changed according to $\mathcal{\beta}' = \{\vec{u}^{(1)'}, \vec{u}^{(2)'}, \vec{u}^{(3)'}, \vec{u}^{(4)'}\}$, $\mathcal{\eta}' = \{\vec{v}^{(1)'}, \vec{v}^{(2)'}, \vec{v}^{(3)'}\}$, where
\begin{align*}
\vec{u}^{(1)'} &= \vec{u}^{(1)} & \vec{v}^{(1)'} &= \vec{v}^{(1)} + \vec{v}^{(2)} \\
\vec{u}^{(2)'} &= \vec{u}^{(3)} & \vec{v}^{(2)'} &= -\vec{v}^{(2)} + \vec{v}^{(3)} \\
\vec{u}^{(3)'} &= \vec{u}^{(1)} + \vec{u}^{(2)} & \vec{v}^{(3)'} &= \vec{v}^{(1)} - \vec{v}^{(3)} \\
\vec{u}^{(4)'} &= \vec{u}^{(1)} + \vec{u}^{(4)}
\end{align*}
\end{exmp}
\begin{solution}
The change of coordinate matrices suggested by Properties \ref{proper:chcoordsmat} are
\begin{align*}
P_{\beta'}^\beta &= 
\begin{bmatrix}
1 & 0 & 1 & 1 \\
0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}_{\beta'}^\beta
&
Q_{\eta'}^\eta &= 
\begin{bmatrix}
1 & 0 & 1 \\
1 & -1 & 0 \\
0 & 1 & -1
\end{bmatrix}_{\eta'}^\eta
\end{align*}
and the new matrix representation of $T$ is
\begin{align*}
[T]_{\beta'}^{\eta'} &= (Q_{\eta'}^\eta)^{-1} [T]_\beta^\eta P_{\beta'}^\beta \\
&= \left(\begin{bmatrix}
1 & 0 & 1 \\
1 & -1 & 0 \\
0 & 1 & -1
\end{bmatrix}_{\eta'}^\eta\right)^{-1}
\begin{bmatrix}
1 & 0 & 2 & -2 \\
0 & 0 & 1 & 0 \\
1 & -2 & 1 & 0
\end{bmatrix}_\beta^\eta
\begin{bmatrix}
1 & 0 & 1 & 1 \\
0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}_{\beta'}^\beta \\
&= \begin{bmatrix}
\frac{1}{2} & \frac{1}{2} & \frac{1}{2} \\
\frac{1}{2} & -\frac{1}{2} & \frac{1}{2} \\
\frac{1}{2} & -\frac{1}{2} & -\frac{1}{2}
\end{bmatrix}_{\eta}^{\eta'}
\begin{bmatrix}
1 & 0 & 2 & -2 \\
0 & 0 & 1 & 0 \\
1 & -2 & 1 & 0
\end{bmatrix}_\beta^\eta
\begin{bmatrix}
1 & 0 & 1 & 1 \\
0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}_{\beta'}^\beta \\
&=
\begin{bmatrix}
1 & 2 & 0 & 0 \\
1 & 1 & 0 & 0 \\
0 & 0 & 1 & -1
\end{bmatrix}_{\beta'}^{\eta'}
\end{align*}
where it can be seen that
\begin{align*}
\begin{bmatrix}
1 & 2 & 0 & 0 \\
1 & 1 & 0 & 0 \\
0 & 0 & 1 & -1
\end{bmatrix} =
\begin{bmatrix}
1 & 2 \\
1 & 1
\end{bmatrix}
\oplus 
\begin{bmatrix}
1 & -1
\end{bmatrix}
\end{align*}
\end{solution}

\section{Python Programming}
Complex numbers in Python are written as \verb|a+bj|. For example,
\begin{lstlisting}
z_1 = 1 - 2j
z_2 = 3 + 1j
print(z_1, z_2)
\end{lstlisting}
returns \verb|(1-2j) (3+1j)| (\texttt{1} in front of \texttt{j} is needed). Conjugate, modulus, and argument can be found by
\begin{lstlisting}
import numpy as np
from scipy import linalg

print(np.conjugate(z_1))
print(np.abs(z_2))    
print(np.angle(z_1))
\end{lstlisting}
which yields \verb|(1+2j)|, \verb|3.162278| ($\sqrt{10}$), and \verb|-1.10715| (in radians). Addition, subtraction, multiplication, and division of complex numbers in Python are coded just like if they are ordinary numbers.
\begin{lstlisting}
print(3*z_1 + z_2) # (6-5j)
print(z_1 - 2*z_2) # (-5-4j)
print(z_1 * z_2) # (5-5j)
print(z_1 / z_2) # (0.1-0.7000000000000001j), floating-point error
\end{lstlisting}
The same goes for complex matrices and their multiplication. As an example,
\begin{lstlisting}
A = np.array([[4.  , 2.+1.j],
              [-3.j, 1-2.j]])
B = np.array([[3-1.j, 0],
              [2-5.j, 4.j]])
print(A @ B)
\end{lstlisting}
produces
\begin{lstlisting}
[[ 21.-12.j  -4. +8.j]
 [-11.-18.j   8. +4.j]]    
\end{lstlisting}
Conjugate and Hermitian transpose of a complex matrix is retrieved by
\begin{lstlisting}
print(np.conjugate(A))
print(np.conjugate(B).T) # Hermitian transpose = conjugate + transpose, or just .H if np.matrix is used instead of np.array
\end{lstlisting}
resulting in
\begin{lstlisting}
[[ 4.-0.j  2.-1.j]
 [-0.+3.j  1.+2.j]]
[[3.+1.j 2.+5.j]
 [0.-0.j 0.-4.j]]
\end{lstlisting}
The usual functions for inverse and determinant also work on complex matrices. The lines
\begin{lstlisting}
print(linalg.det(A))
print(linalg.inv(B))
\end{lstlisting}
give
\begin{lstlisting}
(1-2j)
[[3.00000000e-01+0.1j   0.00000000e+00+0.j   ]
 [3.25000000e-01+0.275j 1.38777878e-17-0.25j ]] # Again, round-off error    
\end{lstlisting}
We can use 1D complex matrices as complex vectors.
\begin{lstlisting}
u = np.array([1+1.j, -3.j, 2])
v = np.array([5, 1+2.j, 1-4.j])
\end{lstlisting}
The complex dot product between two complex vectors are then found by \verb|vdot|
\begin{lstlisting}
print(np.vdot(u,v).conj())
\end{lstlisting}
notice that a conjugate is needed since \verb|numpy| defines complex dot product with a different convention such that the first complex vector is conjugated instead of the second one. It then outputs the correct answer of \verb|(1+10j)|. The \verb|norm| function still works fine, e.g. \verb|print(linalg.norm(u))| gives \verb|3.87298| ($\sqrt{(1+i)(1-i) + (-3i)(3i) + (2)^2} = \sqrt{15}$). Finally, for the discussion in the last section, to build a block matrix using submatrices, we can use the \verb|block| function as
\begin{lstlisting}
C = np.array([1, 3+2.j])
D = np.array([-1.j, 2])

print(np.block([[A, B],
                [C, D]]))
\end{lstlisting}
which outputs
\begin{lstlisting}
[[ 4.+0.j  2.+1.j  3.-1.j  0.+0.j]
 [-0.-3.j  1.-2.j  2.-5.j  0.+4.j]
 [ 1.+0.j  3.+2.j -0.-1.j  2.+0.j]]    
\end{lstlisting}
Another example of constructing a block diagonal matrix is
\begin{lstlisting}
print(np.block([[A, np.zeros([2,3])],
                [np.zeros([3,2]), np.identity(3)]]))
\end{lstlisting}
generating
\begin{lstlisting}
[[ 4.+0.j  2.+1.j  0.+0.j  0.+0.j  0.+0.j]
 [-0.-3.j  1.-2.j  0.+0.j  0.+0.j  0.+0.j]
 [ 0.+0.j  0.+0.j  1.+0.j  0.+0.j  0.+0.j]
 [ 0.+0.j  0.+0.j  0.+0.j  1.+0.j  0.+0.j]
 [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j  1.+0.j]]    
\end{lstlisting}

\section{Exercises}

\begin{Exercise}
By considering Euler's formula stated in Definition \ref{defn:Euler}, we have for any $\theta$, $\phi$
\begin{align*}
e^{i \theta} &= \cos \theta + i \sin \theta \\
e^{i \phi} &= \cos \phi + i \sin \phi \\
e^{i (\theta+\phi)} &= \cos (\theta+\phi) + i \sin (\theta+\phi)
\end{align*}
If we take the product of the first two equations, we also have
\begin{align*}
e^{i (\theta+\phi)} &= (\cos \theta + i \sin \theta)(\cos \phi + i \sin \phi)
\end{align*}
By equating the two expressions of $e^{i (\theta+\phi)}$, expand and compare the real and imaginary parts, prove the famous angle sum identities, which are
\begin{align*}
\cos(\theta+\phi) &= \cos\theta \cos\phi - \sin\theta \sin\phi \\
\sin(\theta+\phi) &= \sin\theta \cos\phi + \cos\theta \sin\phi 
\end{align*}
Hence, by either using the results above or De Moivre's Formula, prove the double angle formula shown below.
\begin{align*}
\cos(2\theta) &= \cos^2\theta - \sin^2\theta \\
\sin(2\theta) &= 2\sin\theta \cos\theta  
\end{align*}
\end{Exercise}
\begin{Answer}
By De Moivre's Formula:
\begin{align*}
\cos(2\theta) + i \sin(2\theta) &= (\cos\theta + i \sin\theta)^2 \\
&= (\cos^2\theta - \sin^2 \theta) + 2i \sin\theta\cos\theta
\end{align*}
Now simply compare the real and imaginary parts.
\end{Answer}

\begin{Exercise}
Evaluate
\begin{enumerate}[label=(\alph*)]
\item $(1+i)(3-2i)$,
\item $\overline{(2-i)/(4+i)}$,
\item $(3+5i)\overline{(1+i)/(2-3i)}$
\end{enumerate}
as well as their modulus and argument.
\end{Exercise}
\begin{Answer}
\begin{enumerate}[label=(\alph*)]
\item $5+i$, $\sqrt{26}$, $0.1974$;
\item $\frac{7}{17}+\frac{6}{17}i$, $\sqrt{\frac{5}{17}}$, $0.7086$;
\item $\frac{22}{13}-\frac{20}{13}i$, $\frac{2\sqrt{17}}{\sqrt{13}}$, $-0.7378$. (All arguments are in radian.)
\end{enumerate}
\end{Answer}

\begin{Exercise}
For $\vec{u} = (1+i, 2-i, 3)^T$, $\vec{v} = (2+i, 1-2i, i)^T$, and $\vec{w} = (-i, 3, 1-i)^T$, find
\begin{enumerate}[label=(\alph*)]
\item $\vec{u} \cdot \vec{v}$;
\item $(\vec{u} + \vec{v}) \cdot (\vec{u} - \vec{w})$;
\item $\norm{\vec{u}} \vec{v} - \norm{\vec{v}} \vec{w}$.
\end{enumerate}
\end{Exercise}
\begin{Answer}
\begin{enumerate}[label=(\alph*)]
\item $7-i$;
\item $(3+2i, 3-3i, 3+i)^T \cdot (1+2i,-1-i,2+i)^T = 14+i$;
\item $4(2+i, 1-2i, i)^T - \sqrt{11}(-i, 3, 1-i)^T \\
= (8+(4+\sqrt{11})i, (4-3\sqrt{11})-8i, -\sqrt{11}+(4+\sqrt{11})i)^T $
\end{enumerate}
\end{Answer}

\begin{Exercise}
For the two complex matrices below,
\begin{align*}
& A=
\begin{bmatrix}
1+i & -i & 3 \\
0 & 2-i & 1 \\
-1 & i & 2
\end{bmatrix}
& B=
\begin{bmatrix}
1 & 2-i & i \\
-i & 3+i & 1-i \\
0 & 1 & 2i
\end{bmatrix}
\end{align*}
compute $AB^*$, and verify $(AB^*)^* = BA^*$.
\end{Exercise}
\begin{Answer}
\begin{align*}
AB^* &=
\begin{bmatrix}
1+i & -i & 3 \\
0 & 2-i & 1 \\
-1 & i & 2
\end{bmatrix}
\begin{bmatrix}
1 & i & 0 \\ 
2+i & 3-i & 1 \\ 
-i & 1+i & -2i
\end{bmatrix} \\
&=
\begin{bmatrix}
2-4i&1+i&-7i\\ 
5-i&6-4i&2-3i\\ 
-2&3+4i&-3i
\end{bmatrix}
\end{align*}
and
\begin{align*}
BA^* &= 
\begin{bmatrix}
1 & 2-i & i \\
-i & 3+i & 1-i \\
0 & 1 & 2i
\end{bmatrix}
\begin{bmatrix}
1-i & 0 & -1\\
i & 2+i & -i\\
3 & 1 & 2
\end{bmatrix} \\
&= 
\begin{bmatrix}
2+4i&5+i&-2\\ 
1-i&6+4i&3-4i\\ 
7i&2+3i&3i
\end{bmatrix} = (AB^*)^*
\end{align*}
\end{Answer}

\begin{Exercise}
For the matrix
\begin{align*}
A=
\begin{bmatrix}
1-4i&-3i&2+i\\ 
2-3i&0&4i\\ 
-2&1&3-i
\end{bmatrix}    
\end{align*}
find its determinant and inverse.
\end{Exercise}
\begin{Answer}
\begin{align*}
\det(A) &= i \\
A^{-1} &= 
\begin{bmatrix}
-4&10-5i&-12i\\ 
3i+3&-11-3i&-8+9i\\ 
-3-2i&10+i&6-9i
\end{bmatrix}
\end{align*}    
\end{Answer}

\begin{Exercise}
\phantomsection
\label{ex:schurinvD}
Prove the formulae in Properties \ref{proper:schurinvD}, by noting that
\begin{align*}
\begin{bmatrix}
I_p & [\textbf{0}]_{p \times q} \\
-D^{-1}C & D^{-1}
\end{bmatrix}  
=
\begin{bmatrix}
I_p & [\textbf{0}]_{p \times q} \\
[\textbf{0}]_{q \times p} & D^{-1}
\end{bmatrix} 
\begin{bmatrix}
I_p & [\textbf{0}]_{p \times q} \\
-C & I_q
\end{bmatrix} 
\end{align*}
and
\begin{align*}
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix}
\begin{bmatrix}
I_p & [\textbf{0}]_{p \times q} \\
-D^{-1}C & D^{-1}
\end{bmatrix} =
\begin{bmatrix}
A - BD^{-1}C & BD^{-1} \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix}
\end{align*}
\end{Exercise}
\begin{Answer}
Notice that
\begin{align*}
\begin{bmatrix}
I_p & -BD^{-1} \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix}
\begin{bmatrix}
A - BD^{-1}C & BD^{-1} \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix}
=
\begin{bmatrix}
A - BD^{-1}C & [\textbf{0}]_{p \times q} \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix}
\end{align*}
then
\begin{align*}
&\begin{bmatrix}
I_p & -BD^{-1} \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix}
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix}
\begin{bmatrix}
I_p & [\textbf{0}]_{p \times q} \\
-D^{-1}C & D^{-1}
\end{bmatrix} \\
=&
\begin{bmatrix}
I_p & -BD^{-1} \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix}
\begin{bmatrix}
A - BD^{-1}C & BD^{-1} \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix} =
\begin{bmatrix}
A - BD^{-1}C & [\textbf{0}]_{p \times q} \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix}
\end{align*}
The determinant formula can easily be derived from this equation. Moreover,
\begin{align*}
& \begin{bmatrix}
A & B \\
C & D
\end{bmatrix}^{-1} \\
=& 
\left(
\begin{bmatrix}
I_p & -BD^{-1} \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix}^{-1}
\begin{bmatrix}
A - BD^{-1}C & [\textbf{0}]_{p \times q} \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix}
\begin{bmatrix}
I_p & [\textbf{0}]_{p \times q} \\
-D^{-1}C & D^{-1}
\end{bmatrix}^{-1}\right)^{-1} \\
=& 
\begin{bmatrix}
I_p & [\textbf{0}]_{p \times q} \\
-D^{-1}C & D^{-1}
\end{bmatrix}
\begin{bmatrix}
(A - BD^{-1}C)^{-1} & [\textbf{0}]_{p \times q} \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix}
\begin{bmatrix}
I_p & -BD^{-1} \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix} \\
=& 
\begin{bmatrix}
(A - BD^{-1}C)^{-1} & [\textbf{0}]_{p \times q} \\
-D^{-1}C(A - BD^{-1}C)^{-1} & D^{-1}
\end{bmatrix}
\begin{bmatrix}
I_p & -BD^{-1} \\
[\textbf{0}]_{q \times p} & I_q
\end{bmatrix} \\
=& 
\begin{bmatrix}
(A - BD^{-1}C)^{-1} & -(A - BD^{-1}C)^{-1}BD^{-1} \\
-D^{-1}C(A - BD^{-1}C)^{-1} & D^{-1} + D^{-1}C(A - BD^{-1}C)^{-1}BD^{-1}
\end{bmatrix} \\
=&
\begin{bmatrix}
(M/D)^{-1} & -(M/D)^{-1}BD^{-1}  \\
-D^{-1}C (M/D)^{-1} & D^{-1} + D^{-1}C (M/D)^{-1} BD^{-1}
\end{bmatrix}
\end{align*}
\end{Answer}

\begin{Exercise}
Write down the direct sum of the following three matrices.
\begin{align*}
A &= 
\begin{bmatrix}
2 & 1 \\
0 & 4 \\
-1 & 3
\end{bmatrix} &
C &= 
\begin{bmatrix}
1 & 4 & 0 & -3 \\
0 & 2 & -1 & 1
\end{bmatrix} \\
B &= \begin{bmatrix}
1
\end{bmatrix}
\end{align*}
\end{Exercise}
\begin{Answer}
\begin{align*}
A \oplus B \oplus C =
\begin{bmatrix}
2 & 1 & 0 & 0 & 0 & 0 & 0\\
0 & 4 & 0 & 0 & 0 & 0 & 0\\
-1 & 3 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 4 & 0 & -3 \\
0 & 0 & 0 & 0 & 2 & -1 & 1
\end{bmatrix}
\end{align*}
\end{Answer}

\begin{Exercise}
Show that given two bases $\mathcal{\beta} = \{\cos x, \sin x, 1, x, x^2\}$ and $\mathcal{\eta} = \{\cos x, \sin x, 1, x\}$ which generate vector spaces $\mathcal{U}$ and $\mathcal{V}$ respectively, the differentiation operator $T(f(x)) = f'(x): \mathcal{U} \to \mathcal{V}$ has a $2 \times 2$ block matrix direct sum representation.
\end{Exercise}
\begin{Answer}
It is simply
\begin{align*}
[T]_{\beta}^{\eta} &=
\begin{bmatrix}
0 & 1 & 0 & 0 &0\\
-1 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 2
\end{bmatrix} \\
&=
\begin{bmatrix}
0 & 1 \\
-1 & 0 
\end{bmatrix}
\oplus
\begin{bmatrix}
0 & 1 & 0\\    
0 & 0 & 2
\end{bmatrix}
\end{align*}
\end{Answer}